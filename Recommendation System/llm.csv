,S. No.,Title,Authors,Summary,Content
0,1,Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications,Irene Weber,"Large Language Models (LLMs) have become widely adopted recently. Research
explores their use both as autonomous agents and as tools for software
engineering. LLM-integrated applications, on the other hand, are software
systems that leverage an LLM to perform tasks that would otherwise be
impossible or require significant coding effort. While LLM-integrated
application engineering is emerging as new discipline, its terminology,
concepts and methods need to be established. This study provides a taxonomy for
LLM-integrated applications, offering a framework for analyzing and describing
these systems. It also demonstrates various ways to utilize LLMs in
applications, as well as options for implementing such integrations.
  Following established methods, we analyze a sample of recent LLM-integrated
applications to identify relevant dimensions. We evaluate the taxonomy by
applying it to additional cases. This review shows that applications integrate
LLMs in numerous ways for various purposes. Frequently, they comprise multiple
LLM integrations, which we term ``LLM components''. To gain a clear
understanding of an application's architecture, we examine each LLM component
separately. We identify thirteen dimensions along which to characterize an LLM
component, including the LLM skills leveraged, the format of the output, and
more. LLM-integrated applications are described as combinations of their LLM
components. We suggest a concise representation using feature vectors for
visualization.
  The taxonomy is effective for describing LLM-integrated applications. It can
contribute to theory building in the nascent field of LLM-integrated
application engineering and aid in developing such systems. Researchers and
practitioners explore numerous creative ways to leverage LLMs in applications.
Though challenges persist, integrating LLMs may revolutionize the way software
systems are built.","Large Language Models as Software Components:
A Taxonomy for LLM-Integrated Applications
Irene Weber
Kempten University of Applied Sciences, Germany
irene.weber@hs-kempten.de
Abstract
Large Language Models (LLMs) have become widely adopted recently. Research explores their use both
as autonomous agents and as tools for software engineering. LLM-integrated applications, on the other
hand, are software systems that leverage an LLM to perform tasks that would otherwise be impossible or
require significant coding effort. While LLM-integrated application engineering is emerging as new discipline,
its terminology, concepts and methods need to be established. This study provides a taxonomy for LLM-
integrated applications, offering a framework for analyzing and describing these systems. It also demonstrates
various ways to utilize LLMs in applications, as well as options for implementing such integrations.
Following established methods, we analyze a sample of recent LLM-integrated applications to identify rel-
evant dimensions. We evaluate the taxonomy by applying it to additional cases. This review shows that
applications integrate LLMs in numerous ways for various purposes. Frequently, they comprise multiple
LLM integrations, which we term “LLM components”. To gain a clear understanding of an application’s
architecture, we examine each LLM component separately. We identify thirteen dimensions along which to
characterize an LLM component, including the LLM skills leveraged, the format of the output, and more.
LLM-integrated applications are described as combinations of their LLM components. We suggest a concise
representation using feature vectors for visualization.
The taxonomy is effective for describing LLM-integrated applications. It can contribute to theory building in
the nascent field of LLM-integrated application engineering and aid in developing such systems. Researchers
and practitioners explore numerous creative ways to leverage LLMs in applications.
Though challenges
persist, integrating LLMs may revolutionize the way software systems are built.
Keywords:
large language model, LLM-integrated, taxonomy, copilot, architecture, AI agent, LLM
component
1. Introduction
Large Language Models (LLMs) have significantly
impacted various sectors of economy and society [47].
Due to their proficiency in text understanding, cre-
ative work, communication, knowledge work, and
code writing, they have been adopted in numerous
fields, such as medicine, law, marketing, education,
human resources, etc.
Public discussions often focus on the ethical aspects
and societal consequences of these systems [36, 39].
Meanwhile, research investigates Artificial General
Intelligences and autonomous AI agents that can use
services, data sources, and other tools, and collabo-
arXiv:2406.10300v1  [cs.SE]  13 Jun 2024
rate to solve complex tasks [11, 62, 57, 21]. In addi-
tion, LLMs offer many opportunities to enhance soft-
ware systems. They enable natural language interac-
tion [59], automate complex tasks [19], and provide
supportive collaboration, as seen with recent LLM-
based assistant products often branded as “copilots”1.
This paper addresses the potential of LLMs for soft-
ware development by integrating their capabilities as
components into software systems.
This contrasts
with current software engineering research, which
views LLMs as tools for software development rather
than as software components [14, 22], and with the
considerable body of research examining LLMs as au-
tonomous agents within multiagent systems [21].
Software systems that invoke an LLM and process
its output are referred to as “LLM-integrated appli-
cations”, “LLM-integrated systems”, “LLM-based ap-
plications”, etc. [32, 13, 57]. LLMs are versatile, mul-
tipurpose tools capable of providing functionalities
that would otherwise be unfeasible or require sub-
stantial development efforts [15, 24]. By significantly
expediting system development, they have the poten-
tial to revolutionize not only the way users interact
with technology, but also the fundamental processes
of software development.
LLM-integrated applications engineering is emerging
as a research field.
E.g., [10] proposes LLM Sys-
tems Engineering (LLM-SE) as a novel discipline, and
[44, 8, 7] discuss experiences and challenges that de-
velopers of such systems encounter in practice.
This study develops a taxonomy that provides a
structured framework for categorizing and analyzing
LLM-integrated applications across various domains.
To develop and evaluate the taxonomy, we collected
a sample of LLM-integrated applications, concentrat-
ing on technical and industrial domains. These ap-
plications showcase a broad range of opportunities
to leverage LLMs, often integrating LLMs in mul-
tiple ways for distinct purposes. In developing the
taxonomy, we found that examining each of these in-
tegrations, termed “LLM components”, separately is
1E.g., https://docs.github.com/en/copilot,
https://copilot.cloud.microsoft/en-us/copilot-excel,
https://www.salesforce.com/einsteincopilot
crucial for a clear understanding of an application’s
architecture.
The taxonomy adopts an original architectural per-
spective, focusing on how the application interacts
with the LLM while abstracting from the specifics
of application domains. For researchers, the taxon-
omy contributes to shape a common understanding
and terminology, thus aiding theory building in this
emerging domain [29, 50, 18]. For practitioners, the
taxonomy provides inspiration for potential uses of
LLMs in applications, presents design options, and
helps identify challenges and approaches to address
them.
Objectives. In this study, a taxonomy is understood
as a set of dimensions divided into characteristics.
The objective is to identify dimensions that are useful
for categorizing the integration of LLMs in applica-
tions from an architectural perspective. To be most
effective, the taxonomy should be easy to understand
and apply, yet distinctive enough to uncover the es-
sential aspects.
Additionally, we aim to develop a
visual representation tailored to the taxonomy’s in-
tended purposes.
Overview. The following section 2 provides back-
ground on LLMs and introduces relevant concepts.
Section 3 presents an overview of related work. The
study design adheres to a Design Science Research
approach [46]. We apply established methods for tax-
onomy design [42, 48] as described in Section 4. This
section also presents the sample of LLM-integrated
applications used for this study. The developed tax-
onomy is presented, demonstrated and formally eval-
uated in section 5. In section 6, we discuss its usabil-
ity and usefulness. Section 7 summarizes the contri-
butions, addresses limitations, and concludes.
2. Large Language Models
2.1. Background
State-of-the-art LLMs such as GPT-3.5, GPT-4,
Llama, PALM2, etc., are artificial neural networks
consisting of neurons, i.e., very simple processing
2
units, that are organized in layers and connected by
weighted links.
Training a neural network means
adapting these weights such that the neural network
shows a certain desired behavior.
Specifically, an
LLM is trained to predict the likelihoods of pieces
of text termed, tokens, to occur as continuations of
a given text presented as input to the LLM. This in-
put is referred to as prompt. The prompt combined
with the produced output constitutes the context of
an LLM. It may comprise more than 100k tokens in
state-of-the-art LLMs2. Still, its length is limited and
determines the maximum size of prompts and outputs
that an LLM is capable of processing and generating
at a time.
Training of an LLM optimizes its parameters such
that its computed likelihoods align with real text ex-
amples. The training data is a vast body of text snip-
pets extracted, processed, and curated from sources
such as Wikipedia, Github code repositories, common
websites, books, or news archives. An LLM trained
on massive examples is termed a foundation model
or pre-trained model. During training, an LLM not
only learns to produce correct language but also ab-
sorbs and stores information and factual knowledge.
However, it is well known that LLMs frequently pick
up biases, leading to ethical problems.
They may
also produce factually incorrect outputs that sound
plausible and convincing, termed hallucinations.
Recent findings show that LLMs can be applied to
a wide range of tasks by appropriately formulating
prompts. Different prompt patterns succeed in dif-
ferent tasks.
Basic approaches rely on instructing
the LLM to solve a task described or explained in
the prompt. In few-shot prompting (also known as
few-shot learning), the prompt is augmented with ex-
ample input-output pairs illustrating how to solve the
task, e.g., the requested output format. The number
of examples can vary. Prompting with one example is
called one-shot prompting, while prompting without
any examples is called zero-shot prompting. One-shot
and few-shot prompting fall under the broader cat-
egory of in-context learning. Prompt patterns such
2https://platform.openai.com/docs/models
as chain-of-thought and thinking-aloud aim to elicit
advanced reasoning capabilities from LLMs.
As effective prompts are crucial for unlocking the di-
verse capabilities of an LLM, the discipline of prompt
engineering is evolving, focusing on the systematic
design and management of prompts [66, 9, 53, 31].
2.2. Definitions
Invoking an LLM results in an input-processing-
output sequence: Upon receiving a prompt, the LLM
processes it and generates an output. We refer to an
individual sequence of input-processing-output per-
formed by the LLM as LLM invocation, and define
an LLM-integrated application as a system in which
the software generates the prompt for the LLM and
processes its output. The concept of an application
is broad, encompassing service-oriented architectures
and systems with components loosely coupled via
API calls.
Given an LLM’s versatility, an application can uti-
lize it for different tasks, each demanding a specific
approach to create the prompt and handle the re-
sult. This paper defines a particular software compo-
nent that accomplishes this as an LLM-based software
component or, simply, LLM component.
An LLM-
integrated application can comprise several LLM
components.
The study develops a taxonomy for
LLM components. LLM-integrated applications are
described as combinations of their LLM components.
3. Related Work
With the recent progress in generative AI and LLMs,
the interest in these techniques has increased, and
numerous surveys have been published, providing an
extensive overview of technical aspects of LLMs [72],
reviewing LLMs as tools for software engineering [22],
and discussing the technical challenges of applying
LLMs across various fields [25]. Further studies ad-
dress the regulatory and ethical aspects of Genera-
tive AI and ChatGPT, with a particular focus on
AI-human collaboration [41], and Augmented Lan-
guage Models (ALMs), which are LLMs that enhance
3
their capabilities by querying tools such as APIs,
databases, and web search engines [38].
Taxomonies related to LLMs include a taxonomy for
prompts designed to solve complex tasks [49] and a
taxonomy of methods for cost-effectively invoking a
remote LLM [60]. A comparative analysis of stud-
ies on applications of ChatGPT is provided by [27],
whereas LLMs are compared based on their applica-
tion domains and the tasks they solve in [20]. Most
closely related to the taxonomy developed here is a
taxonomy for LLM-powered multiagent architectures
[21] which focuses on autonomous agents with less
technical detail. Taxonomies of applications of AI in
enterprises [48] and applications of generative AI, in-
cluding but not limited to LLMs [52], are developed
using methods similar to those in our study.
Several taxonomies in the field of conversational
agents and task-oriented dialog (TOD) systems ad-
dress system architecture [1, 40, 12, 3]. However, they
omit detailed coverage of the integration of generative
language models.
4. Methods
We constructed the taxonomy following established
guidelines [42, 48, 29], drawing from a sample of
LLM-integrated applications. These applications are
detailed in section 4.1.
4.1. Development
Taxonomy. We derived an initial taxonomy from the
standard architecture of conversational assistants de-
scribed in [3], guided by the idea that conversational
assistants are essentially “chatbots with tools”, i.e.,
language-operated user interfaces that interact with
external systems. This approach proved unsuccessful.
The second version was based on the classical three-
tier software architecture, and then extended over
several development cycles.
By repeatedly apply-
ing the evolving taxonomy to the example instances,
we identified dimensions and characteristics using an
“empirical-to-conceptual” approach.
When new di-
mensions emerged, additional characteristics were de-
rived in a “conceptual-to-empirical” manner.
After
five major refinement cycles, the set of dimensions
and characteristics solidified. In the subsequent eval-
uation phase, we applied the taxonomy to a new set
of example instances that were not considered while
constructing the taxonomy. As the dimensions and
characteristics remained stable, the taxonomy was
considered complete. In the final phase, we refined
the wording and visual format of the taxonomy.
Visualization. Developing a taxonomy involves cre-
ating a representation that effectively supports its
intended purpose [29].
Taxonomies can be repre-
sented in various formats, with morphological boxes
[54, 55] or radar charts [21] being well-established
approaches. We evaluated morphological boxes, be-
cause they effectively position categorized instances
within the design space. However, we found that they
make it difficult to perceive a group of categorized in-
stances as a whole since they occupy a large display
area. This drawback is significant for our purposes,
as LLM-integrated applications often comprise mul-
tiple LLM components. Therefore, we developed a
more condensed visualization of the taxonomy based
on feature vectors.
Example instances. We searched for instances of
LLM-integrated applications for taxonomy develop-
ment that should meet the following criteria:
• The application aims for real-world use rather
than focusing on research only (such as testbeds
for experiments or proofs-of-concept). It demon-
strates efforts towards practical usability and ad-
dresses challenges encountered in real-world sce-
narios.
• The application’s architecture, particularly its
LLM components, is described in sufficient de-
tail for analysis.
• The sample of instances covers a diverse range
of architectures.
• The example instances are situated within indus-
trial or technical domains, as we aim to focus on
LLM-integrated applications beyond well-known
fields like law, medicine, marketing, human re-
sources, and education.
4
The search revealed a predominance of theoretical re-
search on LLM-integrated applications while papers
focusing on practically applied systems were scarce.
Searching non-scientific websites uncovered commer-
cially advertised AI-powered applications, but their
internal workings were typically undisclosed, and reli-
able evaluations were lacking. Furthermore, the het-
erogeneous terminology and concepts in this emerg-
ing field make a comprehensive formal literature
search unfeasible.
Instead, by repeatedly search-
ing Google Scholar and non-scientific websites using
terms “LLM-integrated applications”, “LLM-powered
applications”, “LLM-enhanced system”, “LLM” and
“tools”, along similar variants, we selected six suitable
instances. Some of them integrate LLMs in multiple
ways, totaling eleven distinct LLM components.
For a thorough evaluation, we selected new instances
using relaxed criteria, including those intended for
research. Additionally, we included a real-world ex-
ample lacking explicit documentation to broaden the
diversity of our sample and assess the taxonomy’s
coverage. Within the five selected instances, we iden-
tified ten LLM components.
4.2. Sample of LLM-integrated applications
Table 1 gives an overview of the sample. Names of ap-
plications and LLM components are uniformly writ-
ten as one CamelCase word and typeset in small caps,
deviating from the format chosen by the respective
authors.
Honeycomb. Honeycomb is an observability plat-
form collecting data from software applications in
distributed environments for monitoring.
Users
define queries to retrieve information about the
observed software systems through Honeycomb’s
Query Builder UI. The recently added LLM-based
QueryAssistant allows users to articulate inquiries
in plain English, such as “slow endpoints by status
code” or “which service has the highest latency?”
The QueryAssistant converts these into queries in
Honeycomb’s format, which users can execute and
manually refine [7, 8].
LowCode. LowCode is a web-based application
consisting of a prompt-definition section and a di-
alogue section.
The prompt-definition section sup-
ports the design of prompts for complex tasks, such
as composing extensive essays, writing resumes for
job applications or acting as a hotel service chatbot
[5]. In the dialogue section, users converse with an
LLM to complete the complex task based on the de-
fined prompt.
LowCode comprises two LLM components termed
Planning and Executing. Planning operates in
the prompt-definition section, where a user roughly
describes a complex task, and Planning designs a
workflow for solving it. The prompt-definition section
offers a low-code development environment where the
LLM-generated workflow is visualized as a graphi-
cal flowchart, allowing a user to edit and adjust the
logic of the flow and the contents of its steps. For
instance, in essay-writing scenarios, this involves in-
serting additional sections, rearranging sections, and
refining the contents of sections. Once approved by
the user, LowCode translates the modified work-
flow back into natural language and incorporates it
into a prompt for Executing. In the dialogue sec-
tion, users converse in interactive, multi-turn dia-
logues with Executing. As defined in the prompt, it
acts as an assistant for tasks such as writing an essay
or resume, or as a hotel service chatbot. While the
idea of the LLM planning a workflow might suggest
using the LLM for application control, LowCode
Planning actually serves as a prompt generator that
supports developing prompts for complex tasks.
MyCrunchGpt. MyCrunchGpt acts as an ex-
pert system within the engineering domain, specif-
ically for airfoil design and calculations in fluid me-
chanics. These tasks require complex workflows com-
prising several steps such as preparing data, param-
eterizing tools, and evaluating results, using vari-
ous software systems and tools.
The aim of My-
CrunchGpt is to facilitate the definition of these
workflows and automate their execution [28].
MyCrunchGpt offers a web interface featuring a
dialogue window for inputting commands in plain
English, along with separate windows displaying the
5
Table 1: Example instances selected for development (top 6) and evaluation (bottom 5)
Application
References
LLM components
Honeycomb
[7, 8]
QueryAssistant
LowCode
[5],[35]
Planning, Executing
MyCrunchGpt
[28]
DesignAssistant, SettingsEditor, DomainExpert
MatrixProduction
[69]
Manager, Operator
WorkplaceRobot
[37]
TaskPlanning
AutoDroid
[64]
TaskExecutor, MemoryGenerator
ProgPrompt
[51]
ActionPlanning, ScenarioFeedback
FactoryAssistants
[26]
QuestionAnswering
SgpTod
[71]
DstPrompter, PolicyPrompter
TruckPlatoon
[70]
Reporting
ExcelCopilot
[16, 44]
ActionExecutor, Advisor, IntentDetector, Explainer
output and results of software tools invoked by My-
CrunchGpt in the backend. MyCrunchGpt relies
on predefined workflows, not supporting deviations
or cycles. By appending a specific instruction to the
dialogue history in the prompt for each step of the
workflow, it uses the LLM as a smart parser to ex-
tract parameters for APIs and backend tools from
user input. APIs and tools are called in the prede-
fined order [28, p. 56].
MyCrunchGpt is still in development. The paper
[28] explains the domain as well as the integration of
the LLM, but does not fully detail the implementa-
tion of the latter. Still, MyCrunchGpt illustrates
innovative applications of an LLM in a technical do-
main. We categorize three LLM components solving
tasks within MyCrunchGpt: a DesignAssistant
guiding users through workflows and requesting pa-
rameters for function and API calls; a SettingsEd-
itor updating a JSON file with settings for a back-
end software tool; and a DomainExpert which helps
evaluating results by comparing them to related re-
sults, e.g., existing airfoil designs, which it derives
from its trained knowledge.
MatrixProduction. MatrixProduction
em-
ploys an LLM for controlling a matrix production
system [69].
While in a classical line production
setup, workstations are arranged linearly and the
manufacturing steps follow a fixed sequence, matrix
production is oriented towards greater flexibility.
Autonomous
transport
vehicles
carry
materials
and intermediate products to workstations, termed
automation modules, each offering a spectrum of
manufacturing skills that it can contribute to the
production process.
Compared to line production,
matrix production is highly adaptable and can
manufacture a variety of personalized products with
full automation. This requires intelligent production
management to (a) create workplans that orchestrate
and schedule the automation modules’ skills, and (b)
program the involved automation modules such that
they execute the required processing steps.
MatrixProduction incorporates two LLM compo-
nents: Manager creates workplans as sequences of
skills (a), while Operator generates programs for
the involved automation modules (b).
MatrixProduction prompts Manager and Op-
erator to provide textual explanations in addition
to the required sequences of skills or automation
module programs.
The LLM output is processed
by a parser before being used to control the physi-
cal systems. Manager relies on built-in production-
specific knowledge of the LLM such as “a hole is pro-
duced by drilling”.
Noteworthy in this approach is its tight integra-
tion into the system landscape of Industry 4.0.
The few-shot Manager and Operator prompts
are generated automatically using Asset Adminis-
tration Shells, which are standardized, technology-
6
independent data repositories storing digital twins of
manufacturing assets for use in Industry 4.0 [2].
WorkplaceRobot. An experimental robot system
is enhanced with LLM-based task planning in [37].
The robot operates in a workplace environment fea-
turing a desk and several objects. It has previously
been trained to execute basic operations expressed
in natural language such as “open the drawer” or
“take the pink object and place it in the drawer”.
LLM-based task planning enables the robot to per-
form more complex orders like “tidy up the work area
and turn off all the lights”. To this end, an LLM is
prompted to generate a sequence of basic operations
that accomplish the complex order.
Although the robot expects operations phrased in
natural language, the LLM is prompted with a
Python coding task. For instance, the basic opera-
tion “turn on the green light” corresponds to a Python
command push_button(’green’). The prompt for
the LLM includes several examples each consisting
of a description of an environment state, a complex
order formatted as a comment, and a sequence of
Python robot commands that accomplish the com-
plex order. When invoking the LLM to generate the
Python program for a new order, the prompt is aug-
mented with a description of the environment’s cur-
rent state and the new order as a comment.
The Python code produced by the LLM is trans-
lated back to a sequence of basic operations in nat-
ural language. When the robot executes these oper-
ations, there is no feedback about successful comple-
tion. Rather, the system assumes that all basic op-
erations require a fixed number of timesteps to com-
plete.
AutoDroid. The goal of mobile task automation is
hands-free user interaction for smartphones through
voice commands. AutoDroid is a voice control sys-
tem for smartphones that can automatically execute
complex orders such as “remind me to do laundry on
May 11th” or “delete the last photo I took” [64, 65].
Such complex orders are fulfilled by performing se-
quences of basic operations in an Android app, such
as “scroll down, then press button x” in the calen-
dar app. AutoDroid employs an LLM component
TaskExecutor to plan these sequences of opera-
tions. The challenge is that the next operation to ex-
ecute depends on the current state of the Android app
which continuously changes as the app is operated.
AutoDroid solves this by invoking the TaskEx-
ecutor repeatedly after each app operation with the
prompt comprising the updated state of the Graph-
ical User Interface (GUI) along with the user’s com-
plex order.
Before executing irrevocable operations, such as per-
manently deleting data or calling a contact, Auto-
Droid prompts the user to confirm or adjust the op-
eration. TaskExecutor is instructed to include a
“confirmation needed” hint in its output for such op-
erations.
The prompt for TaskExecutor comprises an ex-
tract from a knowledge base which is built automati-
cally in an offline learning phase as follows: In a first
step, a “UI Automator” (which is not an LLM com-
ponent) automatically and randomly operates the
GUI elements of an Android app to generate a UI
Transition Graph (UTG). The UTG has GUI states
as nodes and the possible transitions between GUI
states as edges. As next steps, AutoDroid invokes
two LLM components referred to as MemoryGen-
erators to analyze the UTG.
The first MemoryGenerator is prompted repeat-
edly for each GUI state in the UTG. Its task is to
explain the functionality of the GUI elements. Be-
sides instructions and examples of the table format
desired as output, its prompt includes an HTML rep-
resentation of the GUI state, the GUI actions preced-
ing this state, and the GUI element operated next.
Its output consists of tuples explaining the function-
ality of a GUI element by naming the derived func-
tionality (e.g., “delete all the events in the calendar
app”) and the GUI states and GUI element actions in-
volved. Similarly, the second MemoryGenerator
is prompted to output a table listing GUI states and
explanations of their functions. These tables consti-
tute AutoDroid’s knowledge base.
ProgPrompt. ProgPrompt [51] is an approach
to
LLM-based
robot
task
planning
similar
to
7
WorkplaceRobot.
Its robot is controlled by
Python code and works in a real and a simulated
household environment.
ProgPrompt comprises two LLM components. Ac-
tionPlanning generates Python scripts for tasks
such as “microwave salmon”
using basic opera-
tions like grab(’salmon’),
open(’microwave’),
and putin(’salmon’, ’microwave’), notably with-
out considering the current state of the environment.
To establish a feedback loop with the environment,
ActionPlanning adds assert statements. These
statements verify the preconditions of basic opera-
tions and trigger remedial actions when preconditions
are not met. For instance, a script for “microwave
salmon” comprises the following code fragment:
if assert(’microwave’ is ’opened’)
else:
open(’microwave’)
putin(’salmon’, ’microwave’)
When
operating
in
the
simulated
environment,
ProgPrompt
can
verify
an
assert
statement
through its second LLM component, Scenario-
Feedback. Prompted with the current state of the
environment and the assert statement, Scenario-
Feedback evaluates it and outputs True or False.
FactoryAssistants. FactoryAssistants advise
workers on troubleshooting production line issues in
two manufacturing domains: detergent production
and textile production [26]. The assistants leverage
domain knowledge from FAQs and documented prob-
lem cases to answer user queries. The required do-
main knowledge is provided as a part of the prompt.
SgpTod. SgpTod employs an LLM to implement a
chatbot, specifically, a task-oriented dialogue (TOD)
system [71]. TOD systems are also known as conver-
sational assistants. In contrast to open-domain dia-
logue (ODD) systems, which engage users in goalless
conversations, they are designed for assisting users in
specific tasks.
In general,
TOD systems require the following
components [3]:
Natural Language Understanding
(NLU), analyzing the user’s input to classify intents
and extract entities; Dialogue Management (DM) for
deciding on a system action that is appropriate in
a given dialogue state (e.g., ask for more informa-
tion or invoke a hotel booking service); and Natu-
ral Language Generation (NLG) for producing a re-
sponse that the TOD system can present to the user.
Intent classification, also known as intent detection,
matches free-text user input to one of several tasks a
TOD system can perform (e.g., book a hotel). Entity
extraction isolates situational values, called entities,
from the user input (e.g., the town and the date of
the hotel booking). The TOD system may require
several dialogue turns to elicit all necessary entities
from the user.
In TOD research, the system’s in-
ternal representation of the user’s intentions and the
entity values is commonly referred to as its “belief
state”. For example, in the restaurant search domain,
the belief state may include attribute-value pairs like
cuisine:Indian and pricerange:medium.
SgpTod is a multi-domain TOD system, concur-
rently handling multiple task domains found in stan-
dard TOD evaluation datasets, such as recommend-
ing restaurants or finding taxis. Similar to other ex-
perimental TOD systems [23], SgpTod accesses a
database that stores information from the task do-
mains, such as available hotels and restaurants.
SgpTod comprises two LLM components, called
DstPrompter and PolicyPrompter, that are
both invoked in every dialogue turn between SgpTod
and the user. The DstPrompter handles the NLU
aspect, analyzing the user’s input and populating the
system’s belief state.
It outputs is an SQL query
suited to extract the database entries that match the
current belief state. Upon retrieving the database en-
tries, SgpTod invokes its PolicyPrompter which
covers both DM and NLG. Prompted with the dia-
logue history and the database entries retrieved, it
produces a two-part output: a natural language re-
sponse for NLG and a system action for DM.
TruckPlatoon. The concept of truck platooning
means that trucks travel closely together for bet-
ter fuel efficiency and traffic flow.
TruckPla-
toon comprises an algorithmic control loop which
autonomously maintains a consistent distance be-
tween trucks. It invokes an LLM to generate natural-
language reports on the platoon’s performance and
8
stability from measurements tracked by the control
algorithm, providing easily understandable informa-
tion for engineers involved in monitoring and opti-
mizing the truck platooning system.
ExcelCopilot. ExcelCopilot is an example of
a recent trend where software companies integrate
LLM-based assistants, often termed “copilots”, into
their products [44]. These copilots not only provide
textual guidance but also perform actions within the
software environment, constituting a distinctive type
of LLM-integrated application.
We chose Excel-
Copilot as an example for evaluating our taxonomy.
Since its implementation is undisclosed, we infer its
architecture from indirect sources, including a screen-
cast and a report on insights and experiences from
copilot developers [16, 44]. This inferred architecture
may deviate from the actual implementation.
ExcelCopilot is accessible in a task bar along-
side the Excel worksheet. It features buttons with
context-dependent suggestions of actions and a text
box for users to type in commands in natural lan-
guage. ExcelCopilot only works with data tables,
so its initial suggestion is to convert the active work-
sheet’s data into a data table. Copilot functions ac-
tivate when a data table or part of it is selected. It
then presents buttons for four top-level tasks: “add
formula columns”, “highlight”, “sort and filter”, and
“analyze”. The “analyze” button triggers the copilot
to display more buttons, e.g., one that generates a
pivot chart from the selected data. ExcelCopilot
can also add a formula column to the data table and
explain the formula in plain language.
When a user inputs a free-text command, Excel-
Copilot may communicate its inability to fulfill
it. This constantly occurs with commands requiring
multiple steps, indicating that ExcelCopilot lacks
a planning LLM component as seen in, for example,
MatrixProduction. This observation, along with
its mention in [44], suggests that ExcelCopilot em-
ploys an intent detection-skill routing architecture.
This architecture includes an LLM component that
maps free-text user commands to potential intents
and then delegates to other LLM components tasked
with generating actions to fulfill those intents. Ac-
cordingly, ExcelCopilot comprises several types of
LLM components:
• Several distinct Action Executors generate
code for specific application actions, such as cre-
ating a pivot table, designing a worksheet for-
mula, inserting a diagram, and so on.
• An Advisor suggests meaningful next actions.
Its outputs serve to derive button captions and
prompts for ActionExecutors.
• When a user inputs a free-text command, the
IntentDetector is invoked to determine and
trigger a suitable ActionExecutor. The In-
tentDetector communicates its actions to
users and informs them when it cannot devise
a suitable action.
• The Explainer generates natural language ex-
planations of formulae designed by ExcelCopi-
lot. It is unclear whether under the hood, the
ActionExecutor is generating both the for-
mula and the explanation, or if two separate
LLM components are being invoked. We assume
the latter, i.e., that a separate Explainer LLM
component exists.
While users interact repeatedly with ExcelCopi-
lot, each interaction adheres to a single-turn pat-
tern, with the user providing a command and Ex-
celCopilot executing it [44].
5. A Taxonomy for LLM Components and
LLM-Integrated Applications
When developing the taxonomy, it emerged that an-
alyzing an LLM-integrated application should begin
with identifying and describing its distinct LLM com-
ponents. Analyzing each LLM component separately
helps capture details and provides a clear understand-
ing of how the application utilizes LLM capabili-
ties.
The LLM-integrated application can then be
described as a combination of the LLM components
it employs.
9
Table 2: Dimensions and characteristics of the taxonomy. Codes of characteristics are printed in uppercase. “Meta” means
“metadimension”. “MuEx” means “mutual exclusiveness”.
Meta
Dimension
Characteristics
MuEx
Invocation
Interaction
App, Command, Dialog
enforced
Frequency
Single, Iterative
yes
Function
Logic
cAlculate, Control
yes
UI
none, Input, Output, Both
yes
Data
none, Read, Write, Both
yes
Prompt
Instruction
none, User, LLM, Program
enforced
State
none, User, LLM, Program
enforced
Task
none, User, LLM, Program
yes
Check
none, User, LLM, Program
enforced
Skills
reWrite, Create, conVerse, Inform, Reason, Plan
no
Output
Format
FreeText, Item, Code, Structure
no
Revision
none, User, LLM, Program
enforced
Consumer
User, LLM, Program, Engine
enforced
5.1. Overview and demonstration
The taxonomy identifies 13 dimensions for LLM com-
ponents, grouped into five metadimensions as shown
in table 2. It comprises both dimensions with gen-
uinely mutually exclusive characteristics and those
with non-exclusive characteristics.
For dimensions
related to the technical integration of LLMs within
applications, mutual exclusiveness is enforced. Given
the open nature of software architecture, the inte-
gration of LLMs allows for significant diversity. In
practice, LLM components may show multiple char-
acteristics within these dimensions. Nonetheless, the
taxonomy requires categorizing each component with
a predominant characteristic, enforcing a necessary
level of abstraction to effectively organize and struc-
ture the domain.
We applied the taxonomy to categorize each of the
example instances described in section 4.2. The re-
sults are depicted in figure 1. The dimensions and
their characteristics are detailed and illustrated with
examples in section 5.2.
The taxonomy visualizes an LLM component by a
feature vector comprising binary as well as multi-
valued features. Non-mutually exclusive dimensions
are represented by a set of binary features. The re-
maining dimensions are encoded as n-valued features
where n denotes the number of characteristics. For
compactness, we use one-letter codes of the charac-
teristics as feature values in the visualizations.
In
table 2, these codes are printed in upper case in the
respective characteristic’s name.
A feature vector representing an LLM component
is visualized in one line. For dimensions with non-
mutually exclusive characteristics, all possible codes
are listed, with the applicable ones marked. The re-
maining dimensions are represented by the code of
the applicable characteristic, with the characteris-
tic none shown as an empty cell. We shade feature
values with different tones to support visual percep-
tion. LLM components within the same application
are grouped together, visualizing an LLM-integrating
application in a tabular format.
5.2. Dimensions and characteristics
5.2.1. Invocation dimensions
Two Invocation dimensions address the way the LLM
is invoked within the application.
Interaction describes how the user interacts with the
LLM with three characteristics:
App: Users never converse with the LLM directly
in natural language, rather the application invokes
the LLM automatically. E.g., users do not interact
10
Invocation
Function
Prompt
Skills
Out. Format
Output
z
}|
{
z
}|
{
z
}|
{
z
}|
{
z
}|
{
z }| {
Interaction
Frequency
Logic
UI
Data
Instruction
State
Task
Check
reWrite
Create
conVerse
Inform
Reason
Plan
FreeText
Item
Code
Structure
Revision
Consumer
Honeycomb QueryAssistant
C
S
A
R
P
P
U
P
P
C
P
E
LowCode Planning
C
S
A
P
U
I
P
S
U
L
LowCode Executing
D
I
A
B
P
L
U
C
V
I
F
U
MyGrunchGpt DesignAssistant
D
I
A
B
P
P
U
V
S
E
MyGrunchGpt SettingsEditor
C
S
A
P
P
P
W
C
E
MyGrunchGpt DomainExpert
C
S
A
P
P
P
I
F
U
MatrixProduction Manager
C
S
C
I
P
P
U
I
P
F
S
L
MatrixProduction Operator
A
S
C
P
P
L
P
F
S
E
WorkplaceRobot
C
S
C
I
P
P
U
P
C
E
AutoDroid Executor
C
I
C
I
P
L
U
P
P
I
S
E
AutoDroid MemoryGenerator2
A
I
A
P
P
P
R
S
L
ProgPrompt ActionPlanning
C
S
C
I
P
U
P
C
E
ProgPrompt ScenarioFeedback
A
I
C
P
P
L
R
I
E
FactoryAssistant
D
S
A
P
P
U
W
V
F
U
SgpTod DstPrompter
D
S
A
I
R
P
P
U
V
R
C
E
SgpTod PolicyPrompter
A
S
C
O
P
P
P
R
F
I
P
TruckPlatoon
A
S
A
O
P
P
P
W
F
U
ExcelCopilot ActionExecutor∗
A
S
A
P
P
L
P
F
C
E
ExcelCopilot Advisor
A
S
A
P
P
P
R
F
S
P
ExcelCopilot IntentDetector
C
S
C
P
P
U
R
I
S
P
ExcelCopilot Explainer
A
S
A
P
P
P
R
F
U
Figure 1: Categorized example instances. See table 2 for a legend. ∗, 2: multiple LLM components.
directly with ExcelCopilot ActionExecutor or
with MatrixProduction Operator.
Command:
Users input single natural language
commands.
E.g., users interact with AutoDroid
TaskExecutor through single natural language
commands.
Dialog: Users engage in multi-turn dialogues with the
LLM component to achieve a use goal. E.g., users
repeatedly prompt LowCode Executing or My-
CrunchGpt DesignAssistant in multi-turn dia-
logues to obtain an essay or an airfoil design, respec-
tively.
Frequency addresses how often the application in-
vokes a specific LLM component to fulfill a goal:
Single: A single invocation of an LLM component
is sufficient to produce the result.
E.g., in My-
CrunchGpt, the application internally invokes dis-
tinct LLM components once for each user input by
injecting varying prompt instructions.
Iterative: The LLM component is invoked repeatedly
to produce the result. E.g., AutoDroid TaskEx-
11
ecutor is invoked multiple times to fulfill a com-
mand with an updated environment description in
the State prompt; LowCode Executing is repeat-
edly prompted by the user to achieve the use goal
while the application updates the dialogue history.
5.2.2. Function dimensions
The Function dimensions are derived from the classi-
cal three-tier software architecture model which seg-
regates an application into three distinct layers: pre-
sentation, logic and data [17]. The presentation layer
implements the UI. On the input side, it allows users
to enter data and commands that control the appli-
cation. On the output side, it presents information
and provides feedback on the execution of commands.
The logic layer holds the code that directly realizes
the core objectives and processes of an application
such as processing data, performing calculations, and
making decisions. The data layer of an application
manages the reading and writing of data from and
to persistent data storage. Due to its versatility, an
LLM component can simultaneously implement func-
tionality for all three layers. The taxonomy addresses
this with three Function dimensions.
UI indicates whether an LLM component contributes
significantly to the user interface of an application,
avoiding the need to implement graphical UI controls
or display elements:
none: No UI functionality is realized by the LLM.
E.g., in ExcelCopilot, the LLM does not replace
any UI elements.
Input:
Input UI is (partially) implemented by
the LLM. E.g., in MatrixProduction Manager,
users input their order in natural language, obviating
a product configuration GUI.
Output: Output UI is (partially) implemented by the
LLM. E.g., in TruckPlatoon, the output gener-
ated by the LLM component can replace a data cock-
pit with gauges and other visuals displaying numeri-
cal data.
Both:
Input and output UI are (partially) imple-
mented by the LLM. E.g., in MyCrunchGpt, the
DesignAssistant provides a convenient conversa-
tional interface for parameterization of APIs and
tools and feedback on missing values, which other-
wise might require a complex GUI.
Logic indicates whether the LLM component deter-
mines the control flow of the application. It discerns
two characteristics:
cAlculate: The output does not significantly impact
the control flow of the application, i.e., the output
is processed like data. E.g., MyCrunchGpt Set-
tingsEditor modifies a JSON file, replacing a pro-
grammed function; MyCrunchGpt DesignAssis-
tant asks the user for parameters, but the sequence
of calling APIs and tools follows a predefined work-
flow; the workflow computed by LowCode Plan-
ning is displayed without influencing the applica-
tion’s control flow.
Control: The output of the LLM is used for con-
trolling the application.
E.g., the plans generated
by MatrixProduction Manager serve to sched-
ule and activate production modules; the actions pro-
posed by AutoDroid TaskExecutor are actually
executed and determine how the control flow of the
app proceeds.
Since an LLM invocation always computes a result,
cAlculate is interpreted as “calculate only”, making
cAlculate and Control mutually exclusive.
Data addresses whether the LLM contributes to read-
ing or writing persistent data:
none: The LLM does not contribute to reading or
writing persistent data.
This characteristic applies
to most sample instances.
Read: The LLM is applied for reading from persistent
data store. E.g., SgpTod DstPrompter generates
SQL queries which the application executes; Honey-
comb QueryAssistant devises analytical database
queries.
Write and Both:
No LLM component among the
samples generates database queries for creating or
updating persistent data.
5.2.3. Prompt-related dimensions
Integrating an LLM into an application poses spe-
cific requirements for prompts, such as the need for
prompts to reliably elicit output in the requested
12
form [68]. While a broad range of prompt patterns
have been identified and investigated [66], there is
still a lack of research on successful prompt pat-
terns specifically for LLM-integrated applications, on
which this taxonomy could build. Developing prompt
taxonomies is a challenging research endeavor in itself
[49] and is beyond the scope of this research. There-
fore, the taxonomy does not define a dimension with
specific prompt patterns as characteristics, but rather
focuses on how the application generates the prompt
for an LLM component from a technical perspective.
Prompts generally consist of several parts with dis-
tinct purposes, generated by different mechanisms.
Although many authors explore the concepts, a com-
mon terminology has yet to be established. This is
illustrated in table 3, showing terms from an ad-hoc
selection of recent papers addressing prompt gener-
ation in applications.
In the table, italics indicate
that the authors refrain from introducing an abstract
term and instead use a domain-specific description.
The term “examples” indicates a one-shot or few-shot
prompt pattern. The terms that are adopted for the
taxonomy are underlined.
The taxonomy distinguishes three prompt parts re-
ferred to as Prompt Instruction, Prompt State, and
Prompt Task. These parts can occur in any order,
potentially interleaved, and some parts may be ab-
sent.
• Instruction is the part of a prompt that outlines
how to solve the task. Defined during LLM com-
ponent development, it remains static through-
out an application’s lifespan.
• State is the situation-dependent part of the
prompt that is created dynamically every time
the LLM is invoked. The taxonomy opts for the
term State instead of “context” in order to avoid
confusion with the “LLM context” as explained
in section 2. The State may include the current
dialogue history, an extract of a knowledge base
needed specifically for the current LLM invoca-
tion, or a state or scene description, etc.
• Task is the part of the prompt conveying the
task to solve in a specific invocation.
Prompt Instruction, State and Task describe the ori-
gins of the prompt parts by uniform characteristics:
none: The prompt part is not present. E.g., Prog-
Prompt ActionPlanning has no State prompt,
nor does LowCode Planning (except the dialogue
history when planning a subprocess).
Instruction
and Task prompt parts are present in all sample in-
stances.
User: The user phrases the prompt part. E.g., the
Task for ExcelCopilot IntentDetector or for
LowCode Planning is phrased by the user. There
are no sample instances where the user provides the
Instruction or State prompt parts.
LLM : The prompt part is generated by an LLM. E.g.,
LowCode Planning generates the State for Low-
Code Executing and ExcelCopilot IntentDe-
tector generates the Task for ExcelCopilot Ac-
tionExecutors.
Program:
Application code generates the prompt
part. E.g., AutoDroid programmatically generates
the State and the Task parts for its MemoryGen-
erators in the knowledge base building phase.
The Prompt Instruction dimension is always gener-
ated by Program. While a user and possibly an LLM
have defined this prompt part during application de-
velopment, this falls outside the scope of this taxon-
omy. Therefore, the Prompt Instruction dimension is
not discriminating and categorizes all cases as Pro-
gram. It is retained in the taxonomy for completeness
and better understandability.
Prompt Check describes whether the application em-
ploys a review mechanism to control and modify the
prompt before invoking the LLM. The same charac-
teristics as for the prompt parts are applicable:
none: The prompt is used without check.
User: The user checks and revises the prompt.
LLM : Another LLM component checks or revises the
prompt.
Program: The application comprises code to check
or revise the prompt.
E.g., AutoDroid removes
personal data, such as names, to ensure privacy
before invoking the TaskExecutor; Honeycomb
QueryAssistant incorporates a coded mechanism
against prompt injection attacks.
13
Table 3: Terms used for prompt parts. Expressions specific to a domain are printed in italics, “examples” indicates a one-shot
or few-shot prompt pattern. Terms adopted for the taxonomy are underlined.
Source
Instruction
State
Task
[72]
task description + examples
test instance
[34]
instruction prompt
data prompt
[32]
predefined prompt
user prompt
[45]
prompt template + examples
DB schema
user input question
[45]
examples
SQL query result
[37]
prompt context, i.e., examples
environment state, scene
description
input task commands
[5]
education prompt
dialogue history
user input task prompt
[5]
education prompt
dialogue history + provided
workflow
(circumscribed)
[69]
role and goal + instruction + examples
context
current task
[26]
predefined system instruction +
domain-specific information
query results from
knowledge graph
the user’s request
Most example instances omit prompt checks. There
are no examples where a Check is performed by a
User or an LLM.
5.2.4. Skills dimensions
The Skills dimension captures the types of LLM ca-
pabilities that an application utilizes. It is designed
as a dimension with six non-mutually exclusive char-
acteristics.
Skills is decomposed into six specific capabilities:
reWrite:
The LLM edits or transforms data or
text, such as rephrasing, summarizing, reformat-
ting, correcting, or replacing values.
E.g., My-
CrunchGpt SettingsEditor replaces values in
JSON files; TruckPlatoon converts measurements
into textual explanations.
Create:
The LLM generates novel output.
E.g.,
LowCode Executing generates substantial bodies
of text for tasks like essay writing.
conVerse: The application relies on the LLM’s capa-
bility to engage in purposeful dialogues with humans.
E.g., MyCrunchGpt DesignAssistant asks users
for missing parameters; SgpTod PolicyPrompter
decides how to react to user inputs and formulates
chatbot responses.
Inform: The application depends on knowledge that
the LLM has acquired during its training, unlike
applications that provide all necessary information
within the prompt. E.g., MyCrunchGpt Domain-
Expert provides expert knowledge on airfoil designs;
MatrixProduction relies on built-in knowledge of
production processes, such as “a hole is produced
by drilling”; LowCode Executing uses its learned
knowledge for tasks like essay writing.
Reason: The LLM draws conclusions or makes log-
ical inferences.
E.g., FormulaExplainer in Ex-
celCopilot explains the effects of Excel functions
in formulas; AutoDroid MemoryGenerators ex-
plain the effects of GUI elements in Android apps.
Plan: The LLM designs a detailed method or course
of action to achieve a specific goal.
E.g., Au-
toDroid TaskExecutor and WorkplaceRobot
TaskPlanning devise action plans to achieve goals.
The Plan and Reason characteristics are interrelated,
as planning also requires reasoning.
The intended
handling of these characteristics is to categorize an
LLM component as Plan only and understand Plan
as implicitly subsuming Reason.
The effectiveness of LLMs as components of software
applications relies on their commonsense knowledge
and their ability to correctly interpret and handle a
broad variety of text inputs, including instructions,
14
examples, and code. It is reasonable to assume that a
fundamental capability, which might be termed Un-
terstand, is leveraged by every LLM component. As
it is not distinctive, the taxonomy does not list it
explicitly in the Skills dimension.
Applying this taxonomy dimension requires users to
determine which skills are most relevant and worth
highlighting in an LLM component. Given the versa-
tility of LLMs, reducing the focus to few predominant
skills is necessary to make categorizations distinctive
and expressive.
5.2.5. Output-related dimensions
Output Format characterizes the format of the LLM’s
output. As an output may consist of several parts in
diverse formats, this dimension is designed as non-
mutually exclusive, same as the Skills dimension. It
distinguishes four characteristics that are distinctive
and well discernible:
FreeText: unstructured natural language text out-
put.
E.g., TruckPlatoon and MyCrunchGpt
DomainExpert generate text output in natural lan-
guage; MatrixProduction Manager and Ma-
trixProduction Operator produce FreeText ex-
planations complementing output in custom formats
to be parsed by the application.
Item:
a single text item from a predefined set of
items, such as a class in a classification task. E.g.,
ProgPrompt ScenarioFeedback outputs either
True or False.
Code: source code or other highly formalized output
that the LLM has learned during its training, such
as a programming language, XML, or JSON. E.g.,
AutoDroid TaskExecutor produces code to steer
an Android app; MyCrunchGpt SettingsEditor
outputs JSON.
Structure: structured, formalized output adhering to
a custom format. E.g., LowCode Planning out-
puts text in a format that can be displayed as a flow
chart; MatrixProduction Manager and Oper-
ator produce output in custom formats combined
with FreeText explanations.
Output Revision indicates whether the application
checks or revises the LLM-generated output before
utilization. These characteristics and their interpre-
tations mirror those in the Prompt Check dimension:
none: There is no revision of the LLM output.
User:
The user revises the LLM output.
E.g.,
the user improves the plan generated by LowCode
Planning.
LLM : A further LLM component checks or revises
the output of the LLM component under considera-
tion.
Program:
Programmed code checks or revises the
LLM output. E.g., Honeycomb QueryAssistant
corrects the query produced by the LLM before exe-
cuting it [7].
There are no instances in the sample set where an-
other LLM revises or checks the output of the LLM.
Most sample applications do not check or revise the
LLM’s output, though several of them parse and
transform it.
The purpose of the Output Revision
dimension is to indicate whether the application in-
cludes control or correction mechanisms, rather than
just parsing it.
Output Consumer addresses the way of utilizing the
LLM output:
User signifies that the LLM output is presented to
a human user. E.g., the text output of TruckPla-
toon is intended for humans, as well as the output
of MyCrunchGPT DomainExpert.
LLM indicates that the output serves as a prompt
part in a further LLM invocation. E.g., the knowl-
edge base entries generated by an AutoDroid Mem-
oryGenerator become part of the prompt for
AutoDroid TaskExecutor; the plan output by
LowCode Planning serves as a part of the prompt
for LowCode Executing.
Program describes instances where the LLM output
is consumed and processed further by a software com-
ponent of the application. E.g., the output of Ma-
trixProduction Manager is handled by software
systems (including a Manufacturing Execution Sys-
tem) which use it to compute prompts for other LLM
components.
Engine covers scenarios where the LLM output is in-
tended for execution on a runtime engine. E.g., the
SQL query generated by SgpTod DstPrompter is
15
processed by a SQL interpreter; a part of the output
of MatrixProduction Operator is executed by
automation modules.
Although applications may parse and transform the
LLM output before use, the Output Consumer di-
mension is meant to identify the ultimate consumer,
such as an execution engine, rather than an interme-
diary parser or transformation code. When applica-
tions divide the LLM output into parts for different
consumers, users applying the taxonomy need to de-
termine which consumer is most relevant, since this
dimension is designed to be mutually exclusive.
5.3. Evaluation
Figure 2 displays the number of occurrences of char-
acteristics within the example instances.
It must
be noted, however, that these do not reflect actual
frequencies, as similar LLM components within the
same application are aggregated together, indicated
by symbols ∗and 2 in figure 1. Furthermore, Ex-
celCopilot likely includes occurrences of Prompt
Check and Output Revision which are not counted
due to insufficient system documentation.
We evaluate the taxonomy against commonly ac-
cepted quality criteria: comprehensiveness, robust-
ness, conciseness, mutual exclusiveness, explanatory
power, and extensibility [58, 42].
The taxonomy
encompasses all example instances including those
that were not considered during its development.
This demonstrates comprehensiveness. As figure 1
shows, all example instances have unique categoriza-
tions, supporting the taxonomy’s robustness. This
not only indicates that the dimensions and charac-
teristics are distinctive for the domain, but also high-
lights the wide variety possible in this field. Concise-
ness demands that the taxonomy uses the minimum
number of dimensions and characteristics. The tax-
onomy gains conciseness by identifying relatively few
and abstract characteristics within each dimension.
However, it does not adhere to the related subcri-
terion that each characteristic must be present in at
least one investigated instance [54]. Unoccupied char-
acteristics are retained for dimensions whose char-
acteristics were derived conceptually, specifically, for
the Prompt dimensions, the Output Revision dimen-
sion, and the Data Function dimension, enhancing
the taxonomy’s ability to illustrate design options
and inspire novel uses for LLM integrations in ap-
plications. Some dimensions are constructed in par-
allel, sharing common sets of characteristics. While
this affects conciseness, it makes the taxonomy easier
to understand and apply.
As is often seen in tax-
onomy development [54], we deliberately waived the
requirement for mutual exclusiveness for some di-
mensions, specifically the Output Format and Skills
dimensions. In the context of this taxonomy, these
can equivalently be understood as a set of of six
and four binary dimensions respectively, each divided
into characteristics “yes” and “no”. However, framing
them as a single dimension with non-mutually exclu-
sive characteristics seems more intuitive.
Metadimensions structure the taxonomy, and most
of the characteristics are illustrated through exam-
ples. These measures are recognized for enhancing
the explanatory power of a taxonomy [58]. The
taxonomy’s flat structure allows for the easy addition
of dimensions and characteristics, indicating that its
extensibility is good. Potential extensions and fur-
ther aspects of the taxonomy, including its usefulness
and ease of use, are discussed in section 6.
We visualize the taxonomy (or, strictly speaking, cat-
egorized instances) in a compact form using feature
vectors with characteristics abbreviated to single-
letter codes.
This approach has a drawback, as
it requires referencing a legend. Additionally, non-
applicable characteristics in mutually exclusive di-
mensions are not visible, which means the design
space is not completely shown. However, the com-
pactness of the representation allows LLM compo-
nents within a common application to be grouped
closely, so that an LLM-integrated application can
be perceived as a unit without appearing convoluted.
This is a significant advantage for our purposes.
6. Discussion
The discussion first focuses on the taxonomy’s appli-
cability and ease of use before considering its overall
usefulness.
16
Invocation
Function
Prompt
Output
Output
z
}|
{
z
}|
{
z
}|
{
Skills
Format
z
}|
{
Inter.
Freq. Logic
UI
Data
Instr.
State
Task
Check
z
}|
{
z }| { Revision Consumer
A C D I
S C A
I O B R W B U L P U L P U L P U L P W C V I R P F I C S U L P U L P E
8 9 4
5 16 8 13 5 2 2
2 0 0
0 0 21 0 2 17 11 3 7
0 0 2
3 1 4 4 7 8 10 4 6 8
1 0
1
5 3 3 10
Figure 2: Occurrences of characteristics in the sample set of LLM-integrated applications.
6.1. Applicability and ease of use
The taxonomy was effectively applied to LLM-
integrated applications based on research papers,
source code blog posts, recorded software demonstra-
tions, and developer experiences.
The analysis of
LowCode revealed it to be a prompt definition tool
combined with an LLM-based chatbot, which devi-
ates from the strict definition of an LLM-integrated
application. Still, the taxonomy provided an effective
categorization and led to a clear understanding of the
system’s architecture.
Obviously, the ease of categorization depends on the
clarity and comprehensiveness of the available infor-
mation, which varies across analyzed systems. An-
alyzing applications of LLMs in novel and uncom-
mon domains can be challenging. While these papers
present inspiring and innovative ideas for LLM inte-
gration, such as MyCrunchGpt and TruckPla-
toon, they may prioritize explaining the application
area and struggle to detail the technical aspects of the
LLM integration. A taxonomy for LLM-integrated
applications can guide and facilitate the writing pro-
cess and lead to more standardized and comparable
descriptions.
Applying the taxonomy is often more straightforward
for research-focused systems.
Omitting the com-
plexities required for real-world applications, such as
prompt checks and output revisions, their architec-
tures are simpler and easier to describe. A taxonomy
can point out such omissions.
A fundamental challenge in applying the taxonomy
arises from the inherent versatility of LLMs, which
allows to define LLM components serving multiple
purposes.
This is exemplified by SgpTod Poli-
cyPrompter, where the prompt is designed to pro-
duce a structure with two distinct outcomes (a class
label and a chatbot response), and similarly by Ma-
trixProduction, as detailed section 4.2.
Draw-
ing an analogy to “function overloading” in classical
programming, such LLM components can be termed
“overloaded LLM components”.
A taxonomy can handle overloaded LLM components
in several ways: (1) define more dimensions as non-
mutually exclusive, (2) label overloaded LLM compo-
nents as “overloaded” without a more detailed catego-
rization, or (3) categorize them by their predominant
purpose or output. While the first approach allows
for the most precise categorization, it complicates the
taxonomy. Moreover, it will likely result in nearly all
characteristics being marked for some LLM compo-
nents, which is ultimately not helpful. The second
approach simplifies categorization but sacrifices much
detail. Our taxonomy adopts the third approach, en-
forcing simplification and abstraction in descriptions
of overloaded LLM components while retaining es-
sential detail. The taxonomy can easily be extended
to include approach (2) as an additional binary di-
mension.
6.2. Usefulness
The search for instances of LLM-integrated appli-
cations uncovered activities across various domains.
Substantial research involving LLM integrations, of-
ten driven by theoretical interests, is notable in robot
task planning [37, 51, 61, 33, 63] and in the TOD
field [23, 71, 4, 6, 56]. Research exploring LLM po-
tentials from a more practical perspective can be
found in novel domains, such as industrial produc-
tion [69, 26] and other technical areas [28, 70]. Fur-
17
thermore, developers of commercial LLM-based ap-
plications are beginning to communicate their efforts
and challenges [44, 7]. The taxonomy has been ap-
plied to example instances from these and additional
areas. This demonstrates its potential as a common,
unified framework for describing LLM-integrated ap-
plications, facilitating the comparison and sharing
of development knowledge between researchers and
practitioners across various domains.
When applying the taxonomy to the example in-
stances, it proved to be effective and useful as an
analytical lens. Descriptions of LLM-integrated ap-
plications commonly explain background information
and details of the application domain in addition to
its LLM integration.
When used as an analytical
lens, the taxonomy quickly directs the analysis to-
wards the aspects of LLM integration, abstracting
from the specificities of the domain.
The taxonomy describes how LLM capabilities can be
leveraged in software systems, offers inspiration for
LLM-based functions, and outlines options for their
implementation as follows. The Skills dimension out-
lines the range of capabilities an LLM can contribute
to an application through a concise set of characteris-
tics, while the Function dimension suggests potential
uses, further supported by the Interaction dimension.
The Output Type dimension indicates options for en-
coding the output of an LLM in formats beyond plain
text, making it processable by software. The Output
Consumer dimension illustrates the diverse ways to
utilize or act upon LLM output. Thus, the taxonomy,
as intended, spans a design space for LLM integra-
tions.
The sampled LLM-integrated applications showcase
the creativity of researchers and developers in ap-
plying and exploiting the potentials of LLMs, rang-
ing from straightforward solutions (e.g., TruckPla-
toon) to highly sophisticated and technically com-
plex ones (e.g., AutoDroid). When using the tax-
onomy to inspire innovative uses of LLMs, we recom-
mend supplementing it with descriptions of example
applications to enhance its illustrativeness. The char-
acteristics of the Skills dimension are derived prag-
matically from the investigated example instances.
While they do not claim to be exhaustive or deeply
rooted in LLM theory or cognitive science, they add
relevant details to the categorizations and illustrate
design options and potentials for using LLMs as soft-
ware components.
It emerged as a key insight of this research that,
rather than analyzing an LLM-integrated application
in whole, analysis should start with the identifica-
tion and description of its distinct LLM components.
This is essential for gaining a clear understanding of
how the application utilizes the capabilities of LLMs.
The LLM-integrated application then manifests as a
combination of its LLM components. As shown in fig-
ure 1, the visualization effectively displays both the
quantity and the variety of LLM components in an
LLM-integrated application.
LLM components interact through prompt chaining,
where one LLM component’s output feeds into an-
other’s input [67]. When an LLM-integrated applica-
tion involves such an interaction, the taxonomy rep-
resents it as an LLM characteristic within a Prompt
dimension. The taxonomy can capture the variance
in these interactions. For instance, in AutoDroid
TaskExecutor and LowCode Executing, the
LLM characteristic appears in the Prompt State di-
mension, because their prompt components (knowl-
edge base excerpts and prompt definition, respec-
tively) are generated by other LLM components in a
preparatory stage. In contrast, the LLM character-
istic appears in the Prompt Task dimension for Ma-
trixProduction Operator, because its prompt
part is generated individually by the MatrixPro-
duction Manager almost immediately before use.
Taxonomy
dimensions
that
cover
entire
LLM-
integrated applications may be useful. Given their
complexity, these dimensions should be designed
based on a broader range of examples, which will only
become available as more LLM-integrated applica-
tions are developed and their architectures disclosed
in the future.
Extensions to the taxonomy could
also include dimensions for describing the structure
of prompts in more detail, as well as dimensions ad-
dressing characteristics of the language models used.
18
Table 4: LLM usage in the sample instances. “Evals” indicates evaluations of various LLMs.
Application
Used or best LLM
Evals
Comments
Honeycomb
GPT-3.5
yes
GPT-4 far too slow
LowCode
GPT-3.5-turbo
MyCrunchGpt
GPT-3.5
then awaiting the publication of GPT-4
MatrixProduction
text-davinci-003
WorkplaceRobot
GPT-3
AutoDroid
GPT-4
yes
GPT-4 best for tasks requiring many steps
ProgPrompt
GPT-3
CODEX better, but access limits prohibitive
FactoryAssistants
GPT-3.5
SgpTod
GPT-3.5
yes
GPT-3.5 best more often than others combined
TruckPlatoon
GPT-3.5-turbo
ExcelCopilot
N/A
combined LLMs in Copilot for Microsoft 365 [43]
7. Conclusion
This paper investigates the use of LLMs as soft-
ware components. Its perspective differs from cur-
rent software engineering research, which investigates
LLMs as tools for software development [14, 22] and
from research examining LLMs as autonomous agents
[11, 62, 57, 21]. This paper defines the concept of an
LLM component as a software component that re-
alizes its functionality by invoking an LLM. While
LLM components implicitly appear in various works,
termed, for example, “prompters”, “prompted LLM”,
“prompt module”, or “module” [30, 71, 6, 7], to our
knowledge, this concept has not yet been formalized
or systematically investigated.
The main contribution of this study is a taxonomy
for the analysis and description of LLM components,
extending to LLM-integrated applications by charac-
terizing them as combinations of LLM components.
In addition to the dimensions and characteristics of
the taxonomy, the study contributes a taxonomy vi-
sualization based on feature vectors, which is more
compact than the established visualizations such as
morphological boxes [55] or radar charts. It repre-
sents an LLM-integrated application as one visual en-
tity in a tabular format, with its LLM components
displayed as rows.
The taxonomy was constructed using established
methods, based on a set of example instances, and
evaluated with a new set of example instances. The
combined samples exhibit broad variation along the
identified dimensions. For some instances, informa-
tion was not available, necessitating speculative in-
terpretation. However, since the sample is used for
identifying options rather than quantitative analysis,
this issue and the representativeness of the sample
are not primary concerns. The evaluation was con-
ducted by the developer of the taxonomy, consistent
with recent related work [21, 52, 48]. Using a new
sample for evaluation strengthens the validity of the
results.
A further significant contribution of the paper is a
systematic overview of a sample of LLM-integrated
applications across various industrial and technical
domains, illustrating a spectrum of conceptual ideas
and implementation options.
As the examples show, LLM components can re-
place traditionally coded functions in software sys-
tems and enable novel use cases. However, practi-
cal challenges persist.
Developers report that new
software engineering methods are required, e.g., for
managing prompts as software assets and for test-
ing and monitoring applications. For instance, the
costs of LLM invocations prohibit the extensive au-
tomated testing that is standard in software devel-
opment practice [44, 7]. Challenges also arise from
the inherent indeterminism and uncontrollability of
LLMs. Small variations in prompts can lead to differ-
ences in outputs, while automated output processing
19
in LLM-integrated applications requires the output
to adhere to a specified format.
Furthermore,
the
deployment
mode
of
LLMs,
whether local (on the same hardware as the ap-
plication) or remote, managed privately or offered
as Language-Models-as-a-Service (LMaaS), has im-
pact on performance and usability. Table 4 gives an
overview of the LLMs used in our sample of appli-
cations.
Where papers report evaluations of mul-
tiple LLMs, the table displays the chosen or best-
performing LLM. Although not representative, the
table provides some insights.
LMaaS dominates,
likely due to its convenience, but more importantly,
due to the superior performance of the provided
LLMs.
Concerns regarding LMaaS include privacy, as sensi-
tive data might be transmitted to the LLM through
the prompt [64], and service quality, i.e., reliability,
availability, and costs. Costs typically depend on the
quantity of processed tokens. This quantity also af-
fects latency, which denotes the processing time of
an LLM invocation. A further important factor for
latency is the size of the LLM, with larger models
being slower [7].
When building LLM-based applications for real-
world use, the reliability and availability of an LMaaS
are crucial.
Availability depends not only on the
technical stability of the service, but also on factors
such as increased latency during high usage periods
or usage restrictions imposed by the provider of an
LMaaS, as reported for ProgPrompt [51]. Beyond
technical aspects, the reliability of an LMaaS also en-
compasses its behavior. For instance, providers might
modify a model to enhance its security, potentially
impacting applications that rely on it.
Despite practical challenges, integrating LLMs into
systems has the potential to alter the way software
is constructed and the types of systems that can be
realized. Prompts are central to the functioning of
LLM components which pose specific requirements
such as strict format adherence. Therefore, an im-
portant direction for future research will be prompt
engineering specifically tailored for LLM-integrated
applications.
In future work, the taxonomy will be extended to
distinguish finer-grained parts of prompts, allowing a
more detailed description and comparison of prompts
and related experimental results. Initial studies share
results on the format-following behavior of LLMs [68]
as a subtopic of instruction-following [73], derived
with synthetic benchmark data.
It is necessary to
complement their results with experiments using data
and tasks from real application development projects
because, in the early stages of this field, synthetic
benchmarks may fail to cover relevant aspects within
the wide range of possible options. Another crucial
research direction involves exploring how LLM char-
acteristics correspond to specific tasks, such as de-
termining the optimal LLM size for intent detection
tasks. The taxonomy developed in this study can sys-
tematize such experiments and their outcomes. Ad-
ditionally, it provides a structured framework for de-
lineating design choices in LLM components, making
it a valuable addition to future training materials.
Acknowledgements
Special thanks to Antonia Weber and Constantin We-
ber for proofreading and providing insightful and con-
structive comments.
References
[1] Eleni Adamopoulou and Lefteris Moussiades. An
Overview of Chatbot Technology. In Ilias Ma-
glogiannis, Lazaros Iliadis, and Elias Pimeni-
dis, editors, Artificial Intelligence Applications
and Innovations, IFIP Advances in Information
and Communication Technology, pages 373–383,
Cham, 2020. Springer International Publishing.
doi:10.1007/978-3-030-49186-4_31.
[2] Sebastian Bader, Erich Barnstedt, Heinz Be-
denbender, Bernd Berres, Meik Billmann, and
Marko Ristin.
Details of the asset adminis-
tration shell-part 1: The exchange of informa-
tion between partners in the value chain of in-
dustrie 4.0 (version 3.0 rc02). Working Paper,
Berlin: Federal Ministry for Economic Affairs
20
and Climate Action (BMWK), 2022. doi.org/
10.21256/zhaw-27075.
[3] Marcos Baez, Florian Daniel, Fabio Casati, and
Boualem Benatallah. Chatbot integration in few
patterns. IEEE Internet Computing, pages 1–1,
2020. doi:10.1109/MIC.2020.3024605.
[4] Tom
Bocklisch,
Thomas
Werkmeister,
Daksh Varshneya, and Alan Nichol.
Task-
Oriented
Dialogue
with
In-Context
Learn-
ing.
(arXiv:2402.12234),
February
2024.
doi:10.48550/arXiv.2402.12234.
[5] Yuzhe Cai, Shaoguang Mao, Wenshan Wu, Ze-
hua Wang, Yaobo Liang, Tao Ge, Chenfei Wu,
Wang You, Ting Song, Yan Xia, Jonathan Tien,
and Nan Duan.
Low-code LLM: Visual Pro-
gramming over LLMs. (arXiv:2304.08103), April
2023. doi:10.48550/arXiv.2304.08103.
[6] Lang Cao. DiagGPT: An LLM-based Chatbot
with Automatic Topic Management for Task-
Oriented Dialogue. (arXiv:2308.08043), August
2023. doi:10.48550/arXiv.2308.08043.
[7] Phillip
Carter.
All
the
Hard
Stuff
No-
body
Talks
About
When
Building
Prod-
ucts
with
LLMs.
Honeycomb,
May
2023.
https://www.honeycomb.io/blog/
hard-stuff-nobody-talks-about-llm.
[8] Phillip Carter.
So We Shipped an AI Prod-
uct. Did It Work?
Honeycomb,
Octo-
ber 2023.
https://www.honeycomb.io/blog/
we-shipped-ai-product.
[9] Banghao
Chen,
Zhaofeng
Zhang,
Nicolas
Langrené,
and
Shengxin
Zhu.
Unleash-
ing the potential of prompt engineering in
Large Language Models:
A comprehensive
review.
(arXiv:2310.14735),
October 2023.
doi:10.48550/arXiv.2310.14735.
[10] Wang Chen, Yan-yi Liu, Tie-zheng Guo, Da-
peng Li,
Tao He,
Li Zhi,
Qing-wen Yang,
Hui-han Wang,
and Ying-you Wen.
Sys-
tems
engineering
issues
for
industry
appli-
cations of large language model.
Applied
Soft Computing,
151:111165,
January 2024.
doi:10.1016/j.asoc.2023.111165.
[11] Yuheng Cheng, Ceyao Zhang, Zhengwen Zhang,
Xiangrui Meng, Sirui Hong, Wenhao Li, Zihao
Wang, Zekai Wang, Feng Yin, Junhua Zhao, and
Xiuqiang He. Exploring Large Language Model
based Intelligent Agents: Definitions, Methods,
and Prospects.
(arXiv:2401.03428), January
2024. doi:10.48550/arXiv.2401.03428.
[12] Silvia
Colabianchi,
Andrea
Tedeschi,
and
Francesco Costantino.
Human-technology in-
tegration with industrial conversational agents:
A conceptual architecture and a taxonomy for
manufacturing.
Journal of Industrial Infor-
mation Integration, 35:100510, October 2023.
doi:10.1016/j.jii.2023.100510.
[13] Jonathan Evertz, Merlin Chlosta, Lea Schön-
herr, and Thorsten Eisenhofer.
Whispers in
the Machine: Confidentiality in LLM-integrated
Systems.
(arXiv:2402.06922), February 2024.
doi:10.48550/arXiv.2402.06922.
[14] Angela Fan, Beliz Gokkaya, Mark Harman,
Mitya Lyubarskiy, Shubho Sengupta, Shin Yoo,
and Jie M. Zhang.
Large Language Models
for Software Engineering:
Survey and Open
Problems. (arXiv:2310.03533), November 2023.
doi:10.48550/arXiv.2310.03533.
[15] Wenqi Fan, Zihuai Zhao, Jiatong Li, Yunqing
Liu, Xiaowei Mei, Yiqi Wang, Zhen Wen, Fei
Wang, Xiangyu Zhao, Jiliang Tang, and Qing
Li. Recommender Systems in the Era of Large
Language Models (LLMs). (arXiv:2307.02046),
August 2023. doi:10.48550/arXiv.2307.02046.
[16] David Fortin.
Microsoft Copilot in Excel:
What It Can and Can’t Do.
YouTube, Jan-
uary 2024. https://www.youtube.com/watch?
v=-fsu9IXMZvo.
[17] Martin Fowler. Patterns of Enterprise Applica-
tion Architecture. 2002. ISBN 978-0-321-12742-
6.
21
[18] Shirley Gregor. The nature of theory in infor-
mation systems. MIS quarterly, pages 611–642,
2006. doi:10.2307/25148742.
[19] Yanchu Guan, Dong Wang, Zhixuan Chu, Shiyu
Wang, Feiyue Ni, Ruihua Song, Longfei Li, Jin-
jie Gu, and Chenyi Zhuang.
Intelligent Vir-
tual Assistants with LLM-based Process Au-
tomation. (arXiv:2312.06677), December 2023.
doi:10.48550/arXiv.2312.06677.
[20] Muhammad Usman Hadi,
Qasem Al Tashi,
Rizwan Qureshi, Abbas Shah, Amgad Muneer,
Muhammad Irfan, Anas Zafar, Muhammad Bi-
lal Shaikh, Naveed Akhtar, Jia Wu, and Seyedali
Mirjalili. Large Language Models: A Compre-
hensive Survey of its Applications, Challenges,
Limitations, and Future Prospects, September
2023. doi:10.36227/techrxiv.23589741.v3.
[21] Thorsten
Händler.
A
Taxonomy
for
Au-
tonomous LLM-Powered Multi-Agent Architec-
tures:.
In Proceedings of the 15th Interna-
tional Joint Conference on Knowledge Discov-
ery,
Knowledge Engineering and Knowledge
Management, pages 85–98, Rome, Italy, 2023.
SCITEPRESS - Science and Technology Publi-
cations. doi:10.5220/0012239100003598.
[22] Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang,
Kailong Wang, Li Li, Xiapu Luo, David Lo, John
Grundy, and Haoyu Wang.
Large Language
Models for Software Engineering: A Systematic
Literature Review. (arXiv:2308.10620), Septem-
ber 2023. doi:10.48550/arXiv.2308.10620.
[23] Vojtěch Hudeček and Ondrej Dusek.
Are
Large Language Models All You Need for Task-
Oriented Dialogue?
In Svetlana Stoyanchev,
Shafiq Joty, David Schlangen, Ondrej Dusek,
Casey Kennington, and Malihe Alikhani, edi-
tors, Proceedings of the 24th Annual Meeting of
the Special Interest Group on Discourse and Di-
alogue, pages 216–228, Prague, Czechia, Septem-
ber 2023. Association for Computational Lin-
guistics. doi:10.18653/v1/2023.sigdial-1.21.
[24] Kevin Maik Jablonka, Qianxiang Ai, Alexander
Al-Feghali, Shruti Badhwar, Joshua D. Bocarsly,
Andres M. Bran, Stefan Bringuier, Catherine L.
Brinson, Kamal Choudhary, Defne Circi, Sam
Cox, Wibe A. de Jong, Matthew L. Evans, Nico-
las Gastellu, Jerome Genzling, María Victoria
Gil, Ankur K. Gupta, Zhi Hong, Alishba Im-
ran, Sabine Kruschwitz, Anne Labarre, Jakub
Lála, Tao Liu, Steven Ma, Sauradeep Majum-
dar, Garrett W. Merz, Nicolas Moitessier, Elias
Moubarak, Beatriz Mouriño, Brenden Pelkie,
Michael Pieler, Mayk Caldas Ramos, Bojana
Ranković, Samuel Rodriques, Jacob Sanders,
Philippe Schwaller, Marcus Schwarting, Jiale
Shi, Berend Smit, Ben Smith, Joren Van Herck,
Christoph Völker,
Logan Ward,
Sean War-
ren, Benjamin Weiser, Sylvester Zhang, Xiaoqi
Zhang, Ghezal Ahmad Zia, Aristana Scour-
tas, K. Schmidt, Ian Foster, Andrew White,
and Ben Blaiszik.
14 examples of how LLMs
can transform materials science and chem-
istry: A reflection on a large language model
hackathon.
Digital Discovery, 2(5):1233–1250,
2023. doi:10.1039/D3DD00113J.
[25] Jean
Kaddour,
Joshua
Harris,
Maximilian
Mozes, Herbie Bradley, Roberta Raileanu, and
Robert McHardy.
Challenges and Applica-
tions of Large Language Models, July 2023.
doi:10.48550/arXiv.2307.10169.
[26] Samuel Kernan Freire, Mina Foosherian, Chao-
fan Wang, and Evangelos Niforatos. Harnessing
Large Language Models for Cognitive Assistants
in Factories. In Proceedings of the 5th Interna-
tional Conference on Conversational User Inter-
faces, CUI ’23, pages 1–6, New York, NY, USA,
July 2023. Association for Computing Machin-
ery. doi:10.1145/3571884.3604313.
[27] Anis Koubaa, Wadii Boulila, Lahouari Ghouti,
Ayyub Alzahem, and Shahid Latif.
Explor-
ing ChatGPT Capabilities and Limitations: A
Survey. IEEE Access, 11:118698–118721, 2023.
doi:10.1109/ACCESS.2023.3326474.
[28] Varun Kumar,
Leonard Gleyzer,
Adar Ka-
hana, Khemraj Shukla, and George Em Karni-
22
adakis. MyCrunchGPT: A LLM Assisted Frame-
work for Scientific Machine Learning.
Jour-
nal of Machine Learning for Modeling and
Computing, 4(4), 2023.
doi.org/10.1615/
JMachLearnModelComput.2023049518.
[29] Dennis
Kundisch,
Jan
Muntermann,
Anna Maria Oberländer, Daniel Rau, Maxi-
milian Röglinger, Thorsten Schoormann, and
Daniel Szopinski.
An Update for Taxonomy
Designers.
Business & Information Systems
Engineering,
64(4):421–439,
August
2022.
doi:10.1007/s12599-021-00723-x.
[30] Gibbeum
Lee,
Volker
Hartmann,
Jongho
Park,
Dimitris
Papailiopoulos,
and
Kang-
wook
Lee.
Prompted
LLMs
as
chatbot
modules for long open-domain conversation.
In Anna Rogers,
Jordan Boyd-Graber,
and
Naoaki Okazaki, editors, Findings of the as-
sociation for computational linguistics:
ACL
2023, pages 4536–4554, Toronto, Canada, July
2023. Association for Computational Linguistics.
doi:10.18653/v1/2023.findings-acl.277.
[31] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zheng-
bao Jiang, Hiroaki Hayashi, and Graham Neu-
big.
Pre-train, Prompt, and Predict: A Sys-
tematic Survey of Prompting Methods in Nat-
ural Language Processing.
ACM Comput-
ing Surveys, 55(9):195:1–195:35, January 2023.
doi:10.1145/3560815.
[32] Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang,
Tianwei Zhang, Yepang Liu, Haoyu Wang, Yan
Zheng, and Yang Liu.
Prompt Injection at-
tack against LLM-integrated Applications, June
2023. doi:10.48550/arXiv.2306.05499.
[33] Yuchen
Liu,
Luigi
Palmieri,
Sebastian
Koch,
Ilche Georgievski,
and Marco Aiello.
DELTA:
Decomposed
Efficient
Long-Term
Robot Task Planning using Large Language
Models.
(arXiv:2404.03275),
April
2024.
doi:10.48550/arXiv.2404.03275.
[34] Yupei Liu, Yuqi Jia, Runpeng Geng, Jinyuan
Jia, and Neil Zhenqiang Gong. Prompt Injec-
tion Attacks and Defenses in LLM-Integrated
Applications. (arXiv:2310.12815), October 2023.
doi:10.48550/arXiv.2310.12815.
[35] Shaoguang
Mao,
Qiufeng
Yin,
Yuzhe
Cai,
and
Dan
Qiao.
LowCodeLLM.
https:
//github.com/chenfei-wu/TaskMatrix/
tree/main/LowCodeLLM, May 2023.
[36] Scott McLean,
Gemma J. M. Read,
Jason
Thompson, Chris Baber, Neville A. Stanton, and
Paul M. Salmon. The risks associated with Ar-
tificial General Intelligence:
A systematic re-
view.
Journal of Experimental & Theoretical
Artificial Intelligence, 35(5):649–663, July 2023.
doi:10.1080/0952813X.2021.1964003.
[37] Oier Mees, Jessica Borja-Diaz, and Wolfram
Burgard. Grounding Language with Visual Af-
fordances over Unstructured Data.
In 2023
IEEE
International
Conference
on
Robotics
and Automation (ICRA), pages 11576–11582,
London, United Kingdom, May 2023. IEEE.
doi:10.1109/ICRA48891.2023.10160396.
[38] Grégoire
Mialon,
Roberto
Dessì,
Maria
Lomeli,
Christoforos
Nalmpantis,
Ram
Pa-
sunuru, Roberta Raileanu, Baptiste Rozière,
Timo
Schick,
Jane
Dwivedi-Yu,
Asli
Ce-
likyilmaz,
Edouard
Grave,
Yann
LeCun,
and
Thomas
Scialom.
Augmented
Lan-
guage
Models:
A
Survey,
February
2023.
doi:10.48550/arXiv.2302.07842.
[39] Melanie
Mitchell.
Debates
on
the
na-
ture of artificial general intelligence.
Sci-
ence,
383(6689):eado7069,
March
2024.
doi:10.1126/science.ado7069.
[40] Quim Motger, Xavier Franch, and Jordi Marco.
Software-Based
Dialogue
Systems:
Survey,
Taxonomy, and Challenges.
ACM Comput-
ing Surveys, 55(5):91:1–91:42, December 2022.
doi:10.1145/3527450.
[41] Fiona Fui-Hoon Nah, Ruilin Zheng, Jingyuan
Cai, Keng Siau, and Langtao Chen.
Gen-
erative AI and ChatGPT: Applications, chal-
lenges, and AI-human collaboration.
Jour-
23
nal of Information Technology Case and Ap-
plication Research, 25(3):277–304, July 2023.
doi:10.1080/15228053.2023.2233814.
[42] Robert C Nickerson,
Upkar Varshney,
and
Jan
Muntermann.
A
method
for
taxon-
omy development and its application in in-
formation systems.
European Journal of In-
formation Systems, 22(3):336–359, May 2013.
doi:10.1057/ejis.2012.26.
[43] Camille Pack, Cern McAtee, Samantha Robert-
son, Dan Brown, Aditi Srivastava, and Kweku
Ako-Adjei.
Microsoft Copilot for Microsoft
365
overview.
https://learn.microsoft.
com/en-us/copilot/microsoft-365/
microsoft-365-copilot-overview,
March
2024.
[44] Chris Parnin,
Gustavo Soares,
Rahul Pan-
dita,
Sumit
Gulwani,
Jessica
Rich,
and
Austin Z. Henley.
Building Your Own Prod-
uct Copilot:
Challenges, Opportunities, and
Needs.
(arXiv:2312.14231), December 2023.
doi:10.48550/arXiv.2312.14231.
[45] Rodrigo Pedro,
Daniel Castro,
Paulo Car-
reira, and Nuno Santos.
From Prompt In-
jections to SQL Injection Attacks:
How Pro-
tected is Your LLM-Integrated Web Appli-
cation?
(arXiv:2308.01990), August 2023.
doi:10.48550/arXiv.2308.01990.
[46] Ken
Peffers,
Tuure
Tuunanen,
Marcus
A.
Rothenberger, and Samir Chatterjee.
A De-
sign Science Research Methodology for Infor-
mation Systems Research.
Journal of Man-
agement Information Systems, 24(3):45–77, De-
cember 2007.
ISSN 0742-1222,
1557-928X.
doi:10.2753/MIS0742-1222240302.
[47] Mohaimenul Azam Khan Raiaan, Md. Sad-
dam Hossain Mukta, Kaniz Fatema, Nur Mo-
hammad Fahad, Sadman Sakib, Most Mar-
ufatul
Jannat
Mim,
Jubaer
Ahmad,
Mo-
hammed Eunus Ali, and Sami Azam. A Review
on Large Language Models: Architectures, Ap-
plications, Taxonomies, Open Issues and Chal-
lenges.
IEEE Access, 12:26839–26874, 2024.
doi:10.1109/ACCESS.2024.3365742.
[48] Jack Daniel Rittelmeyer and Kurt Sandkuhl.
Morphological Box for AI Solutions:
Evalua-
tion and Refinement with a Taxonomy Develop-
ment Method. In Knut Hinkelmann, Francisco J.
López-Pellicer, and Andrea Polini, editors, Per-
spectives in Business Informatics Research, Lec-
ture Notes in Business Information Process-
ing, pages 145–157, Cham, 2023. Springer Na-
ture Switzerland. doi:10.1007/978-3-031-43126-
5_11.
[49] Shubhra Kanti Karmaker Santu and Dongji
Feng.
TELeR:
A
General
Taxonomy
of
LLM
Prompts
for
Benchmarking
Complex
Tasks.
(arXiv:2305.11430),
October
2023.
doi:10.48550/arXiv.2305.11430.
[50] Thorsten Schoormann, Frederik Möller, and
Daniel Szopinski.
Exploring Purposes of Us-
ing Taxonomies.
In Proceedings of the Inter-
national Conference on Wirtschaftsinformatik
(WI), Nuernberg, Germany, February 2022.
[51] Ishika Singh,
Valts Blukis,
Arsalan Mousa-
vian, Ankit Goyal, Danfei Xu, Jonathan Trem-
blay, Dieter Fox, Jesse Thomason, and Ani-
mesh Garg. ProgPrompt: Generating Situated
Robot Task Plans using Large Language Mod-
els. In 2023 IEEE International Conference on
Robotics and Automation (ICRA), pages 11523–
11530, London, United Kingdom, May 2023.
IEEE. doi:10.1109/ICRA48891.2023.10161317.
[52] Gero Strobel, Leonardo Banh, Frederik Möller,
and Thorsten Schoormann.
Exploring Gener-
ative Artificial Intelligence: A Taxonomy and
Types. In Proceedings of the 57th Hawaii Inter-
national Conference on System Sciences, Hon-
olulu, Hawaii, January 2024.
https://hdl.
handle.net/10125/106930.
[53] Hendrik Strobelt, Albert Webson, Victor Sanh,
Benjamin Hoover, Johanna Beyer, Hanspeter
Pfister,
and Alexander M. Rush.
Interac-
tive and Visual Prompt Engineering for Ad-
hoc Task Adaptation With Large Language
24
Models.
IEEE Transactions on Visualization
and Computer Graphics,
pages 1–11,
2022.
doi:10.1109/TVCG.2022.3209479.
[54] Daniel Szopinski, Thorsten Schoormann, and
Dennis Kundisch. Criteria as a Prelude for Guid-
ing Taxonomy Evaluation. In Proceedings of the
53rd Hawaii International Conference on Sys-
tem Sciences, 2020. https://hdl.handle.net/
10125/64364.
[55] Daniel Szopinski, Thorsten Schoormann, and
Dennis Kundisch.
Visualize different:
To-
wards
researching
the
fit
between
taxon-
omy visualizations and taxonomy tasks.
In
Tagungsband Der 15. Internationalen Tagung
Wirtschaftsinformatik
(WI
2020),
Potsdam,
2020. doi:10.30844/wi_2020_k9-szopinski.
[56] Manisha Thakkar and Nitin Pise. Unified Ap-
proach for Scalable Task-Oriented Dialogue Sys-
tem. International Journal of Advanced Com-
puter Science and Applications, 15(4), 2024.
doi:10.14569/IJACSA.2024.01504108.
[57] Oguzhan Topsakal and Tahir Cetin Akinci. Cre-
ating Large Language Model Applications Uti-
lizing Langchain: A Primer on Developing LLM
Apps Fast.
In International Conference on
Applied Engineering and Natural Sciences, vol-
ume 1, pages 1050–1056, 2023.
[58] Michael Unterkalmsteiner and Waleed Adbeen.
A compendium and evaluation of taxonomy
quality attributes.
Expert Systems,
40(1):
e13098, 2023. doi:10.1111/exsy.13098.
[59] Bryan Wang, Gang Li, and Yang Li.
En-
abling
Conversational
Interaction
with
Mo-
bile UI using Large Language Models.
In
Proceedings of the 2023 CHI Conference on
Human Factors in Computing Systems, CHI
’23, pages 1–17, New York, NY, USA, April
2023. Association for Computing Machinery.
doi:10.1145/3544548.3580895.
[60] Can Wang, Bolin Zhang, Dianbo Sui, Zhiying
Tu, Xiaoyu Liu, and Jiabao Kang. A Survey on
Effective Invocation Methods of Massive LLM
Services.
(arXiv:2402.03408), February 2024.
doi:10.48550/arXiv.2402.03408.
[61] Jun Wang, Guocheng He, and Yiannis Kan-
taros.
Safe Task Planning for Language-
Instructed Multi-Robot Systems using Confor-
mal Prediction.
(arXiv:2402.15368), February
2024. doi:10.48550/arXiv.2402.15368.
[62] Lei Wang, Chen Ma, Xueyang Feng, Zeyu
Zhang,
Hao Yang,
Jingsen Zhang,
Zhiyuan
Chen,
Jiakai Tang,
Xu Chen,
Yankai Lin,
Wayne Xin Zhao,
Zhewei Wei,
and Jirong
Wen.
A survey on large language model
based autonomous agents.
Frontiers of Com-
puter
Science,
18(6):186345,
March
2024.
doi:10.1007/s11704-024-40231-1.
[63] Shu Wang, Muzhi Han, Ziyuan Jiao, Zeyu
Zhang, Ying Nian Wu, Song-Chun Zhu, and
Hangxin Liu.
LLM3:Large Language Model-
based Task and Motion Planning with Motion
Failure Reasoning.
(arXiv:2403.11552), March
2024. doi:10.48550/arXiv.2403.11552.
[64] Hao Wen, Yuanchun Li, Guohong Liu, Shan-
hui Zhao, Tao Yu, Toby Jia-Jun Li, Shiqi Jiang,
Yunhao Liu, Yaqin Zhang, and Yunxin Liu. Em-
powering LLM to use Smartphone for Intelligent
Task Automation. (arXiv:2308.15272), Septem-
ber 2023. doi:10.48550/arXiv.2308.15272.
[65] Hao Wen, Yuanchun Li, and Sean KiteFly-
Kid. MobileLLM/AutoDroid. Mobile LLM, Jan-
uary 2024.
https://github.com/MobileLLM/
AutoDroid.
[66] Jules White, Quchen Fu, Sam Hays, Michael
Sandborn, Carlos Olea, Henry Gilbert, Ashraf
Elnashar,
Jesse
Spencer-Smith,
and
Dou-
glas C. Schmidt.
A Prompt Pattern Cat-
alog
to
Enhance
Prompt
Engineering
with
ChatGPT. (arXiv:2302.11382), February 2023.
doi:10.48550/arXiv.2302.11382.
[67] Tongshuang
Wu,
Michael
Terry,
and
Car-
rie Jun Cai.
AI Chains:
Transparent and
25
Controllable Human-AI Interaction by Chain-
ing
Large
Language
Model
Prompts.
In
Proceedings of the 2022 CHI Conference on
Human Factors in Computing Systems, CHI
’22, pages 1–22, New York, NY, USA, April
2022. Association for Computing Machinery.
doi:10.1145/3491102.3517582.
[68] Congying Xia, Chen Xing, Jiangshu Du, Xinyi
Yang, Yihao Feng, Ran Xu, Wenpeng Yin,
and Caiming Xiong.
FOFO: A Benchmark
to Evaluate LLMs’ Format-Following Capa-
bility.
(arXiv:2402.18667),
February
2024.
doi:10.48550/arXiv.2402.18667.
[69] Yuchen Xia, Manthan Shenoy, Nasser Jazdi,
and Michael Weyrich.
Towards autonomous
system:
Flexible
modular
production
sys-
tem
enhanced
with
large
language
model
agents. In 2023 IEEE 28th International Con-
ference on Emerging Technologies and Fac-
tory Automation (ETFA), pages 1–8,
2023.
doi:10.1109/ETFA54631.2023.10275362.
[70] I.
de
Zarzà,
J.
de
Curtò,
Gemma
Roig,
and
Carlos
T.
Calafate.
LLM
Adaptive
PID Control for B5G Truck Platooning Sys-
tems.
Sensors, 23(13):5899, January 2023.
doi:10.3390/s23135899.
[71] Xiaoying Zhang, Baolin Peng, Kun Li, Jingyan
Zhou, and Helen Meng.
SGP-TOD: Build-
ing Task Bots Effortlessly via Schema-Guided
LLM Prompting. (arXiv:2305.09067), May 2023.
doi:10.48550/arXiv.2305.09067.
[72] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi
Tang, Xiaolei Wang, Yupeng Hou, Yingqian
Min, Beichen Zhang, Junjie Zhang, Zican Dong,
Yifan Du, Chen Yang, Yushuo Chen, Zhipeng
Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li,
Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun
Nie, and Ji-Rong Wen. A Survey of Large Lan-
guage Models.
(arXiv:2303.18223), May 2023.
doi:10.48550/arXiv.2303.18223.
[73] Jeffrey Zhou, Tianjian Lu, Swaroop Mishra,
Siddhartha Brahma,
Sujoy Basu,
Yi Luan,
Denny
Zhou,
and
Le
Hou.
Instruction-
Following Evaluation for Large Language Mod-
els.
(arXiv:2311.07911),
November
2023.
doi:10.48550/arXiv.2311.07911.
26
"
1,2,Parrot: Efficient Serving of LLM-based Applications with Semantic Variable,"Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu","The rise of large language models (LLMs) has enabled LLM-based applications
(a.k.a. AI agents or co-pilots), a new software paradigm that combines the
strength of LLM and conventional software. Diverse LLM applications from
different tenants could design complex workflows using multiple LLM requests to
accomplish one task. However, they have to use the over-simplified
request-level API provided by today's public LLM services, losing essential
application-level information. Public LLM services have to blindly optimize
individual LLM requests, leading to sub-optimal end-to-end performance of LLM
applications.
  This paper introduces Parrot, an LLM service system that focuses on the
end-to-end experience of LLM-based applications. Parrot proposes Semantic
Variable, a unified abstraction to expose application-level knowledge to public
LLM services. A Semantic Variable annotates an input/output variable in the
prompt of a request, and creates the data pipeline when connecting multiple LLM
requests, providing a natural way to program LLM applications. Exposing
Semantic Variables to the public LLM service allows it to perform conventional
data flow analysis to uncover the correlation across multiple LLM requests.
This correlation opens a brand-new optimization space for the end-to-end
performance of LLM-based applications. Extensive evaluations demonstrate that
Parrot can achieve up to an order-of-magnitude improvement for popular and
practical use cases of LLM applications.","Parrot: Efficient Serving of LLM-based Applications with Semantic Variable
Chaofan Lin1∗
, Zhenhua Han2, Chengruidong Zhang2, Yuqing Yang2
Fan Yang2, Chen Chen1∗
, Lili Qiu2
1Shanghai Jiao Tong University, 2Microsoft Research
Abstract
The rise of large language models (LLMs) has enabled
LLM-based applications (a.k.a. AI agents or co-pilots), a new
software paradigm that combines the strength of LLM and
conventional software. Diverse LLM applications from differ-
ent tenants could design complex workflows using multiple
LLM requests to accomplish one task. However, they have
to use the over-simplified request-level API provided by to-
day’s public LLM services, losing essential application-level
information. Public LLM services have to blindly optimize
individual LLM requests, leading to sub-optimal end-to-end
performance of LLM applications.
This paper introduces Parrot, an LLM service system that
focuses on the end-to-end experience of LLM-based applica-
tions. Parrot proposes Semantic Variable, a unified abstrac-
tion to expose application-level knowledge to public LLM
services. A Semantic Variable annotates an input/output vari-
able in the prompt of a request, and creates the data pipeline
when connecting multiple LLM requests, providing a natu-
ral way to program LLM applications. Exposing Semantic
Variables to the public LLM service allows it to perform con-
ventional data flow analysis to uncover the correlation across
multiple LLM requests. This correlation opens a brand-new
optimization space for the end-to-end performance of LLM-
based applications. Extensive evaluations demonstrate that
Parrot can achieve up to an order-of-magnitude improvement
for popular and practical use cases of LLM applications.
1
Introduction
Large language models (LLMs) have demonstrated a remark-
able language understanding capability [7,41]. This enables
a paradigm shift in application development. In this new
paradigm, one or multiple application entities, known as AI
agents or co-pilots, communicate with LLMs via natural lan-
guage, known as “prompts”, to accomplish a task collabo-
ratively. For example, Meeting applications like Microsoft
Teams or Google Meet can summarize meeting discussions
through LLMs [33]. Search engines like Google and Bing
can be enhanced with Chat ability through LLMs [14, 34].
It is believed such LLM-based applications will become the
mainstream applications in the near future [13].
∗This work is partially done while Chaofan Lin’s internship and Dr. Chen
Chen’s visting scholar in Microsoft Research.
To accomplish a task, LLM-based applications typically
require multiple rounds of conversation. The conversation, im-
plemented through multiple API calls to LLM, demonstrates
complex workflow patterns. Figure 1 illustrates several popu-
lar conversation patterns. For example, a meeting summary
application [8,33] often divides a lengthy document into mul-
tiple shorter sections, each satisfying the length constraint
of the LLM conversation and thus can be summarized and
combined into the final summary through the Map-Reduce
or chaining summary patterns. Chat-based applications, e.g.,
Bing Copilot [34], call LLM APIs multiple times to generate
answers based on user queries. Multiple agents, each repre-
senting a different role played by different LLM calls, can
collaborate to achieve a task [22,47,54].
Public LLM service providers have to face diverse tenants
and applications, each with different workflows and perfor-
mance preference. However, existing API design for LLM
service provision is still request-centric. Public LLM services
only observe tons of individual requests, without knowing any
application-level information, e.g., which requests belong to
the same application, how different requests are connected, or
whether there are any similarities. The lost application-level
information makes public LLM service blindly optimize the
performance of individual requests, leading to sub-optimal
end-to-end performance of LLM applications. In this paper,
we observe there exist significant opportunities to improve
the end-to-end experience of LLM applications by exploiting
the application-level information, especially the correlation
of multiple LLM requests.
First, multiple consecutive LLM requests may be depen-
dent: the result of one request could be the direct input of
the next request. Therefore, it is desirable to colocate those
Chunk 1
Chunk 2
……
Chunk N
LLM
LLM
LLM
……
S1
S2
SN
……
LLM
Final 
Summary
(a) Map-Reduce Summary
Chunk 1
Chunk 2
……
Chunk N
LLM
S1
+
LLM
S1
+
S2
+
……
SN-1
LLM
Final 
Summary
LLM Request
Message Passing
(b) Chain Summary
Query Rewriter
LLM-powered Search
QA w/ search result
Safety Checker
LLM
Final 
Answer
User 
Query
(c) LLM-Powered Search
Product Manger
Architect
Engineer
QA Tester
Code Reviewer
LLM
Final 
Code
Task
(d) Multi-agent Coding
Figure 1: The workflow of popular LLM-based applications.
The final result requires multiple LLM requests.
arXiv:2405.19888v1  [cs.LG]  30 May 2024
requests together and execute them consecutively on the LLM
service side. However, unaware of their dependencies, these
requests have to be executed interactively between the client
side of LLM-based applications and the public LLM ser-
vices. These clients, often located on the other end of the
Internet, can only issue the second request after they receive
the result of the first request. This unnecessarily incurs extra
overhead of consecutive requests on network latency as well
as losing the opportunity of co-scheduling these consecutive
requests (§3).
Second, LLM requests may have diverse scheduling pref-
erence, even within a single application. For example, in Fig-
ure 1a, to reduce the end-to-end latency, the requests represent-
ing multiple Map tasks should be batched more aggressively
to increase the throughput of the Map tasks; while the Re-
duce task, due to its scarcity, should be optimized for latency.
Unfortunately, public LLM services cannot discriminate the
difference between the two types of tasks. As a result, the
current practice is to blindly optimize the latency for individ-
ual requests, which might not be desirable for the end-to-end
experience.
Third, there exists a high degree of commonality across
LLM requests. Popular LLM applications (e.g., Bing Copi-
lot [32], GPTs [42]) use a long system prompt, including task
definitions, examples, and safety rules, to guide the behavior
of LLM applications. The long system prompt is usually static
and common for all users. As existing public LLM services
treat each request individually, these common prefix prompts
are provided repeatedly in each request, leading to a great
waste of storage, computation, and memory bandwidth. Our
analysis of a production LLM-based search engine shows
that over 94% of tokens in the requests are repeated across
different users.
Although we have seen some emerging engine-level tech-
niques [25,56,63] proposed to optimize the above three cases,
they all work based on certain application-level knowledge,
which is lost in nowadays public LLM services. In a nut-
shell, due to the lack of understanding of the correlations of
LLM requests, existing LLM services cannot leverage the
three opportunities, leading to high end-to-end service latency
and reduced throughput. Based on the above facts and in-
sights, we introduce Parrot, an LLM service system that treats
LLM applications as first-class citizens. Parrot retains most of
application-level information by a simple abstraction Seman-
tic Variable, achieving a perfect balance between increasing
system complexity and bringing new information for opti-
mization. A Semantic Variable is a text region in the prompt
with a specific semantic purpose, such as a task instruction, a
list of few-shot examples, an input, or an output. A Semantic
Variable can also work as the data pipeline that connects mul-
tiple LLM requests. Semantic Variable naturally exposes the
information of prompt structures and correlations of requests
to LLM services. By inspecting Semantic Variable at runtime,
Parrot can perform conventional data flow analysis to derive
Figure 2: The communication of consecutive LLM requests
in multi-agent applications.
the data dependency between LLM requests just-in-time.
By analyzing the application-level information, Parrot’s
unified abstraction naturally enables joint optimizations,
which bring better global optimality. The same data pipeline
built by Semantic Variables can enable multiple optimizations
simultaneously, including hiding data pipeline’s latency, ob-
jective deduction for a better scheduling and commonality
analysis to perform de-duplication. Parrot’s scheduling also
takes different opportunities into accounts under the unified
abstraction. Our extensive evaluation of Parrot on popular
LLM-based applications, including the production and open-
source projects, shows Parrot achieves up to 11.7× speedup
or 12× higher throughput compared with the state-of-the-art
solutions.
2
Background
LLM Service.
Most LLM services are provisioned as a
conditional generation service via a text completion API.
Completion(prompt : str) −
→generated_text : str.
The application client provides a text prompt, and the LLM
service responds with the generated text. Behind the API,
an LLM service provider runs one or multiple clusters of
LLM inference engines. A request scheduler dispatches LLM
requests from a queue to an LLM inference engine, which
uses a set of GPUs to conduct the LLM inference.
LLM-based Applications.
Figure 1 highlights the repre-
sentative workflows of how LLM is used in the applications.
Due to the limited context window of LLMs (e.g., 4,096 for
GPT-3.5-Turbo [40]), data analytics on long documents fol-
low a map-reduce style (Figure 1a) or chain style (Figure 1b)
workflow to generate the final results. It splits the long tran-
script into chunks, uses multiple requests to generate partial
results for each chunk (the Map task), and combines them
altogether (a Reduce task) or incrementally (the chain style)
to generate the final result. Chat-based search engine in Fig-
ure 1c may use consecutive LLM requests to discern query
0
1000
2000
3000
4000
Prompt Length (# of tokens)
0
1000
2000
3000
4000
5000
Time (ms)
End-to-end Time (P99))
GPU Inference Time
Other Overhead (median)
(a) Latency Breakdown
LLM Step A
LLM Step B
Scheduler
Internet
LLM App
Other LLM Apps
A
B
LLM Engine
LLM Engine
LLM Engine
A
Queue
B
①
②
③
Query
Response
④
(b) Current LLM Services
LLM Engine
LLM Engine
LLM Engine
Scheduler
LLM Step A
LLM Step B
①
②
Internet
A
B
A
B
Queue
Query
Response
LLM App
Other LLM Apps
(c) Our system: Parrot
Figure 3: The end-to-end latency breakdown of current LLM services. The source of the overhead comes from network and
queuing due to chatty interaction between LLM application and LLM services, which is eliminated in our system Parrot.
intention, enrich the query with supplementary information,
retrieve related data, undergo a safety check, and finally gen-
erate the response. Multi-agent in Figure 1d and Figure 2 is
another type of workflow using multiple LLM requests, each
with a designated role. Different roles work collaboratively on
the same task, e.g., AutoGen [54] and MetaGPT [22] use the
roles like product manager, architect, engineer, and QA tester.
They communicate with each other on a software project.
Each role is supported by one or multiple LLM requests to
act as the designed role to generate their responses.
3
Problems of Serving LLM Applications
Although LLM’s text completion API provides a flexible way
of building LLM applications, it loses the application-level
information to public LLM services, leading to the following
challenges.
Excessive Overhead of Consecutive Requests.
As demon-
strated in Figure 1, LLM applications frequently make multi-
ple LLM calls to complete a single task. Due to the request-
centric design of existing public LLM services, which gener-
ate responses for each request individually, developers have to
parse the output of an LLM request and compose the prompts
for subsequent LLM requests on the client side. Figure 3a
shows our empirical study of the latency breakdown of the
LLM calls from a popular LLM application in our production,
which uses a chain-style workflow. The prompt lengths range
from 150 to 4000 tokens and the output length is around 50
tokens. We find there is a significant portion of the latency of
LLM API call originates outside the LLM engine (30 ∼50%
on average and over 70% in the worst cases). The overhead in-
creases with the growing length of prompts. The high latency
can sometimes result in API timeouts and resubmissions.
Such overhead is due to the chatty interaction between
LLM services and clients. Figure 3b illustrates the overhead
of a simple two-step LLM application (e.g., chain-style sum-
mary of two text chunks). Existing LLM services are unaware
of the dependency among such requests, where the output of
the previous request may be the direct input of the next one.
For such consecutive and dependent requests, the client has
Chunk 1
Chunk 2
Chunk 3
Chunk 4
……
Final Summary
Chunk 5
Chunk 6
Chunk 15
Chunk 16
Batch=2
Time
Chunk 1
Final Summary
Batch=8
Chunk 9
Time
(1) Per-request latency optimized
(2) End-to-end latency optimized
Maximize Throughput
Minimize Latency
Latency=2700 ms
Latency=1100 ms
Reduce Stage
Map Stage
Reduce Stage
Chunk 2
Chunk 10
Chunk 7
Chunk 15
Chunk 8
Chunk 16
Map Stage
……
……
Figure 4: Request-centric scheduling v.s. application-centric
scheduling for the map-reduce style document summary task.
to wait for the arrival of the response to the first LLM request
( 2 ) before submitting the next LLM request ( 3 ). This un-
necessarily incurs heavy network latency because clients and
LLM services are typically in different data centers. More-
over, the next LLM request has to suffer extra queuing delays
( 4 ), because requests from other applications may arrive
between the consecutive LLM requests.
In Table 1, we evaluated four popular LLM applications.
The first two are from our production, and the last two are
popular open-source projects. They all require tens of LLM
calls to complete a single task, which results in high user-
perceived latency. Our evaluation in §8.2 shows LLM services
that treat requests individually could slow down the end-to-
end latency by over 2×. An LLM service can eliminate the
overhead if it can handle consecutive requests in a batch.
Parrot adopts such an approach. As shown in Figure 3c, the
two steps of the same application are scheduled together, thus
allowing the output of Step A to be fed directly into Step
B—with the network and queuing overhead bypassed.
Misaligned Scheduling Objectives.
Due to the lost appli-
LLM-based App.
# Calls
Tokens
Repeated (%)∗
Long Doc. Analytics
2 ∼40
3.5k ∼80k
3%
Chat Search
2 ∼10
5k
94%
MetaGPT [22]
14
17k
72%
AutoGen [54]
17
57k
99%
∗We count a paragraph as repeated if it appears in at least two LLM requests.
Table 1: Statistics of LLM calls of LLM applications.
[system](#instructions) 
## You are the chat mode 
of Microsoft Bing search: 
- You identify as Microsoft 
Bing search to users, 
**not** an assistant. 
- You should ……
[system](#context) 
- New conversation with user A. 
- Time at the start of this conversation is 
Sun, 30 Oct 2022 16:13:49 GMT. The 
user is located in Redmond, Washington, 
United States. 
[user](#message) Hi. ……
[system](#context) 
- New conversation with user B. 
- Time at the start of this conversation is 
Mon, 20 Nov 2023 16:13:49 GMT. The 
user is located in London, UK. 
[user](#message)
Explain AI agent for a kid.
Task Role (static)
Few-shot Examples (quasi-static)
User Input (dynamic)
+
+
Figure 5: The prompt structure of Bing Copilot shows a long
prompt reused by different user queries.
cation information (workflow and application performance
objective), existing public LLM services have to blindly use
a universal treatment for all requests, e.g., optimizing per-
request latency [44]. However, LLM-based applications are
more concerned about the end-to-end experience, rather than
individual requests. This misaligned optimization objectives
may negatively impact end-to-end performance. Considering
the map-reduce document summary in Figure 1a, the system
should minimize the end-to-end time it takes to receive the
final summary, rather than the latency of individual requests.
The LLM services optimized for individual requests are not
optimal for end-to-end latency.
As depicted in Figure 4, current LLM services must limit
the number of concurrent requests running on each LLM en-
gine to control the latency of individual requests. However,
there is a trade-off between latency and throughput in LLM in-
ference. Increasing the batch size can bring up to 8.2× higher
throughput but lead to 95% higher latency [9]. Yet, if we un-
derstand the application-level performance objective, which
in this case is the end-to-end latency, we can determine that
the ideal scheduling strategy should maximize the throughput
(using higher batch sizes) during the map stage and minimize
request latency during the reduce stage. This strategy reduces
end-to-end latency by 2.4×. Moreover, it uncovers the po-
tential to enhance cluster throughput without compromising
the end-to-end latency of LLM applications. This insight is
essential for addressing the conflict between rising demand
and limited hardware resources. It underscores the necessity
of scheduling LLM requests from the perspective of LLM
applications, but it also presents the challenge of managing
diverse LLM requests with varying performance objectives.
Redundant Computations.
Currently, most LLM-based
applications exhibit a high degree of redundancy in the
prompts of their requests. For instance, Bing Chat [32] has
handled more than 1 billion chat prompts. These prompts
share the same system prompts that defines the functionality
of Bing Chat. OpenAI introduces GPTs [42] to let users cus-
tomize a ChatGPT for a specific purpose whose prompt tem-
plate is the same across users. The commonality in prompts
is crucial as it delineates the functionality and restrictions
of LLM-based applications. The prompt structure in Fig-
ure 5 [52] includes a role definition, several examples to
enhance the precision of LLM’s behaviors and user query
details. While the user input is dynamic, the task role is al-
Parrot LLM Engine
Parrot LLM Engine
Parrot LLM Engine
Parrot APIs w/ Semantic Variables
Parrot Manager w/ Inter-Request Analysis
Parrot 
App-centric 
LLM Service
Applications
Internet
Applications (front-end)
Parrot Front-end
Others (LangChain, SK, etc.)
Perf. Objective Deduction
Sharing Prompt Prefix
App-centric Scheduling
Efficient GPU Kernels
Context Management
Contextual Fill / Gen
Inter-Request Comm.
Figure 6: Parrot system overview.
ways fixed, and the few-shot examples could be quasi-static in
that the same type of tasks use the same examples. This is why
more than 94% of prefix tokens could be repetitively used
across LLM requests for various users (Table 1). Such com-
monality also exists in multi-agent applications. For example,
MetaGPT [22] and AutoGen [54] recurrently incorporate con-
versation history into the prompt over several rounds of LLM
requests, leading to 72% and 99% redundancy respectively.
These redundant sections excessively utilize GPU memory
bandwidth and are computed for multiple times. Earlier re-
sults have proposed optimizations in LLM engines to avoid
redundant GPU memory of shared prompt [25]. However, it is
hard for public LLM services to swiftly detect and co-locate
the prompt-sharing requests, which be dynamically generated,
from tons of diverse requests from diverse applications. With-
out knowledge about the prompt structure, extensive token-
by-token matching for every LLM request is expensive at the
cluster level. Hence, if the cluster scheduler of public LLM
service cannot dispatch prompt-sharing requests to the same
engine, the engine-level redundancy avoidance optimizations
would be hard to take effect.
4
Parrot Design
Figure 6 depicts the overview of Parrot’s design. Parrot pro-
vides a natural way of programming LLM applications with
Semantic Variable annotations (§4.1), which is compatible of
existing LLM orchestration frameworks, e.g., LangChain [8].
Centering on this abstraction, Parrot Manager is designed
to schedule LLM requests at a cluster-level, by deriving the
application-level knowledge (§4.2) and optimizing end-to-end
performance of application (§5). The manager will schedule
the LLM requests to LLM Engine, which is formed by a GPU
server (or a group of servers) in the cluster that can serve LLM
requests independently.
4.1
Semantic Variable
Parrot treats an LLM request as a semantic function1 im-
plemented using natural language and executed by LLMs.
1The term semantic function is borrowed from Semantic Kernel [36].
import Parrot as P
from Parrot.PerformanceCriteria import LATENCY
@P.SemanticFunction
def WritePythonCode(task: P.SemanticVariable):
"""""" You are an expert software engineer.
Write python code of {{input:task}}.
Code: {{output:code}}
""""""
@P.SemanticFunction
def WriteTestCode(
task: P.SemanticVariable,
code: P.SemanticVariable):
"""""" You are an experienced QA engineer.
You write test code for {{input:task}}.
Code: {{input:code}}.
Your test code: {{output:test}}
""""""
def WriteSnakeGame():
task = P.SemanticVariable(""a snake game"")
code = WritePythonCode(task)
test = WriteTestCode(task, code)
return code.get(perf=LATENCY), test.get(perf=LATENCY)
Figure 7: Example: a multi-agent application in Parrot.
A Semantic Variable is defined as a input or output vari-
able of a semantic function, which is referred as a place-
holder in the prompt. Figure 7 shows a simplified example of
multi-agent application like MetaGPT [22]. It contains two
SemanticFunctions, one for the software engineer to write
code and one for the QA engineer to write test code. It has
three Semantic Variables: task, code, and test, for task de-
scription, the code to be developed by the software engineer,
and the test code to be developed by the QA engineer, re-
spectively. Although existing LLM orchestration frameworks
(e.g., LangChain [8]) also allow placeholders in a prompt,
however, the placeholders are rendered with real data before
the submission, hence public LLM services cannot detect such
a structure. Instead, Parrot relies on Semantic Variables to
preserve the prompt structure for further inter-request analysis
in public LLM services side.
In addition to the semantic functions, LLM application
developers can further define orchestration functions that con-
nect multiple semantic functions (e.g., WriteSnakeGame in
Figure 7). The Semantic Variables connecting multiple se-
mantic functions form the data pipeline of multiple LLM
requests in the public LLM service.
A simple data flow
analysis of the semantic functions can be done to reveals
the connections of multiple LLM requests. E.g., in Figure 7,
the code variable connects the two LLM requests originat-
ing from WritePythonCode and WriteTestCode, showing
their sequential dependency. Different from traditional com-
pletion API, Parrot splits a completion request to submit
operation and get operation (§7). A function calling of
SemanticFunction will trigger the submit API to submit a
LLM request with its prompt and input Semantic Variables.
The execution of a SemanticFunction is asynchronous
thus it returns the futures of the output Semantic Variables.
task
code
WritePythonCode
WriteTestCode
test
You are an expert software engineer. Write python code of
You are an expert ...... code of: {{input:task}}. Code:
Hash(
)
Hash(
)
① PrefixHash()
④ GetPerfObj() 
Latency   
③ GetConsumers() 
[Request(
)]
② GetProducer()  Request(
)
WritePythonCode
WriteTestCode
Figure 8: Primitives (selected) for Inter-Request Analysis.
Through the get API, applications can fetch the value of
an output Semantic Variable from the public LLM service
in an on-demand manner. This asynchronous design allows
Parrot-powered LLM service to receive all LLM requests not
blocked by native functions and analyze their relationships
just-in-time.
The get operation supports annotation of performance cri-
teria, showing the end-to-end performance requirement of
an application, which can be end-to-end latency or through-
put (extensible to more criteria like per-token latency when
streaming, and time-to-first-token). For example, the final out-
puts, code and test in Figure 7, are fetched using get with
an objective of end-to-end latency. Criteria of middle vari-
ables will be automatically deduced and propagated from final
outputs (§5.2). After propagation, each variable is attached to
a criterion, which finally works by serving as a hint to Parrot’s
scheduler (§5.4).
4.2
Primitives of Inter-Request Analysis
In general, Parrot perform inter-request analysis mainly by
two types of application-level information deduced from Se-
mantic Variable: DAG of requests and prompt structure. Fig-
ure 8 illustrates the DAG workflow of the example shown in
Figure 7 and the primitives used for inter-request analysis and
optimizations.
DAG-based analysis.
As requests, or SemanticFunctions,
are submitted beforehand, Parrot can receive them all at once
and analyze their correlations just-in-time on the service side.
Parrot maintains a DAG-like data structure in each user’s
registered session. Each node is either a request or a Seman-
tic Variable that connects different requests. When a request
comes, Parrot inserts it to DAG by linking edges with Seman-
tic Variables it refers through placeholders in the prompts.
Parrot can perform conventional dataflow analysis [1, 38]
using the primitives to get the producer and consumers of Se-
mantic Variables (i.e., GetProducer and GetConsumers) to
recover dependency of LLM requests. Using the request DAG
and the annotated performance criteria (via GetPerfObj) of
final output Semantic Variables, Parrot can deduct the request-
level scheduling preference by analyzing the DAG and the
performance objective of final outputs (§5.2).
Prompt structure-based analysis.
Based on the prompt
structure declared by Semantic Variables, Parrot supports ex-
tracting the hash values of an LLM request at multiple po-
sitions split by Semantic Variables (i.e., PrefixHash). For
example, the prompt of WritePythonCode has two potential
sharing prefix: the text before {{input:task}} and the text
before {{output:code}}, thus there will be two prefix hash
values generated. The prefix hashes of LLM requests will
be used by swift detection of commonality across multiple
requests, supporting both static and dynamically generated
contents, as well as within the same type of application or
even across applications (§5.3).
5
Optimizations with Semantic Variable
5.1
Serving Dependent Requests
To avoid the unnecessary client-side execution, it requires
the dependency of requests at the application level, which
is lost in today’s public LLM services. With the DAG and
primitives illustrated in §4.2, Parrot serves dependent requests
efficiently through a graph-based executor. The executor polls
constantly and sends it to corresponding engine once ready
(i.e. producer requests are all finished), which allows instant
execution and maximizes batching opportunities. For con-
secutive execution of dependent requests, materialized value
is transmitted through a message queue allocated for cor-
responding Semantic Variable, avoiding unnecessary chatty
communication between clients and LLM services.
The value of a Semantic Variable in a request may require
transformation before being exchanged, e.g., the value of a
Semantic Variable is extracted from the JSON-formatted out-
put of an LLM request, which is then fed into consecutive
LLM requests. Similar to existing message queue systems
that support message transformation (e.g., Kafka [5]), Parrot
also supports string transformation to manipulate Semantic
Variables during value exchanging among LLM requests. Par-
rot supports most output parsing methods of LangChain [8],
which covers most use cases of LLM applications.
5.2
Performance Objective Deduction
To optimize the end-to-end performance of applications, we
need to know the application-level performance criteria. To
help deriving the request-level scheduling preference from the
end-to-end application’s performance requirement, we need
to understand the workflow of the LLM application, which is
the DAG of LLM requests derived by Parrot’s primitives.
When an application annotates a Semantic Variable to pre-
fer higher throughput, all requests generating this Seman-
tic Variable (both directly or indirectly) will be marked as
throughput-preferred when scheduling. This scheduling pref-
erence is usually beneficial for offline data processing, such
as bulk document analysis.
1
3
5
4
6
7
x.get(perf=LATENCY)
Task 
Group 0
Task 
Group 1
2
y.get(perf=LATENCY)
Figure 9: Performance deduction for an LLM-based applica-
tion generating two latency-sensitive Semantic Variable.
Handling latency-sensitive applications is more intricate.
As demonstrated in Figure 4, achieving low end-to-end la-
tency may sometimes require prioritizing throughput at the
Mapping stage. The latency of individual requests can sacri-
ficed so as to reduce the completion time of the entire DAG of
requests. Parrot analyzes LLM requests in reverse topological
order, beginning with those linked to latency-critical Semantic
Variable, as depicted in Figure 9. With the extracted DAG,
LLM requests that directly result in latency-critical Seman-
tic Variables are labeled as latency-sensitive (Request 1 and
2), as are their immediate predecessors (Request 3). Parallel
LLM requests at the same stage are grouped into a task group
(Task Groups 0 and 1). The scheduler should minimize the
latency of the entire task group, often leading to a higher batch
capacity for higher throughput of token generation.
5.3
Sharing Prompt Prefix
When an LLM request is scheduled to an LLM engine, a con-
text on the engine is created to store the state of the model
execution for this request (mainly KV cache). Existing works
have proposed to share the KV cache of common prefix of
prompts in LLM engines to save the GPU memory. However,
as we have explained in §3, today’s public LLM service face
diverse applications and requests, which is hard to identify
the commonality at the cluster level. Token-by-token compar-
ison is impractical due to high time complexity, especially for
very long context with massive requests. In Parrot, by expos-
ing Semantic Variables to LLM service, we can understand
the prompt structure to automatically detect the commonality
more efficiently at the granularity of Semantic Variables.
Using Parrot’s primitive of PrefixHash, Parrot only needs
to check the hash value at positions after each Semantic Vari-
able in a request’s prompt. Parrot maintains a key-value store,
where each entry maps a (hashed) prefix of tokens to a list of
requests, thus the scheduler can quickly check the opportunity
in an online manner, supporting both static and dynamically-
generated prompt within one application or even across dif-
ferent applications.
Furthermore, we propose better GPU kernel for the atten-
tion computation of the requests with a common prefix. We
first leverage vLLM’s paged memory management [25] to
save the redundent GPU memory. But vLLM’s kernel still
suffers from redundant computation and memory loading
of the shared tokens. Therefore, we design a new Attention
decoding algorithm by combining FlashAttenation [12] and
PagedAttention [25] that treat the shared and non-shared to-
Algorithm 1: Parrot’s Request Scheduling.
Data: Q: the request queue
1 Q.sort() ; /* Topological order
*/
2 for r ∈Q do
3
SharedReqsInQueue, CtxInEngine =
FindSharedPrefix(r);
4
if r.TaskGroup ̸= ∅then
5
r∗= FindEngine(r.TaskGroup);
6
else if SharedReqsInQueue ̸= ∅then
7
r∗= FindEngine(SharedReqsInQueue);
8
else if CtxInEngine ̸= ∅then
9
r∗= FindEngine(r, filter=CtxInEngine);
10
if r∗= ∅then
11
r∗= FindEngine(r);
12
Q.remove(r∗);
ken separately. This significantly accelerates the attention of
shared contexts (implementation details in §7).
5.4
Application-Centric Scheduling
To fix the problem of existing public LLM service that blindly
optimize diverse individual requests, Parrot’s scheduling pol-
icy leverages the application-level knowledge to optimize the
end-to-end performance. Specifically, the primary goal of Par-
rot’s scheduler is to meet the varied performance goals of
LLM applications while optimizing GPU cluster utilization.
As explained in §3, a conflict arises when combining through-
put and latency oriented requests: large batch sizes increase
throughput and GPU efficiency but degrade latency, and vice
versa. Transformer-based LLM inference is largely memory-
bound, with latency influenced by the count of concurrent
tokens within the engine. To meet performance targets of
LLM applications, particularly latency, an LLM engine must
regulate the token count below a specified threshold, which
is determined by the LLM request with the most strict la-
tency constraint. Therefore, Parrot’s scheduling principles are
twofold: (1) group LLM requests with similar performance
requirements to circumvent the conflict, and (2) maximize
opportunities for sharing across requests.
Algorithm 1 outlines the scheduling process of Parrot. With
the extracted DAG, the system arranges the LLM requests
according to their topological order (line 1). Parrot tends to
schedule requests belonging to the same application together
to avoid the slowing down of interleaved scheduling (§8.2).
For requests identified as part of a task group through Parrot’s
performance objective deduction, the scheduler attempts to
allocate the entire task group together (line 4-line 5). Addi-
tionally, if Parrot detects other queued requests or running
contexts with a common prefix, it tries to assign them to
the same LLM engine (line 3, line 6-line 9), to utilize Par-
rot’s context fork to reduce the redundant computation and
GPU memory transactions. For an LLM request without the
above opportunity, Parrot schedules the request independently
(line 10-line 11). Due to limited space, we omit the details of
how Parrot chooses LLM engines (i.e., FindEngine). Briefly,
Parrot finds the engine that satisfies the scheduling preference
of a request while minimizing the negative impacts. For in-
stance, if a latency-sensitive request is scheduled to an LLM
engine that can run up to 64,000 tokens of throughput-driven
requests, its capacity will be significantly reduced to 2,000 to
satisfy its strict latency requirement. But, if it is scheduled to
an engine that has already been running a latency-sensitive
request, the capacity reduction is negligible.
6
Discussion
Dynamic Applications and Function Calling.
Currently,
Parrot only supports cloud-side orchestration of LLM requests
without involving dynamic control flow and native functions
(e.g., Python Code). They still require client-side execution.
We intentionally disable the offloading of these functions
to public LLM services to minimize the security risks of
malicious injection. For private LLM services whose LLM
applications are trusted or there is a trusted zone to execute
these functions, Parrot’s APIs can be easily extended with
conditional connections and native code submission. More-
over, these extensions further enable new optimizations, e.g.,
we can speculatively pre-launch high-probability branches in
dynamic applications based on past profiles. This also proves
the potential of Parrot’s design when facing new types of
applications. We leave these extensions as future works.
Other Applications of Inter-Request Analysis.
The inter-
request analysis in Parrot enables a new optimization space
not limited to the ones we introduced in §5. A large-scale
service has more scheduling features to consider, including
handling outliers [3], job failures [58], delay scheduling [57],
fairness [15,61], starvation [17], or supporting heterogeneous
clusters [24, 37], which have been widely studied in other
systems. Parrot provides a new view from the perspective
of LLM-based applications: we need to understand the inter-
connection and commonality of LLM requests to optimize
applications’ end-to-end performance. These features can be
revisited in the LLM service system by considering the new
characteristics of LLM applications. In this paper, we focus
on Parrot’s mechanisms and a few use cases, leaving other
optimizations as promising future works.
Parrot with LLM Orchestration Frameworks.
There
have been several frameworks for developers to build LLM-
based applications, e.g., LangChain [8], SemanticKernel [36],
and PromptFlow [35]. The key function of these frameworks
is to “glue” different LLM calls to accomplish a complex
task (aka. LLM orchestration). Parrot can be integrated with
these frameworks by extending their calling of LLM service
APIs with Semantic Variables. Most of these frameworks
have already used a template-based approach in which devel-
opers can design a template with placeholders, and render the
placeholders at runtime. These placeholders naturally have
the same concept as Parrot’s Semantic Variable. However,
because these frameworks will render the template prompt
before the submission, LLM services lose the information on
the prompt structure. To make these frameworks compatible
with Parrot, both the template itself and the variables to render
the template (using Semantic Variable in Parrot) need to be
wrapped as a SemanticFunction so the necessary informa-
tion is exposed to Parrot’s LLM service.
7
Implementation
Parrot is an end-to-end LLM service for LLM applications,
implemented on Python with about 14,000 lines of code. Its
front-end provides the abstraction of Semantic Variable, and
SemanticFunction, which is transformed into Parrot’s APIs
(implemented with FastAPI [48]) to be submitted as LLM
requests. A centralized Parrot manager handles the manage-
ment of LLM requests, including Semantic Variables, com-
munication, and scheduling. We also build an LLM engine
based on efficient kernels from vLLM [25], xFormers [26],
and ourselves. The engine supports advanced features for
LLM serving, including paged memory management [25] and
continues batching [56]. Parrot’s front-end and manager are
implemented in 1,600 and 3,200 lines of Python, respectively.
Parrot’s LLM engine is implemented in 5,400 lines of Python
and 1,600 lines of CUDA. We have implemented OPT [60]
and LLaMA [51] with PyTorch [45] and Transformers [53].
APIs.
Applications programmed by SemanticFunctions
or other frontends are finally lowered to requests to universal
APIs through different adapters. Parrot provides OpenAI-like
APIs with the extension of Semantic Variables. The request
body of two operations mentioned in §4.1 is shown as follows:
(submit) {""prompt"": str, ""placeholders"": [{""name"":
str, ""in_out"": bool, ""semantic_var_id"": str,
""transforms"": str}, ...], ""session_id"": str}
,
→
,
→
(get) {""semantic_var_id"": str, ""criteria"": str,
""session_id"": str}
,
→
In addition to the static string prompt, Parrot preserves the
input and output placeholders. A placeholder is associated
with a semantic variable either for rendering the input or
parsing the output. As introduced in §5.1. Parrot supports
transformations before the input or after the output. Parrot
also supports other APIs for setting and fetching the value of
Semantic Variables. The error message will be returned when
fetching an Semantic Variable, whose intermediate steps fail
(including engine, communication, and string transformation).
Kernel Optimization.
vLLM’s GPU kernel, while capable
of reusing results cached in GPU memory for shared prefix to-
kens in a prompt, sometimes excessively reloads these tokens
from global to shared memory, impeding attention score com-
putations. Using OpenAI Triton [43] and CUDA, we have
developed a novel GPU kernel, integrating concepts from
PagedAttention [25] and FlashAttention [11,12], to acceler-
ate attention decoding computation involving shared prefixes.
This kernel retains PagedAttention’s approach of storing the
key-value (KV) cache in disparate memory segments and
utilizes a page table per request to monitor block status and
placement. Furthermore, employing FlashAttention princi-
ples, the kernel maximizes data reuse within shared memory.
Unlike reloading tiles repeatedly in the PagedAttention’s im-
plementation, it loads KV cache tiles for the shared prefix
to shared memory only once, diminishing memory transac-
tions between the L2 Cache and Shared Memory. The kernel
initially calculates interim attention metrics (including atten-
tion scores, qk_max, exp_sum) for the shared prefix using the
loaded tiles and records these back to HBM. Subsequently, it
processes the new tokens’ partial attention beyond the prefix,
amalgamating this with the prefix’s interim results to derive
the ultimate attention output.
Universal Engine Abstraction.
Parrot’s cluster manager
controls multiple engines running various models, tokeniz-
ers, KV cache layouts, etc. To enable Parrot’s optimizations,
LLM engines need to support (1) stateful generation (e.g.,
guidance [18]) and (2) sharing KV cache states across dif-
ferent requests. Hence we propose a universal abstraction to
describe the minimal capability required to LLM engines to
be integrated into Parrot.
def Fill(token_ids: List[int], context_id: int,
parent_context_id: int)
,
→
def Generate(sampling_configs: Dict, context_id:
int, parent_context_id: int)
,
→
def FreeContext(context_id: int)
These three methods not only cover the basic completion
functionality of LLM inference engine, but also provide a
flexible context management interface. The Fill method pro-
cesses the initial prompt tokens, calculates and fills the KV
cache into corresponding context. The Generatemethod pro-
duces tokens via generative decoding that produces one token
per iteration until it reaches the length limit, user-defined
termination character or EOS (end-of-sequence) token, un-
der certain sampling configurations (e.g. temperature). Fills
and Generates are scheduled and batched by engine’s sched-
uler per iteration using continuous batching [56]. Creating
and forking contexts can also be realized with these two
methods by setting context_id and parent_context_id,
respectively. The FreeContextmethod explicitly frees a con-
text (i.e. free its KV cache in GPU memory). Separating
Filland Generatenot only fits Semantic Variable naturally:
constant text and input values are processed by Fill; the out-
put values are generated by Generate, but also breaks the
request-level dependency into a finer granularity, enabling
more parallel execution opportunities [2,21,46,64].
8
Evaluation
8.1
Experimental Setup
Testbed.
We evaluate Parrot with two separate setups for
single-GPU and multi-GPU experiments. The single-GPU
evaluations use a server with a 24-core AMD-EPYC-7V13
CPUs equipped with one NVIDIA A100 (80GB) GPU. The
multi-GPU evaluations use a server with 64-core EPYC AMD
CPU and four NVIDIA A6000 (48GB) GPUs. Both servers
run CUDA 12.1 and cuDNN 8.9.2.
Workloads.
Our evaluations are performed to run four rep-
resentative LLM applications. Each LLM engine uses one
GPU and runs a LLaMA 13B or LLaMA 7B model [51] .
For LLM-based data analytics on long documents, we use the
Arxiv dataset [27], executing chain and map-reduce summa-
rizations on an extensive collection of academic papers. To
investigate the sharing opportunities of LLM-based applica-
tions with many users, we run the prompts from Bing Copilot
and GPTs [42] with synthesized user queries. For multi-agent
applications, we build a multi-agent programming application
using MetaGPT [22], which contains a system architect to
design APIs, multiple programmers to write code for different
files, reviewers to share review comments. The programmers
will also revise the code based on comments. For chat ser-
vice workloads, we derived scenarios from the ShareGPT
dataset [50], which mirrors real LLM chat conversations. Ac-
cording to the distribution of our measurement, we introduced
a random delay of 200 ∼300 ms to LLM requests to emulate
typical network overhead seen over the Internet. To create
realistic workloads, we documented the LLM responses us-
ing GPT-4 [41], ensuring the LLaMA models generated text
of similar length for system performance analysis. Table 2
presents the workloads and their optimizations in Parrot.
Baseline.
We benchmark Parrot against sate-of-the-art so-
lutions for building LLM applications and serving LLM re-
quests. The majority of LLM applications used in our baseline
Workload
Serving
Dependent
Requests.
Perf. Obj.
Deduction
Sharing
Prompt
App-centric
Scheduling
Data Analytics
✓
✓
✓
Serving Popular
LLM Applications
✓
✓
Multi-agent App.
✓
✓
✓
✓
Mixed Workloads
✓
✓
✓
Table 2: The workloads and the optimizations taking effect.
5
10
15
20
25
Requests/s
20
30
40
50
60
Mean Latency (ms)
Capacity=2048
Capacity=4096
Capacity=6144
Capacity=8192
Capacity=10240
Capacity=12288
(a) Mean Latency
5
10
15
20
25
Requests/s
20
30
40
50
60
P90 Latency (ms)
Capacity=2048
Capacity=4096
Capacity=6144
Capacity=8192
Capacity=10240
Capacity=12288
(b) P90 Latency
Figure 10: Latency (per output token) of vLLM with varying
token capacities and request rates. Requests are sampled from
ShareGPT [50] and their arrival time follows Poisson distri-
butions.
comparisons are developed using LangChain [8], which is the
predominant framework for LLM application development.
The LLM applications in baselines leverage OpenAI-style
chat completion APIs as provided by FastChat [62]. FastChat
is a widely recognized open-source LLM serving system
with over 30,000 stars on its repository. Incoming requests to
FastChat are allocated to LLM engines that run either Hug-
gingFace’s Transformers library [53] or vLLM [25], both of
which incorporate cutting-edge enhancements for LLM exe-
cution, such as FlashAttention [12], PagedAttention [25], and
continuous batching techniques [56]. The default scheduling
strategy employed by FastChat assigns incoming requests
to the LLM engine with the smallest current queue. Since
existing LLM services typically expose their functionality
through ""chat"" completion APIs, baseline assessments treat
all requests as independent and assume a high sensitivity to
latency. To manage token generation response times, each
LLM engine is subject to a capacity threshold, which is the
aggregate token count from all active requests on the engine.
Since existing LLM token generation is usually bound by
memory bandwidth, the per-token generation latency of an
engine is mainly affected by the number of running tokens in
a batch. As depicted in Figure 10, our experiments indicate
that the latency per output token, i.e. TPOT (Time-per-output-
token) for vLLM, with continuous batching enabled, experi-
ences a notable uptick when the engine’s workload using a
batch capacity beyond 6144. In our evaluation, we use the
setting that an LLM engine can keep its generation latency
under 40 ms/s for latency-sensitive requests, consistent with
our experience of OpenAI’s LLM services. When all LLM
engines hit their maximum capacity, any additional LLM re-
quests are queued in a FIFO (First In, First Out) manner,
awaiting the completion and release of resources by ongoing
tasks. Serving longer context (e.g., 32k or even 1M tokens)
within a satisfactory latency require either more GPUs using
tensor-parallel [49] or sequence-parallel [6] approaches, or
approximate attention (e.g., StreamingLLM [55]), which is
beyond the scope of this paper.
25
50
75
100
Output Length (# tokens)
0
50
100
150
200
250
Average Latency (s)
1.38x
1.21x
1.14x
1.11x
1.88x
1.64x
1.55x
1.52x
Parrot
Baseline (vLLM)
Baseline (HuggingFace)
(a) Output lengths
512
1024
1536
2048
Chunk Size (# tokens)
0
50
100
150
200
250
Average Latency (s)
1.21x
1.21x
1.20x
1.19x
1.63x
1.62x
1.60x
1.61x
Parrot
Baseline (vLLM)
Baseline (HuggingFace)
(b) Chunk sizes
Figure 11: Average E2E latency of chain summarization with
varying output lengths and chunk sizes.
8.2
Data Analytics on Long Documents
Our experimental analysis within data analytics randomly
picks ten long documents from the Arxiv-March dataset [27],
using chain-summary and map-reduce summary. Each docu-
ment has over 20,000 tokens. The results measures the mean
end-to-end latency across all documents.
Chain-style Applications.
Our evaluation demonstrates
how Parrot enhances chain summarization by mitigating the
excessive communication overhead stemming from client in-
teractions. Figure 11 presents the average end-to-end latency
for summarizing a single document using one LLM engine
(A100, LLaMA 13B) . We adjust the chunk size (the count of
tokens per chunk) and the output length, with results shown in
Figure 11a and Figure 11b, respectively. Parrot achieves a re-
duction in end-to-end latency by as much as 1.38× and 1.88×
compared to the baselines employing vLLM and Hugging-
Face, respectively. The efficiency of Parrot primarily stems
from the decreased network latency, which is a consequence
of reduced client interaction. As the output length increases,
the time spent on generation becomes more significant, lead-
ing to a diminishing advantage for Parrot over the baseline. By
increasing the chunk size, we decrease the number of chunks,
yet the extent of the speedup is contingent upon the network
latency savings for each chunk. Given that token generation is
substantially more time-consuming than prompt processing,
we observe a consistent speedup with variable chunk sizes
and a fixed output length (1.2× and 1.66× relative to vLLM
and HuggingFace, respectively). This indicates that Parrot’s
optimization for dependent LLM requests is particularly bene-
ficial for shorter outputs, which are prevalent in various LLM
applications such as summarization, short answer generation,
scoring, and choice provision. Due to HuggingFace’s slower
performance relative to vLLM, subsequent evaluations focus
solely on the comparison between Parrot and vLLM.
Figure 12a extends the evaluation by introducing back-
ground LLM requests at varying rates to examine the capa-
bility of Parrot in mitigating additional queuing delays for
dependent requests. Parrot slashes the end-to-end latency by a
factor of 2.38× in comparison to the baseline (vLLM). With
Parrot, as soon as the summary for the first chunk is completed,
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5
Request Rate (reqs/s)
50
100
150
200
250
Average Latency (s)
1.21x
1.19x
1.31x
1.79x
2.38x
Parrot
Baseline (vLLM)
(a) With background requests
10
15
20
25
Number of Apps
0
100
200
300
Average Latency (s)
1.38x
1.52x
1.63x
1.68x
Parrot
Baseline (vLLM)
(b) Multiple summary apps.
Figure 12: Average E2E latency of chain-summary with back-
ground requests or other chain-summary applications.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
Application No.
0
50
100
150
200
250
Latency in Baseline - Latency in Parrot (s)
Figure 13: The difference in E2E latency of the 25 chain-
summary application between Baseline and Parrot. All appli-
cations finish earlier in Parrot.
the subsequent chunk is processed immediately by incorporat-
ing the summaries of previous chunks into the prompt, which
aids in generating the summary for the next chunk. In con-
trast, the baseline treats all LLM requests individually. As a
result, in addition to the network latency from client interac-
tions, subsequent requests must re-enter the queue, leading
to added queuing delays. Figure 12b further illustrates the
end-to-end latency when multiple chain-summary applica-
tions are submitted concurrently, with each application tasked
with generating a summary for a separate document. Parrot
manages to reduce the average end-to-end latency for all ap-
plications by 1.68× without slowing down any applications
compared to the baseline according to Figure 13. The base-
line, by interleaving the execution of different applications,
exacerbates the slowdown of the end-to-end latency for all
applications. These experiments validate that recognizing the
interconnections of LLM requests can significantly enhance
end-to-end performance, as opposed to processing requests
in isolation.
Map-Reduce Applications.
An alternative implementation
of the document summarization application follows the map-
reduce paradigm as depicted in Figure 1a. This approach
consists of multiple parallel mapping LLM requests, where
each request summarizes a distinct segment of the document,
followed by a reducing LLM request that aggregates these
individual summaries into a final summary. As shown in
Figure 14, Parrot realizes a 2.37× acceleration over the base-
25
50
75
100
Output Length (# tokens)
0
10
20
30
40
Average Latency (s)
1.70x
2.04x
2.22x
2.37x
Parrot
Baseline (vLLM)
(a) Output lengths
512
1024
1536
2048
Chunk Size (# tokens)
0
10
20
30
Average Latency (s)
1.96x
2.07x
2.07x
2.16x
Parrot
Baseline (vLLM)
(b) Chunk sizes
Figure 14: Average E2E latency of Map-Reduce document
summary with varying output lengths and chunk sizes.
line with one LLM engine (A100, LLaMA 13B). Since the
mapping LLM requests are independent, they are dispatched
concurrently by both Parrot and the baseline. The primary ad-
vantage of Parrot stems from its deduction of a performance
objective that identifies the mapping tasks as a task group.
By recognizing this relationship, Parrot is capable of optimiz-
ing the latency of the entire task group through larger batch
sizes, which in turn enhances throughput. In contrast, the
baseline processes each LLM request in isolation, operating
under the presumption that they are all sensitive to latency.
This constrains the baseline to utilize a limited token capacity
(4096 tokens) on the LLM engine to achieve optimal latency
for individual tasks, which is detrimental to the end-to-end
performance of applications. It underscores the necessity for
LLM services to distinguish LLM requests to optimize the
end-to-end performance of varied LLM applications.
8.3
Serving Popular LLM Applications
Production applications need to face massive users. As ex-
plained in Figure 5, developers often need to use a very long
system prompt to define the behavior of LLMs. Therefore,
users of the same LLM application often use the shared
prompt, which can benefit from Parrot’s context fork mech-
anism and Parrot’s scheduling policy that co-locates LLM
requests sharing a long prompt prefix. Because we do not
have access to the intermediate steps of Bing Copilot, we only
evaluate the final request generating the response to users.
We synthesized 64 requests from the length distribution we
measured using Bing Copilot. The system prompt length is
about 6000 tokens. The output lengths ranges from 180 to
800 tokens. Figure 15 shows the average request latency of
Bing Copilot of Parrot and the baselines. Because the LLM
service in the baseline system does not know the prompt struc-
ture, it is hard to infer the shared prompt from massive LLM
requests. Compared to the baseline without sharing prompt,
Parrot achieves 1.8× ∼2.4× speedup for batch sizes of 8 and
16. Further increasing the batch size leads to out-of-memory
due to the massive KV cache of shared system prompt. We
also build an advanced baseline using vLLM’s paged atten-
tion to support sharing the prompt with a static prefix. Both
8
16
32
64
Batch Size
0
10
20
30
40
Avg. Latency (s)
1.1x
1.3x
1.4x
1.7x
1.8x
2.4x
x
x
Parrot
Baseline w/ Sharing
Baseline w/o Sharing
Figure 15: Latency of Bing Copilot with varying batch sizes.
200
400
600
800
Output Length (# tokens)
0.00
0.02
0.04
0.06
0.08
0.10
0.12
Latency per token (s)
1.44x
1.53x
1.56x
1.58x
Parrot
Baseline w/ Sharing
(a) Batch Size = 32
100
200
300
400 480
Output Length (# tokens)
0.00
0.05
0.10
0.15
0.20
0.25
Latency per token (s)
1.44x
1.64x
1.74x
1.81x
1.84x
Parrot
Baseline w/ Sharing
(b) Batch Size = 64
Figure 16: Latency per output token of Bing Copilot.
Parrot and vLLM use the paged memory management [25],
thus both systems can hold the same number of tokens in
an LLM engine (A100, LLaMA 7B). Parrot further achieves
1.1× ∼1.7× speedup over vLLM because of the better GPU
kernel. Although vLLM can save extra memory usage of the
shared prompt, its GPU kernel still has to reload the tokens
repeatedly. Given that the token generation of LLMs is bound
by memory bandwidth, such redundant memory loading slows
down the end-to-end inference. By combining FlashAtten-
tion and PagedAttention, Parrot only needs to load the tokens
of the shared prompt once, when computing the attention
from the diverged tokens of different users. Parrot’s speedup
of shared prompt mainly comes from the token generation,
thus the longer output length leads to higher improvement.
Figure 16 shows Parrot achieves 1.58× and 1.84× speedup
compared to vLLM using paged attention, showing 40 ms
per-output-token latency at a batch size of 32.
In Figure 17, we further evaluated the serving of multiple
GPTs applications [42], each of which has multiple users, in
a multi-GPU cluster. Four A6000 (48GB) GPUs are deployed
with four LLM engines (LLaMA 7B). We select four GPTs
applications in four popular categories including productivity,
programming, image generation, and data analysis. The LLM
requests are randomly generated from the four categories with
equal probability. LLM requests arrive at fixed rates following
Poisson distribution. Parrot can sustain 12× higher request
rates compared to the baseline without sharing. Because the
baseline’s scheduling policy is not aware of the shared prompt
within each LLM application, the requests are mixed in all
LLM engines making it impossible to reuse the common
prompt prefix. Parrot’s scheduling policy co-locates LLM
requests of the same applications to maximize the sharing op-
0 1 2 3 4 5 6 7 8 9 10111213141516
Request rate (req/s)
0
100
200
300
Normalized latency
 (ms/token)
Parrot
Parrot w/ PagedAttention
Parrot w/o Scheduling
Baseline (vLLM)
Figure 17: Serving multiple GPTs applications.
portunity, achieving both lower inference latency and higher
cluster throughput. After turning off such affinity scheduling
policy, Parrot only exhibits 3× higher request rates compared
to the baseline, because the requests with shared prefix are
often dispatched to different engines thus reduced the sharing
opportunities. Moreover, Parrot’s attention kernel helps Parrot
to achieve 2.4× higher rate compared to Parrot using vLLM’s
PagedAttention, by avoiding the redundant memory loading
for attention of shared prompts.
8.4
Multi-agent Applications
We assess the performance of multi-agent systems utiliz-
ing MetaGPT [22] within Parrot. A workflow is constructed
with three distinct roles. Initially, the Architect outlines the
project’s file structures and specifies APIs within each file
for a given task. Subsequently, multiple Coders undertake the
project implementation, with each focusing on a specific file.
Following the integration of the code from all files, several
Reviewers engage in the process, each examining and com-
menting on a single file. The Coders then revise their code
based on these comments. This review-and-revision cycle
is iterated three times to produce the final code. Figure 18
illustrates the latency and memory consumption of Parrot
compared to baseline systems on one A100 running LLaMA
13B. Parrot achieves a speedup of up to 11.7× compared
with the latency-centric baseline. The primary improvement
is attributed to Parrot’s capability to deduct the performance
objectives for LLM requests based on the end-to-end perfor-
mance criteria. For this specific multi-agent scenario, the goal
is to minimize the time taken to deliver the final code. Parrot
identifies multiple task groups within the parallel processes of
coding, reviewing, and revising, facilitating larger batch sizes
to enhance throughput and reduce the completion time of task
groups. We also contrast Parrot with an throughput-centric
baseline that uses larger batch on purpose to optimize cluster
throughput, which also shows higher concurrency and better
completion time than the latency-centric baseline.
Even when compared to the throughput-centric baseline,
Parrot demonstrates superiority, being faster by up to 2.45×.
This enhancement mainly stems from Parrot’s ability to
2400
2600
11.7x
4
8
12
16
Number of Files
0
500
1000
1500
Average Latency (s)
1.00x
1.04x
1.14x
1.16x
1.22x
1.61x
1.88x
2.35x
1.27x
1.58x
2.03x
2.45x
7.19x
4.9x
3.0x
Parrot
Parrot w/ PagedAttention
Parrot w/o Sharing
Baseline (vLLM, Throughput)
Baseline (vLLM, Latency)
(a) End-to-end Latency
4
8
12
16
Number of Files
0
10
20
30
40
50
GPU Memory of
KV Cache (GB)
GPU Memory Capacity
Parrot
Parrot w/o Sharing
(b) GPU Memory of KV Cache
Figure 18: The latency and memory usage for multi-agent
programming, with varying number of files to program.
decrease redundancy through its prompt structure analysis,
which contributes a 2.35× acceleration. Given the interactive
nature of the roles in MetaGPT, there is considerable overlap
in the context among different roles, which Parrot capitalizes
on by sharing this common context as a prompt prefix. The
static prefix sharing mechanism from vLLM does not work
in this dynamic scenario. Without a grasp of the prompt’s
structure, it cannot identify dynamically generated Semantic
Variables that could also be shared during runtime. As de-
picted in Figure 18b, Parrot without this sharing capability
would hit the GPU memory ceiling. Additionally, Parrot’s spe-
cialized GPU kernel for processing the shared prefix achieves
a further 1.2× speedup when there are 16 files, compared to
using vLLM’s PagedAttention, due to the reduced memory
transactions.
8.5
Scheduling of Mixed Workloads
To assess the performance of Parrot on a multi-GPU setup, we
configure a cluster with four A6000 (48GB) GPUs, each host-
ing a separate LLM engine (LLaMA 7B), resulting in a total
of four LLM engines. We emulate a real-world scenario where
LLM services encounter a variety of demands by injecting a
mix of requests from chat applications at a rate of 1 req/s and
from data analytic tasks (i.e., map-reduce applications) previ-
ously analyzed in §8.2. Requests from the chat applications
are characterized by their need for low latency, whereas the
map-reduce applications prioritize high throughput, creating a
challenge when they are concurrently processed by the same
LLM engine. We benchmark Parrot against two reference
implementations: one tailored for latency, limiting engine ca-
pacity to reduce decoding time, and another for throughput,
0
200
400
600
800
149.1
184.6
827.6
Average Chat
 Normalized Latency (ms)
0
20
40
60
80
45.1
77.8
41.4
Average Chat
Decode Time (ms)
0
20
40
60
80
100
23.2
24.5
86.4
Average
Map-Reduce JCT (s)
Parrot
Baseline (Throughput)
Baseline (Latency)
Figure 19: The mixture of chat and map-reduce applications.
utilizing full engine capacity to maximize GPU utilization.
The results depicted in Figure 19 demonstrate that Par-
rot attains a 5.5× and 1.23× improvement in normalized
latency (measured as request latency per number of output
tokens) [25, 56] for chat applications in comparison to the
latency-focused and throughput-focused baselines, respec-
tively. In terms of token generation speed for chat applications,
Parrot delivers performance on par with the latency-centric
baseline and outperforms the throughput-centric baseline by
1.72×. For map-reduce applications, Parrot reaches a 3.7×
speedup over the latency-centric baseline and is 1.05× more
efficient than the throughput-centric baseline. Parrot excels
by providing both low latency for chat applications and high
throughput for map-reduce applications. It mitigates the con-
tention between chat and map-reduce workloads by intelli-
gently scheduling them on separate engines. These findings
underscore the significance of specialized handling for diverse
requests to enhance the overall performance of LLM services.
9
Related Works
Deep Learning Serving Systems.
The field of model serv-
ing has seen a surge of research activity in recent years,
with many systems developed to address the different chal-
lenges of deep learning model deployment. The systems in-
clude Clipper [10], TensorFlow Serving [39], Clockwork [19],
REEF [20], AlpaServe [28], which have explored many as-
pects including batching, caching, placement, scheduling,
model parallelism for the serving of single or multiple models.
These systems were proposed for serving general deep learn-
ing models, which have less consideration about the unique
requirements of large language models, e.g., autoregressive
decoding. Orca [56] proposed a fine-grained scheduling mech-
anism that can batch multiple LLM requests at the iteration
level, which is also known as continuous batching. vLLM
proposes PagedAttention [25] allows the batching of LLM
requests with different lengths using non-contiguous memory,
increasing memory utilization. These systems for LLM serv-
ing still treat LLM requests separately, missing the opportuni-
ties to understand the interconnections within an application
and exploit the commonality of different requests. Parrot is
orthogonal to them. With more application-level knowledge
exposed by Semantic Variables, Parrot can do data flow analy-
sis on LLM requests, which enables a brand new optimization
space with the final goal of optimizing the end-to-end perfor-
mance of applications, rather than individual requests.
LLM Orchestrator Frameworks.
LLM orchestration
frameworks help developers create and manage applications
powered by LLMs. They simplify the process of prompt de-
sign, and orchestration of multiple LLM requests, which en-
able developers to interact with LLMs easily. LangChain [8]
is a Python framework that provides many workflow patterns,
e.g., chain, map-reduce so that developers can easily cus-
tomize their own LLM applications. Semantic Kernel [36]
introduces Planners are semantic agents that can automati-
cally generate plans based on the needs of the users. Prompt-
Flow [35] supports chains of native and semantic functions
and visualizes them as a graph. LlamaIndex [29] allows de-
velopers to use natural language queries to retrieve relevant
documents. Parrot is orthogonal to these frameworks and can
be easily integrated with these frameworks to support Parrot’s
APIs with Semantic Variable abstraction, as discussed in §6.
DAG-aware System Optimizations.
Dependency graphs
or DAGs (Directed Acyclic Graphs) widely exist in many
kinds of systems, and many optimizations have been proposed
to optimize the systems by exploiting the DAG information.
Tez [4], Dryad [23], and Graphene [16] use the task depen-
dency to optimize the scheduling and packing of parallel data
analytic workloads. SONIC [30], Caerus [59], and Orion [31]
optimize serverless functions from the aspects of communica-
tion, latency, and cost. Parrot learns from the previous system
works and realizes the importance of correlations of LLM
requests to optimize the end-to-end performance of LLM ap-
plications. This motivates Parrot to build APIs for exposing
such dependency information. Moreover, it is unique to LLM
applications to understand the prompt structure in addition to
request-level dependency, which is necessary for communica-
tion and identifying commonality across LLM requests. This
motivates us to propose the Semantic Variable abstraction,
instead of just using a DAG of requests.
10
Conclusion
This paper proposes Parrot that treats LLM applications as
first-class citizens and targets to optimize the end-to-end per-
formance of LLM applications, instead of only optimizing
individual LLM requests. We propose Semantic Variable as
the key abstraction that exposes the dependency and common-
ality of LLM requests, enabling a new optimization space.
Our evaluation shows Parrot can optimize LLM-based ap-
plications by up to 11.7×. We envision this new angle of
efficiency improvement of LLM applications brings a broad
future direction to study other scheduling features like the
fairness of end-to-end performance of LLM applications.
Acknowledgments
We thank the anonymous reviewers and the shepherd for their
constructive feedback and suggestions. Zhenhua Han, Yuqing
Yang and Chen Chen are the corresponding authors.
References
[1] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng
Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, San-
jay Ghemawat, Geoffrey Irving, Michael Isard, Man-
junath Kudlur, Josh Levenberg, Rajat Monga, Sherry
Moore, Derek G. Murray, Benoit Steiner, Paul Tucker,
Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu,
and Xiaoqiang Zheng. TensorFlow: A system for Large-
Scale machine learning. In 12th USENIX Symposium on
Operating Systems Design and Implementation (OSDI
16), pages 265–283, Savannah, GA, November 2016.
USENIX Association.
[2] Amey Agrawal, Nitin Kedia, Ashish Panwar, Jayashree
Mohan, Nipun Kwatra, Bhargav S Gulavani, Alexey Tu-
manov, and Ramachandran Ramjee. Taming throughput-
latency tradeoff in llm inference with sarathi-serve.
arXiv preprint arXiv:2403.02310, 2024.
[3] Ganesh Ananthanarayanan, Srikanth Kandula, Albert
Greenberg, Ion Stoica, Yi Lu, Bikas Saha, and Edward
Harris. Reining in the outliers in Map-Reduce clusters
using mantri. In 9th USENIX Symposium on Operating
Systems Design and Implementation (OSDI 10), Van-
couver, BC, October 2010. USENIX Association.
[4] Apache. Tez. https://tez.apache.org/, November
2019.
[5] Apache. Kafka. https://kafka.apache.org/, Octo-
ber 2023.
[6] Zhengda Bian, Hongxin Liu, Boxiang Wang, Haichen
Huang, Yongbin Li, Chuanrui Wang, Fan Cui, and Yang
You. Colossal-ai: A unified deep learning system for
large-scale parallel training. CoRR, abs/2110.14883,
2021.
[7] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan,
Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee,
Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori,
Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang.
Sparks of artificial general intelligence: Early experi-
ments with gpt-4, 2023.
[8] Harrison Chase. LangChain. https://github.com/
langchain-ai/langchain, October 2022.
[9] Lequn Chen. Dissecting batching effects in gpt infer-
ence.
https://le.qun.ch/en/blog/2023/05/13/
transformer-batching/, May 2023.
[10] Daniel Crankshaw, Xin Wang, Guilio Zhou, Michael J.
Franklin, Joseph E. Gonzalez, and Ion Stoica. Clipper:
A Low-Latency online prediction serving system. In
14th USENIX Symposium on Networked Systems Design
and Implementation (NSDI 17), pages 613–627, Boston,
MA, March 2017. USENIX Association.
[11] Tri Dao. Flashattention-2: Faster attention with bet-
ter parallelism and work partitioning. arXiv preprint
arXiv:2307.08691, 2023.
[12] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and
Christopher Ré.
Flashattention: Fast and memory-
efficient exact attention with io-awareness. In S. Koyejo,
S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and
A. Oh, editors, Advances in Neural Information Process-
ing Systems, volume 35, pages 16344–16359. Curran
Associates, Inc., 2022.
[13] Bill Gates. Ai is about to completely change how you
use computers and upend the software industry. https:
//www.gatesnotes.com/AI-agents, Nov 2023.
[14] Google. Google bard. https://bard.google.com/,
Nov 2023.
[15] Robert Grandl, Mosharaf Chowdhury, Aditya Akella,
and Ganesh Ananthanarayanan. Altruistic scheduling
in Multi-Resource clusters. In 12th USENIX Sympo-
sium on Operating Systems Design and Implementa-
tion (OSDI 16), pages 65–80, Savannah, GA, November
2016. USENIX Association.
[16] Robert Grandl, Srikanth Kandula, Sriram Rao, Aditya
Akella, and Janardhan Kulkarni. GRAPHENE: Packing
and Dependency-Aware scheduling for Data-Parallel
clusters. In 12th USENIX Symposium on Operating
Systems Design and Implementation (OSDI 16), pages
81–97, Savannah, GA, November 2016. USENIX Asso-
ciation.
[17] Juncheng Gu, Mosharaf Chowdhury, Kang G. Shin,
Yibo Zhu, Myeongjae Jeon, Junjie Qian, Hongqiang Liu,
and Chuanxiong Guo. Tiresias: A GPU cluster manager
for distributed deep learning. In 16th USENIX Sympo-
sium on Networked Systems Design and Implementation
(NSDI 19), pages 485–500, Boston, MA, February 2019.
USENIX Association.
[18] guidance ai.
Guidance.
https://github.com/
guidance-ai/guidance, November 2023.
[19] Arpan Gujarati, Reza Karimi, Safya Alzayat, Wei Hao,
Antoine Kaufmann, Ymir Vigfusson, and Jonathan
Mace. Serving DNNs like clockwork: Performance
predictability from the bottom up. In 14th USENIX Sym-
posium on Operating Systems Design and Implementa-
tion (OSDI 20), pages 443–462. USENIX Association,
November 2020.
[20] Mingcong Han, Hanze Zhang, Rong Chen, and Haibo
Chen. Microsecond-scale preemption for concurrent
GPU-accelerated DNN inferences. In 16th USENIX
Symposium on Operating Systems Design and Imple-
mentation (OSDI 22), pages 539–558, Carlsbad, CA,
July 2022. USENIX Association.
[21] Connor Holmes, Masahiro Tanaka, Michael Wyatt, Am-
mar Ahmad Awan, Jeff Rasley, Samyam Rajbhan-
dari, Reza Yazdani Aminabadi, Heyang Qin, Arash
Bakhtiari, Lev Kurilenko, et al.
Deepspeed-fastgen:
High-throughput text generation for llms via mii and
deepspeed-inference. arXiv preprint arXiv:2401.08671,
2024.
[22] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng
Cheng, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau,
Zijuan Lin, Liyang Zhou, Chenyu Ran, et al. Metagpt:
Meta programming for multi-agent collaborative frame-
work. arXiv preprint arXiv:2308.00352, 2023.
[23] Michael Isard, Mihai Budiu, Yuan Yu, Andrew Birrell,
and Dennis Fetterly. Dryad: Distributed data-parallel
programs from sequential building blocks. In Proceed-
ings of the 2nd ACM SIGOPS/EuroSys European Con-
ference on Computer Systems 2007, EuroSys ’07, page
59–72, New York, NY, USA, 2007. Association for Com-
puting Machinery.
[24] Suhas Jayaram Subramanya, Daiyaan Arfeen, Shouxu
Lin, Aurick Qiao, Zhihao Jia, and Gregory R. Ganger.
Sia: Heterogeneity-aware, goodput-optimized ml-cluster
scheduling. In Proceedings of the 29th Symposium on
Operating Systems Principles, SOSP ’23, page 642–657,
New York, NY, USA, 2023. Association for Computing
Machinery.
[25] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying
Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez,
Hao Zhang, and Ion Stoica. Efficient memory man-
agement for large language model serving with page-
dattention. In Proceedings of the 29th Symposium on
Operating Systems Principles, SOSP ’23, page 611–626,
New York, NY, USA, 2023. Association for Computing
Machinery.
[26] Benjamin
Lefaudeux, Francisco
Massa, Diana
Liskovich, Wenhan Xiong, Vittorio Caggiano, Sean
Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang,
Patrick Labatut, and Daniel Haziza. xformers: A modu-
lar and hackable transformer modelling library. https:
//github.com/facebookresearch/xformers,
2022.
[27] Yucheng Li. Unlocking context constraints of llms: En-
hancing context efficiency of llms with self-information-
based content filtering, 2023.
[28] Zhuohan Li, Lianmin Zheng, Yinmin Zhong, Vincent
Liu, Ying Sheng, Xin Jin, Yanping Huang, Zhifeng Chen,
Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Al-
paServe: Statistical multiplexing with model parallelism
for deep learning serving.
In 17th USENIX Sympo-
sium on Operating Systems Design and Implementa-
tion (OSDI 23), pages 663–679, Boston, MA, July 2023.
USENIX Association.
[29] Jerry Liu. LlamaIndex, November 2022.
[30] Ashraf Mahgoub, Karthick Shankar, Subrata Mitra,
Ana Klimovic, Somali Chaterji, and Saurabh Bagchi.
SONIC: Application-aware data passing for chained
serverless applications. In 2021 USENIX Annual Tech-
nical Conference (USENIX ATC 21), pages 285–301.
USENIX Association, July 2021.
[31] Ashraf Mahgoub, Edgardo
Barsallo
Yi, Karthick
Shankar, Sameh Elnikety, Somali Chaterji, and Saurabh
Bagchi. ORION and the three rights: Sizing, bundling,
and prewarming for serverless DAGs. In 16th USENIX
Symposium on Operating Systems Design and Imple-
mentation (OSDI 22), pages 303–320, Carlsbad, CA,
July 2022. USENIX Association.
[32] Microsoft. Bing chat. https://www.bing.com/chat,
Nov 2023.
[33] Microsoft.
Meeting
recap
in
microsoft
teams.
https://www.microsoft.com/en-us/
microsoft-teams/premium, May 2023.
[34] Microsoft.
Microsoft 365
copilot.
https:
//www.microsoft.com/en-us/microsoft-365/
enterprise/microsoft-365-copilot, Mar 2023.
[35] Microsoft.
PromptFlow.
https://github.com/
microsoft/promptflow, November 2023.
[36] Microsoft. Semantic Kernel. https://github.com/
microsoft/semantic-kernel, November 2023.
[37] Deepak
Narayanan, Keshav
Santhanam, Fiodar
Kazhamiaka, Amar Phanishayee, and Matei Zaharia.
Heterogeneity-Aware cluster scheduling policies for
deep learning workloads. In 14th USENIX Symposium
on Operating Systems Design and Implementation
(OSDI 20), pages 481–498. USENIX Association,
November 2020.
[38] Flemming Nielson, Hanne R Nielson, and Chris Hankin.
Principles of program analysis. Springer, 2015.
[39] Christopher Olston, Fangwei Li, Jeremiah Harmsen, Jor-
dan Soyke, Kiril Gorovoy, Li Lao, Noah Fiedel, Sukriti
Ramesh, and Vinu Rajashekhar. Tensorflow-serving:
Flexible, high-performance ml serving. In Workshop on
ML Systems at NIPS 2017, 2017.
[40] OpenAI. Chatgpt. https://chat.openai.com/, Nov
2023.
[41] OpenAI. Gpt-4 technical report, 2023.
[42] OpenAI.
Introducing gpts.
https://openai.com/
blog/introducing-gpts, Nov 2023.
[43] OpenAI.
OpenAI Triton.
https://github.com/
openai/triton, November 2023.
[44] OpenAI.
Production
best
practices
-
ope-
nai
api.
https://platform.openai.com/
docs/guides/production-best-practices/
improving-latencies, Nov 2023.
[45] Adam Paszke, Sam Gross, Soumith Chintala, Gregory
Chanan, Edward Yang, Zachary DeVito, Zeming Lin,
Alban Desmaison, Luca Antiga, and Adam Lerer. Auto-
matic differentiation in pytorch. 2017.
[46] Pratyush Patel, Esha Choukse, Chaojie Zhang, Íñigo
Goiri, Aashaka Shah, Saeed Maleki, and Ricardo Bian-
chini. Splitwise: Efficient generative llm inference using
phase splitting. arXiv preprint arXiv:2311.18677, 2023.
[47] Chen Qian, Xin Cong, Cheng Yang, Weize Chen,
Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong
Sun. Communicative agents for software development.
arXiv preprint arXiv:2307.07924, 2023.
[48] Sebastián Ramírez. FastAPI. https://github.com/
tiangolo/fastapi.
[49] Mohammad Shoeybi, Mostofa Patwary, Raul Puri,
Patrick LeGresley, Jared Casper, and Bryan Catanzaro.
Megatron-lm: Training multi-billion parameter language
models using model parallelism. CoRR, abs/1909.08053,
2019.
[50] ShareGPT Team.
Sharegpt dataset.
https://
sharegpt.com/, Nov 2023.
[51] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix, Bap-
tiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar,
Aurelien Rodriguez, Armand Joulin, Edouard Grave,
and Guillaume Lample. Llama: Open and efficient foun-
dation language models, 2023.
[52] Unknown.
Prompt of bing
chat.
https:
//www.make-safe-ai.com/is-bing-chat-safe/
Prompts_Conversations.txt, Nov 2023.
[53] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Perric
Cistac, Clara Ma, Yacine Jernite, Julien Plu, Canwen
Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, and Alexander M. Rush. Transformers:
State-of-the-Art Natural Language Processing. pages
38–45. Association for Computational Linguistics, Oc-
tober 2020.
[54] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu,
Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xi-
aoyun Zhang, and Chi Wang. Autogen: Enabling next-
gen llm applications via multi-agent conversation frame-
work. arXiv preprint arXiv:2308.08155, 2023.
[55] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song
Han, and Mike Lewis. Efficient streaming language
models with attention sinks. arXiv, 2023.
[56] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soo-
jeong Kim, and Byung-Gon Chun. Orca: A distributed
serving system for Transformer-Based generative mod-
els. In 16th USENIX Symposium on Operating Systems
Design and Implementation (OSDI 22), pages 521–538,
Carlsbad, CA, July 2022. USENIX Association.
[57] Matei Zaharia, Dhruba Borthakur, Joydeep Sen Sarma,
Khaled Elmeleegy, Scott Shenker, and Ion Stoica. Delay
scheduling: A simple technique for achieving locality
and fairness in cluster scheduling. In Proceedings of
the 5th European Conference on Computer Systems,
EuroSys ’10, page 265–278, New York, NY, USA, 2010.
Association for Computing Machinery.
[58] Matei Zaharia, Andy Konwinski, Anthony D Joseph,
Randy H Katz, and Ion Stoica. Improving mapreduce
performance in heterogeneous environments. In 8th
USENIX Symposium on Operating Systems Design and
Implementation (OSDI 08), San Diego, CA, 2008.
[59] Hong Zhang, Yupeng Tang, Anurag Khandelwal, Jin-
grong Chen, and Ion Stoica. Caerus: NIMBLE task
scheduling for serverless analytics. In 18th USENIX
Symposium on Networked Systems Design and Imple-
mentation (NSDI 21), pages 653–669. USENIX Associ-
ation, April 2021.
[60] Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher De-
wan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-
haylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel
Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang,
and Luke Zettlemoyer. Opt: Open pre-trained trans-
former language models, 2022.
[61] Hanyu Zhao, Zhenhua Han, Zhi Yang, Quanlu Zhang,
Fan Yang, Lidong Zhou, Mao Yang, Francis C.M. Lau,
Yuqi Wang, Yifan Xiong, and Bin Wang. HiveD: Sharing
a GPU cluster for deep learning with guarantees. In 14th
USENIX Symposium on Operating Systems Design and
Implementation (OSDI 20), pages 515–532. USENIX
Association, November 2020.
[62] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuo-
han Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E.
Gonzalez, and Ion Stoica. Judging llm-as-a-judge with
mt-bench and chatbot arena, 2023.
[63] Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Jeff
Huang, Chuyue Sun, Cody Hao Yu, Shiyi Cao, Chris-
tos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark
Barrett, and Ying Sheng. Efficiently programming large
language models using sglang, 2023.
[64] Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu,
Yibo Zhu, Xuanzhe Liu, Xin Jin, and Hao Zhang. Dist-
serve: Disaggregating prefill and decoding for goodput-
optimized large language model serving. arXiv preprint
arXiv:2401.09670, 2024.
"
2,3,"A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends","Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, Jiachi Chen","General large language models (LLMs), represented by ChatGPT, have
demonstrated significant potential in tasks such as code generation in software
engineering. This has led to the development of specialized LLMs for software
engineering, known as Code LLMs. A considerable portion of Code LLMs is derived
from general LLMs through model fine-tuning. As a result, Code LLMs are often
updated frequently and their performance can be influenced by the base LLMs.
However, there is currently a lack of systematic investigation into Code LLMs
and their performance. In this study, we conduct a comprehensive survey and
analysis of the types of Code LLMs and their differences in performance
compared to general LLMs. We aim to address three questions: (1) What LLMs are
specifically designed for software engineering tasks, and what is the
relationship between these Code LLMs? (2) Do Code LLMs really outperform
general LLMs in software engineering tasks? (3) Which LLMs are more proficient
in different software engineering tasks? To answer these questions, we first
collect relevant literature and work from five major databases and open-source
communities, resulting in 134 works for analysis. Next, we categorize the Code
LLMs based on their publishers and examine their relationships with general
LLMs and among themselves. Furthermore, we investigate the performance
differences between general LLMs and Code LLMs in various software engineering
tasks to demonstrate the impact of base models and Code LLMs. Finally, we
comprehensively maintained the performance of LLMs across multiple mainstream
benchmarks to identify the best-performing LLMs for each software engineering
task. Our research not only assists developers of Code LLMs in choosing base
models for the development of more advanced LLMs but also provides insights for
practitioners to better understand key improvement directions for Code LLMs.","A Survey of Large Language Models for Code: Evolution,
Benchmarking, and Future Trends
ZIBIN ZHENG, Sun Yat-sen University, China
KAIWEN NING, Sun Yat-sen University, China
YANLIN WANG∗, Sun Yat-sen University, China
JINGWEN ZHANG, Sun Yat-sen University, China
DEWU ZHENG, Sun Yat-sen University, China
MINGXI YE, Sun Yat-sen University, China
JIACHI CHEN, Sun Yat-sen University, China
General large language models (LLMs), represented by ChatGPT, have demonstrated significant potential
in software engineering tasks such as code generation. This led to the development of specialized LLMs for
software engineering, called Code LLMs. Further, Code LLMs are often derived from general LLMs through
fine-tuning and their performance can be influenced by the base LLMs. However, there is a lack of systematic
investigation into Code LLMs. In this study, we conduct a comprehensive survey of Code LLMs to address three
questions: (1) What LLMs are specifically designed for software engineering tasks, and their relationship? (2)
Do Code LLMs outperform general LLMs in software engineering tasks? (3) Which LLMs are more proficient
in different software engineering tasks? To answer these questions, we first collect relevant literature and work
from four databases and categorize Code LLMs based on their publishers. Next, we investigate the performance
differences between general LLMs and Code LLMs in software engineering tasks to demonstrate future trends.
Finally, we comprehensively maintained the performance of LLMs to identify the best-performing LLMs for
each software engineering task. Our research helps researchers understand the evolution and performance of
Code LLMs and provides insights for practitioners to improve Code LLMs.
Additional Key Words and Phrases: large language models (LLMs), Code LLMs, software engineering tasks
ACM Reference Format:
Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen. 2024. A
Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends. ACM Trans. Softw.
Eng. Methodol. 1, 1 (January 2024), 44 pages. https://doi.org/10.1145/3488245
1
INTRODUCTION
Large Language Models (LLMs) are blurring the boundaries between human languages and machine
languages with their powerful text understanding and generation capabilities [34]. LLMs have
also had a significant impact on various fields, including software engineering [115]. Currently, a
∗Yanlin Wang is the corresponding author (wangylin36@mail.sysu.edu.cn).
Authors’ addresses: Zibin Zheng, Sun Yat-sen University, Zhuhai, China, zhzibin@mail.sysu.edu.cn; Kaiwen Ning, Sun
Yat-sen University, Zhuhai, China, ningkw@mail2.sysu.edu.cn; Yanlin Wang, Sun Yat-sen University, Zhuhai, China,
wangylin36@mail.sysu.edu.cn; Jingwen Zhang, Sun Yat-sen University, Zhuhai, China, zhangjw273@mail2.sysu.edu.cn;
Dewu Zheng, Sun Yat-sen University, Zhuhai, China, zhengdw5@mail2.sysu.edu.cn; Mingxi Ye, Sun Yat-sen University,
Zhuhai, China, yemx6@mail2.sysu.edu.cn; Jiachi Chen, Sun Yat-sen University, Zhuhai, China, chenjch86@mail.sysu.edu.cn.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee
provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and
the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.
Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires
prior specific permission and/or a fee. Request permissions from permissions@acm.org.
© 2024 Association for Computing Machinery.
1049-331X/2024/1-ART $15.00
https://doi.org/10.1145/3488245
ACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article . Publication date: January 2024.
arXiv:2311.10372v2  [cs.SE]  8 Jan 2024
2
Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen
considerable amount of work focuses on evaluating and optimizing the performance of LLMs on
software engineering tasks such as code generation [123], code summarization [2], vulnerability
mining [114], and more [15, 117]. However, many studies adopt a negative stance on the current
performance of LLMs in the field of software engineering [51, 70, 125]. This is largely due to
the fact that LLMs may not effectively comprehend the structure and semantics of code, lacking
domain-specific knowledge in software engineering [59, 85]. Fortunately, we can enhance the
performance and adaptability of LLMs in specific tasks within the software engineering domain
through fine-tuning and additional training [11], thereby designing LLMs specifically tailored for
software engineering tasks. We refer to these LLMs designed for software engineering tasks as
Code LLMs [10].
Code LLMs can not only be generated through fine-tuning but also through traditional pre-
training methods [50]. As language models specifically developed for the field of software engineer-
ing, ideally, LLMs can have their capabilities further enhanced and possess stronger abilities in code
generation, vulnerability mining, and other tasks [55]. Therefore, there is currently a considerable
amount of work focusing on this area, resulting in the development of many milestone Code LLMs
such as Codex [60], Code Llama [78].
However, due to the uncertainties brought by fine-tuning, training data, and other factors [82, 119],
as well as the remarkable performance of state-of-the-art general LLMs like GPT-4 in various
software engineering domains [98], it is challenging to determine whether the current state-of-
the-art Code LLMs outperform the current state-of-the-art general LLMs in software engineering
tasks. Moreover, there is a wide range of Code LLMs available, many of which are derived through
fine-tuning on general LLMs or other Code LLMs [21, 62, 72, 120]. They may possess different
structures and fine-tuning techniques [3, 64], and it is likely that they excel in different software
engineering tasks. However, there is currently a lack of systematic investigation into Code LLMs
and their performance, particularly in comparison to general LLMs and among different Code
LLMs.
In this study, our goal is to provide a comprehensive review of Code LLMs and their performance.
To achieve this objective, we initially collected relevant literature and works from four major
databases known for publishing new models: GitHub1, dblp2, Google Scholar3, and arXiv4. We
employed a card sorting method [14] to remove duplicate and irrelevant papers and further expanded
our list of research targets through a snowballing approach. After the screening process, we
obtained 149 relevant and valid papers. Through this research, we aim to address the following
three questions:
RQ1: What LLMs are specifically designed for software engineering tasks, and what is the
relationship between them?
We classified Code LLMs based on the types of institutions to which their main developers
belong, such as companies, universities, etc. We not only conducted a chronological review of
Code LLMs but also provided a comprehensive summary of their development relationships. These
development relationships include but are not limited to, iterations, fine-tuning, and improvements.
RQ2: Do Code LLMs really outperform general LLMs in software engineering tasks?
To address this question, we selected relevant content from the aforementioned literature,
specifically focusing on experimental sections or evaluation work that compared general LLMs
and Code LLMs. We conducted a detailed statistical analysis and presentation of the experimental
1https://github.com/
2https://dblp.uni-trier.de/
3https://scholar.google.com
4https://arxiv.org/
A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends
3
findings or evaluation results. Our findings indicate that, for the same model, the newly fine-
tuned models specifically tailored for software engineering tasks outperform their base models.
Furthermore, when the number of parameters is comparable, Code LLMs tend to outperform
general LLMs. Overall, the current state-of-the-art Code LLMs, such as CodeFuse-CodeLlama-34B,
show better performance than the current state-of-the-art general LLMs, such as GPT-4, in code
generation tasks. However, GPT-4 still demonstrates strong competitiveness in other tasks.
RQ3: Which LLMs are more proficient in different software engineering tasks?
We have comprehensively summarized the experimental part of the aforementioned work, manu-
ally annotating the performance of 130 Code LLMs on major benchmarks such as HumanEval [16].
Additionally, a considerable amount of research has utilized alternative evaluation methods or
proposed new benchmarks and evaluation metrics. We have also organized and presented the
experimental results from these studies. We found that different Code LLMs do exhibit certain
performance differences across various software engineering tasks. Some of these findings can
assist developers of Code LLMs in making better choices regarding base models and fine-tuning
approaches to develop more advanced LLMs tailored to different software engineering tasks.
The main contributions of this paper are:
• To the best of our knowledge, this is the first systematic review of Code LLMs. We manually
selected 149 relevant works from a large number of articles in four major open-source
communities or databases5;
• We classified Code LLMs based on the types of institutions to which their main developers
belong and organized them chronologically and in terms of their development relationships.
This can help practitioners better understand and utilize Code LLMs;
• We conducted a comprehensive analysis and compilation of the performance of general
LLMs and Code LLMs in software engineering tasks. We provided statistical results and ana-
lyzed interesting phenomena. This can assist developers in understanding key improvement
directions for Code LLMs;
• We maintained the scores of 126 Code LLMs on major benchmarks and conducted a detailed
analysis of their performance across different software engineering tasks. This can aid
Code LLM developers in making informed decisions regarding base models and fine-tuning
approaches.
The organization of this paper is as follows. In Section 2, we describe the methods of literature
collection and the criteria for screening; In Section 3, we summarize and categorize the Code LLMs
and propose an answer to research question one (RQ1); In Section 4, we address research question
two (RQ2); In Section 5, we compiled the performance of different LLMs across various benchmarks
and literature to address research question there (RQ3); In Section 6, we elucidate related work;
Finally, in Section 7, we summarize the entire paper.
2
METHODOLOGY
In this section, we introduce the detailed steps of conducting a literature review. We follow the
method provided by [41] for the literature review with consists of three steps: literature search,
literature screening, and data analysis. As shown in Fig. 1.
2.1
Literature Search
Based on the previous literature review [41], we selected four databases and communities: dblp,
Google Scholar, GitHub and arXiv. From these search engines, we can not only find publications
5https://github.com/KevinHeiwa/LLM_SE_Papers_List
4
Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen
Key Words Search
Literature 
Search
Literature 
Selection
Seven Exclusion 
Criteria
Data 
Analysis
masterSKevin1
Literature collection and screening process: large-scale collection of 
literature and accurate screening of relevant literature
4 Search Engines
Including 
published 
literature and 
preprint literature
Manually Context
Analysis
Using card 
classification 
methods to check 
data
Paper Reading
Literature 
classification
Discussion and 
Unified 
Conclusion
masterSKevin1
Rolling the snowball to expand is the bibliography list and 
problem distillation.
Fig. 1. Overview of methodology design.
Table 1. Number of keyword searches returned by each search engine.
SE LLM
Software
Engineer-
ing
Large
Language
Model
Software
Engineer-
ing LLM
SE
Large
Language
Model
Code LLM
Code Large
Language
Model
GitHub
399
272
4
85
711
328
dblp
22
92
1
139
15
62
Google
Scholar
128000
4480000
24100
532000
75000
5280000
arXiv
7
364
26
17
705
2484
in journals, conferences, workshops, and numerous preprint papers but also access the latest
advancements in the industry and open-source projects.
We conducted searches using the following six keywords: “SE LLM"", “Software Engineering
Large Language Model"", “Software Engineering LLM"", “SE Large Language Model"", “Code LLM"",
and “Code Large Language Model"" on the aforementioned four databases. The obtained results
are presented in Table 1. It is worth noting that there might be a significant number of duplicate
papers and irrelevant articles resulting from different keyword searches within the same engine or
the same keyword across different engines. Therefore, we need to manually screen and select these
papers, which is known as literature screening or literature selection.
2.2
Literature Selection
During this stage, we not only need to remove duplicate papers but also filter out irrelevant ones.
For example, there are papers that primarily focus on LLMs or the field of software engineering
but do not include LLM/Large Language Model keywords in their abstracts. Additionally, Google
Scholar returns a large number of unrelated results. According to the industry’s definition of
LLMs [115], we excluded works published before 2021. As for GitHub, simple keyword matching
A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends
5
may result in numerous irrelevant issues and pull requests. Therefore, we only focused on the work
done in repositories. In summary, we applied the following eight exclusion criteria to filter the
literature:
Exclusion Criteria
• Studies are not written in English.
• Master or Ph.D. theses.
• Keynote papers.
• Studies not related to LLMs.
• Studies not related to software engineering.
• Duplicate papers.
• Studies up to 2021 (not including 2021).
• Results in GitHub except repositories.
In this study, we only focus on the performance of Code LLMs and General LLMs in software
engineering tasks. Therefore, many works that do not involve the demonstration and evaluation
of LLMs’ performance in software engineering tasks are beyond the scope of our research. We
specifically focus on the following topics:
• Code LLMs.
• Application of Different LLMs in Software Engineering.
• Evaluation of LLMs on Software Engineering Tasks
To improve the accuracy of the literature screening process, we will use the card sorting method
to evaluate the collected data. Card sorting is a common technique used to assess data and derive
categories from it. There are three types of card sorting methods: open card sorting, closed card
sorting, and hybrid card sorting. Among these three types, closed card sorting involves predefined
categories [14]. In our case, we applied closed card sorting to select relevant papers since we have
only two categories: relevant and irrelevant. Each card will have a title (paper title) and a description
(paper abstract). By using this method, we can systematically evaluate and categorise the papers
based on their relevance to our research.
Three experienced researchers, including one non-coauthors, independently conducted a thor-
ough review of the search engine results from the four databases. After individually organizing the
papers, they engaged in a collaborative discussion to align their findings. Through this rigorous
process, we ultimately identified 121 relevant works that met the criteria for inclusion in our study.
Furthermore, we expanded the paper list using a snowballing approach [14]. Specifically, we
manually checked the references of the identified 121 papers and found an additional 13 papers
that met our selection criteria. Therefore, we ultimately selected 134 papers for analysis. The list of
papers can be found at https://github.com/.
2.3
Data Analysis
We used an open card sorting approach to help find the answers to these three research questions.
We carefully read the articles and actively searched for answers related to the two questions shown
in Table 2. If we couldn’t find any answers in a particular paper, it was removed from our list.
For the answers to (1), we primarily examined whether and observed whether the work released
brand new Code LLMs or mentioned other unknown Code LLMs.We organized this information
and categorized the work according to the type of organization (e.g., company, university) in which
the main developers are located, as shown in Fig. 2. We can see that there are 18 corporate-led
Code LLMs, with Microsoft publishing the most, accounting for 8. The report publishes a total of
18 Code LLMs, while research teams and communities publish the most Code LLMs in comparison,
with 21. The specifics of these tasks are presented in 3. For the answers to (2), We mainly read
6
Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen
Fig. 2. Number of Code LLMs issued by organizations.
Table 2. Data Collection for Each RQ.
RQs
Type of Data We Collected
RQ1
What LLMs are specifically designed for software engineering tasks, and what is the
relationship between them? For example, which LLMs are fine-tuned based on other
LLMs?
RQ2
Do Code LLMs really outperform general LLMs in software engineering tasks? For in-
stance, do Code LLMs exhibit better performance than general LLMs in code generation
tasks?
RQ3
Which LLMs are more proficient in different software engineering tasks? Different
LLMs may excel in different software engineering tasks.
whether the articles include in the experimental section a comparison of the performance of Code
LLMs with general LLMs on software engineering tasks. Especially those example studies, and
research articles.
For the answers to (3), we organized the performance of LLMs according to different software
engineering tasks, including code generation, code summarization, code translation, and vulnera-
bility detection. We primarily looked for experimental sections in various papers that involved the
comparison of different LLMs. Additionally, we compiled the performance of different LLMs on
benchmarks commonly used to assess software engineering tasks. In this section, we focused on
evaluating articles and papers introducing new Code LLMs.
3
LLMS FOR SOFTWARE ENGINEERING
In this section, we answer and summarize mainly for RQ1. We organize our collection of LLMs
designed specifically for software engineering tasks and show the relationships between them.
According to the different release times, we can obtain Fig. 3. We can see that with the increase
in years, more and more companies, organizations, and research teams have been involved in the
development of Code LLMs [17, 27, 40, 122]. This fully demonstrates the high level of attention
from both the industry and academia to the performance of LLMs in software engineering tasks. In
order to better illustrate the development process of different LLMs, we will elaborate on the types
of affiliations of the main developers of Code LLMs in the following text. These include compa-
nies, organizations, research teams & open-source communities, and individuals & anonymous
contributors.
A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends
7
3.1
Company-led LLMs
Microsoft: Microsoft has been at the forefront of exploring numerous LLMs in the field of software
engineering and has produced several LLMs specifically designed for software engineering tasks.
In 2020, Microsoft introduced the first LLM for programming, GPT-C [88], followed by the release
of PyMT5 [20] and CodeGPT [56]. GPT-C is a variant of GPT-2 [37], belonging to the class of
multilayer generative Transformer models. GPT-C underwent retraining on a large-scale unsuper-
vised multilingual source code dataset and achieved an average edit similarity of 86.7% on code
completion tasks in the Python programming language. On the other hand, PyMT5 is a code-based
LLM tailored for Python and natural language. Its architecture is built on the encoder-decoder
transformer framework. PyMT5 was trained on a massive parallel corpus containing 26 million
Python methods and 7.7 million method-docstring (documentation strings) pairs. Its primary focus
is on translation tasks between Python methods and method-docstrings, such as predicting Python
methods based on docstrings or generating summaries for Python code. The article also showcases
the performance of PyMT5 on the CodeSearchNet [36]. CodeGPT is another LLM developed by
Microsoft for software engineering. It shares the same model architecture and training objectives
as GPT-2. CodeGPT is pretrained on the CodeSearchNet dataset and primarily used for code com-
pletion and code generation tasks. There are two variants of CodeGPT. The first variant, CodeGPT,
involves retraining the model with randomly initialized parameters. The second variant, CodeGPT-
adapted, involves training directly on top of the GPT-2 model. The article evaluates CodeGPT’s
performance in code completion tasks on two datasets: PY150 [74] and Github Java Corpus [4]. The
results indicate that both CodeGPT and CodeGPT-adapted outperform methods such as LSTM [33]
and Transformer [95], with CodeGPT-adapted achieving the highest scores.
Phi-1 [31] is another code-based LLM proposed by Microsoft Research. Phi-1 utilizes a decoder-
only transformer architecture and has a size of 1.3B. It was trained on a high-quality dataset of
7B samples. Despite its smaller model size and dataset, Phi-1 outperforms some models trained
with larger parameters and larger-scale datasets on multiple benchmarks. This demonstrates that a
high-quality dataset can significantly enhance model performance.
Given the strong potential demonstrated by phi-1, Microsoft continues to explore the power of
small-scale Transformer-based language models with the subsequent version, phi-1.5 [52]. Phi-1.5
is a 1.3 billion parameter LLM trained primarily on a specially curated “textbook-quality"" synthetic
dataset. The paper indicates that phi-1.5 achieves performance on par with models five times its size
in natural language tasks and surpasses most non-cutting edge LLMs in more complex inference
tasks like elementary math and basic coding. Regarding the impressive performance of phi-1.5, the
article emphasizes the importance of data quality. While the model still lags behind the capabilities
of the largest LLMs, it showcases features previously only seen in larger models and demonstrates
the feasibility of achieving high-level functionality in smaller LLMs.
Microsoft also released a code-based LLM called JuPyT5 [12], which was trained on all publicly
available Jupyter Notebook GitHub repositories. Chandel et al.[12] aimed to explore a Data Science
assistant based on the Seq2Seq transformer architecture and introduced a new metric called Data
Science Problems (DSP) in this paper. This metric evaluates the model’s proficiency in using Python
for mathematics and data science by utilizing a curated set of 1,119 questions from educational
notebooks. The results of the study show that JuPyT5 was able to solve 77.5% of the DSP problems
in 100 sampling tests.
PyCodeGPT [109] is a library-oriented code generation model developed by Microsoft using
unlabeled code corpora for training. However, the main focus of the article is on introducing CERT,
a novel two-stage approach for library-oriented code generation. On the HumanEval benchmark,
PyCodeGPT (110M) achieves a competitive pass@1 rate of 8.33%.
8
Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen
WizardCoder [57] is an open-source large model co-developed by Microsoft and Hong Kong
Baptist University. It applies the Evol-Instruct method mentioned in WizardLM [102] to the field of
code and utilizes the Code Alpaca’s 20K instruction-following code dataset to obtain 78K fine-tuning
data containing code instructions of varying complexity. WizardCoder uses StarCoder [50] as the
base model and fine-tunes it on the aforementioned 78K instruction-based fine-tuning dataset. The
article also tests WizardCoder on four widely recognized code generation benchmark metrics.
Google: Google has been exploring ways to improve the safety and quality of LLMs’ output. They
propose addressing this challenge by fine-tuning models using annotated data and enabling models
to query external sources of knowledge. Based on this approach, Google introduced LaMDA [92],
a series of decoder-only Transformer-based LLMs ranging from 2B to 137B parameters. LaMDA
undergoes pretraining on 1.56 trillion words from public dialogue data and web text. It is then
fine-tuned using annotated data and equipped with the ability to invoke external retrieval tools’
APIs.
Google also explores the impact of model scale on few-shot learning [18]. They introduce a new
machine learning system called Pathways, which allows efficient training of large models across
multiple TPU Pods. Utilizing this system, they trained a dense activation Transformer language
model with 540B parameters called Pathways Language Model (PaLM). PaLM uses a standard
decoder-only Transformer model architecture with some modifications, such as using the SwiGLU
activation function in MLPs, employing parallel forms in each Transformer block, and utilizing
multi-query attention.
For code-related tasks, researchers collected a Python code dataset called ExtraPythonData. They
fine-tuned PaLM on this dataset for code tasks, resulting in the model PaLM-Coder. PaLM-Coder
achieves excellent performance on multiple code generation benchmarks and the DeepFix code
repair test [18].
However, current LLMs do not perform well when it comes to evaluating more complex and
unknown problems that require surpassing the mere translation of instructions into code [53]. For
instance, understanding algorithmic and complex natural language competition-style programming
problems. To address this, Google DeepMind proposed AlphaCode [53], which aims to tackle these
programming problems that require deep reasoning to find novel solutions.
AlphaCode utilizes an asymmetric Encoder-Decoder Transformer architecture, consisting of a
shallow encoder and a deep decoder. This modification significantly improves training efficiency
without compromising inference speed. When evaluated on Codeforces, AlphaCode’s performance
is considered average, roughly on par with the median competitor. Interestingly, the authors
note that scaling upsampling and filtering samples to cluster them into a small set, as well as
introducing new efficient transformers that support large-scale sampling, can effectively enhance
the performance of LLMs.
Meta: Facebook AI Research has introduced InCoder [26], a unified generative model that can
perform program synthesis through left-to-right generation and code editing through masking
and infilling. InCoder is the first large-scale generative code model capable of filling arbitrary code
regions. It is trained on a large corpus of code files with permissive licenses, where random code
regions are masked and moved to the end of each file, allowing bidirectional context for code
completion.
InCoder adopts the MoE (Mixture of Experts) model architecture [5]. The paper highlights that
training code generation models with causal masking objectives can achieve strong zero-shot
performance on challenging and practically meaningful code completion and editing tasks. It also
lays the groundwork for future work on supervised filling and editing via model fine-tuning and
iterative decoding.
A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends
9
Meta has also introduced Code Llama [78], a large-scale code language model series based on
Llama 2. Code Llama offers multiple versions, including a base model (Code Llama), a Python
specialization model (Code Llama - Python), and an instruction-following model (Code Llama -
Instruct), each available with 7B, 13B, and 34B parameters. All models are trained on 16k token
sequences and demonstrate improvements on inputs with up to 100k tokens. The paper provides
extensive experimental evidence to demonstrate the superiority of Code Llama. Meta has also made
Code Llama available for research and commercial use under licensing terms.
Huawei: PanGu-Coder [19] is a pre-trained decoder language model proposed by Huawei Noah’s
Ark Lab. It adopts the PANGU-𝛼architecture [111] and is specifically designed for text-to-code
generation. Pangu-Coder utilizes a two-stage training strategy: in the first stage, it undergoes
causal language modelling (CLM) pre-training on raw programming language data, while in the
second stage, a combination of CLM and masked language modelling (MLM) training objectives
is used, focusing on downstream tasks of text-to-code generation. It is trained on loosely filtered
natural language program definitions and code-function pairs, using a dataset of 147GB of code
data collected from GitHub.
The results of the study show that the two-stage training approach helps achieve comparable or
even better performance than models of similar scale, with smaller context windows and fewer
training data requirements. Additionally, the experiments with PANGU-CODER-FT demonstrate
that by carefully selecting and fine-tuning with data closely related to the target task, the base
model’s scores can be improved more quickly, as the model is sensitive to data distribution mismatch
and shifts during fine-tuning. Building upon PanGu-Coder, Huawei Cloud has launched the Huawei
Cloud Intelligent Programming Assistant CodeArts Snap to enhance developers’ programming
efficiency.
PanGu-Coder2 [81] is an iterative upgrade of PanGu-Coder. PanGu-Coder2 achieved a pass@1
rate of 62.20% in the OpenAI HumanEval benchmark test. Furthermore, through extensive eval-
uations on CoderEval and LeetCode benchmarks, the paper demonstrates that PanGu-Coder2
consistently outperforms all previous Code LLMs. Interestingly, PanGu-Coder2 is developed based
on a novel framework called RRTF (Rank Responses to align Test & Teacher Feedback). The paper
claims that this framework can efficiently enhance large-scale pre-trained language models for
code generation.
Baidu: Currently, LLMs in the field of software engineering tend to be English-centric, which
leads to limited comprehension of other natural languages. Therefore, Baidu has proposed a large
code model called ERNIE-Code N90 that supports multiple natural languages and programming
languages. ERNIE-Code is based on the T5 architecture and utilizes two approaches for universal
cross-lingual pre-training. Firstly, it undergoes unsupervised learning using a monolingual pro-
gramming language (PL) and natural language (NL) data. Then, it undergoes supervised learning
on cross-lingual NL-PL or NL-NL pairs to enable the model to learn cross-lingual/modal align-
ment and zero-shot capabilities. The paper acknowledges that while ERNIE-Code demonstrates
strong performance on various tasks involving computer programs and natural language, the lack
of corresponding benchmarks prevents the authors from evaluating ERNIE-Code’s performance
systematically across a wide range of multi-language NLs.
Replit: Replit-Code [76] is an open-source code model developed by Replit, primarily targeting
code completion tasks. Replit-Code has a scale of 2.7B and is trained on a subset of the Stack Dedup
v1.2 dataset [42]. It offers code completion capabilities for 20 programming languages. However,
the current lack of technical and testing reports for Replit-Code makes it difficult for us to know
the specific performance of Replit-Code on software engineering tasks.
BAAI: AquilaCode-multi [119] is a bilingual code model developed by BAAI (Beijing Academy
of Artificial Intelligence). It is trained from scratch on a high-quality corpus of both Chinese and
10
Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen
2021
2022
2023
2020
MultiPL-T
Phi-1
GPT-C
PyMT5
CodeGPT
PLBART
CodeParrot
CodeT5
Codex
GPT-Neo
JuPyT5
PyCodeGPT
LAMDA PaLM-Coder AlphaCode
PanGu-Coder
GPT-J
FIM
CodeRL
CodeT5Mix
GPT-CC
BLOOM
ERNIE-Code
OpenCoderPlus
PolyCoder
MultiCoder
Phi-1.5 WizardCoder
InCoder Code Llama
PanGu-Coder2
Code Alpca
CodeT5+ CodeGen CodeGen2
SantaCoder StarCoder
OctoCoder/
OctoGeeX
CodeGeeX
CodeGeeX2
VeriGen
CodeEditor
CodeFuse
CodeFuse-Codellama
Phind-Codellama
CodeGenAPI
Replit-Code
AquilaCode
UniXcoder
366M
374M
124M
140M-406M
110M-1.5B
220M-770M 12M-175B125M-1.3B
6B
50M
110M
Unknown
8B-540B
284M-41.1B 560M-176B 50M-6.9B
125M-1.3B
560M
15B
160M-2.7B
8B-540B
770M
2B-137B
1.3B-6.7B
220M-16B 350M-16.1B
1.1B
7B-13B
1B-16B
13B
2.7B
7B
15.5B
15B
1.3B
16B
Unknown
350M-6B
7B-34B
Unknown
15B
34B
16B/6B
6B
13B
34B
1.3B
15B
Fig. 3. Evolution of Code LLMs
English languages, with approximately 40% of the data consisting of Chinese text. This approach
ensures that the model accumulates native Chinese knowledge during the pre-training phase,
rather than relying on translation. The training data for code generation is derived from high-
quality filtered code samples that adhere to open-source licenses. Currently, there is also a lack
of comprehensive analytical data on the capabilities of AquilaCode-multi. Further exploration is
needed to uncover the specific performance of AquilaCode-multi.
A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends
11
3.2
University-led LLMs
PLBART [1] N65 is a code-based large language model jointly released by the University of Cali-
fornia and Columbia University. It is built upon the BART architecture and is pre-trained using
denoising autoencoding on a large corpus of Java and Python functions along with their associated
natural language texts. Experimental results demonstrate that PLBART outperforms methods
such as Seq2Seq [58], Transformer [95], RoBERTa [54], and CodeBERT [75] on tasks such as code
summarization, code generation, and code translation across seven programming languages.
Carnegie Mellon University conducted a systematic evaluation of code-based large models such
as Codex, GPT-J, GPT-Neo, GPT-NeoX-20B, and CodeParrot [103]. The article also highlights the
lack of a large-scale open-source model exclusively trained on a multilingual code corpus among
these models. As a result, Carnegie Mellon University introduced PolyCoder, an LLM trained on
a single machine using 249GB of code from 12 programming languages. PolyCoder is available
in three versions: 160M, 400M, and 2.7B. The experimental results in the article demonstrate that
PolyCoder outperforms all the models at that time, including Codex, in terms of performance on
the C programming language. Moreover, it is noteworthy that PolyCoder is open-source, which
adds to its value.
CodeGeeX [116] is an open-source multilingual LLM led by Tsinghua University. CodeGeeX
adopts a decoder-only autoregressive (programming) language modelling approach, with a core
architecture consisting of a 39-layer Transformer decoder. The training corpus includes open-source
code datasets such as the Pile, and CodeParrot, as well as publicly available code scraped directly
from GitHub repositories, totaling 850 billion tokens from 23 different programming languages.
The experimental results in the article demonstrate that CodeGeeX consistently outperforms other
open-source LLMs of similar scale in code generation and translation tasks. Furthermore, the
extension capabilities built by CodeGeeX bring significant benefits in improving coding efficiency.
Tsinghua University has released the second generation of CodeGeeX, called CodeGeeX2 [93].
Unlike the first generation, CodeGeeX2 is built on the ChatGLM2 architecture and incorporates code
data for pretraining. Leveraging the improved performance of ChatGLM2, CodeGeeX2 demonstrates
performance improvements across multiple metrics. With just 6 billion parameters, CodeGeeX2
achieves nearly a 10% improvement over StarCoder-15B, which has over 15 billion parameters.
Due to inheriting the characteristics of the ChatGLM2-6B model, CodeGeeX2-6B provides better
support for both Chinese and English inputs. It also supports a maximum sequence length of
8192 and significantly improves inference speed compared to the first-generation CodeGeeX-13B
model. After quantization, CodeGeeX2-6B only requires 6GB of GPU memory to run and supports
lightweight local deployment.
MultiCoder [29] is a collaborative development by Harbin Institute of Technology and Huawei
Noah’s Ark Lab. It addresses the challenge of code completion on low-resource programming
languages (PL) that are widely used by developers but present difficulties for data-driven approaches.
To enhance code completion capabilities for low-resource PLs, Gong et al. [29] proposes MultiCoder,
which leverages MultiPL pre-training and a MultiPL Mixture-of-Experts (MoE) layer. The article
also introduces a novel PL-level MoE routing strategy (PL-MoE) to improve code completion across
all PLs. Performance analysis of MultiCoder is conducted on CodeXGLUE and MultiCC datasets.
The results indicate that MultiCoder significantly outperforms the MonoPL baseline in low-resource
programming languages. Importantly, the PL-MoE module further enhances performance across
six programming languages.
Cassano et al. [10] also focus on low-resource languages and propose an effective method called
MultiPL-T to improve the performance of low-resource language Code LLM using semi-synthetic
data. By utilizing the data generated by MultiPL-T, the article presents a fine-tuned version of
12
Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen
StarCoderBase and achieves better performance on benchmark problems for Racket, OCaml, and
Lua languages.
Thakur et al. [91] explored the ability of LLMs to generate high-quality Verilog code and fine-
tuned a new LLM, called VeriGen, based on the CodeGen-16B architecture using Verilog datasets
collected from GitHub and Verilog textbooks. The experimental results of the paper demonstrate that
VeriGen outperforms the state-of-the-art commercial GPT-3.5-turbo model in terms of generating
Verilog code, showing an overall improvement of 1.1%. Although the improvement is not significant,
it showcases the potential of small-scale LLMs in hardware design automation.
Due to the frequent usage of private library APIs by programmers when writing proprietary
code, LLMs lack the capability in this aspect. This is because, during pre-training, LLMs have
limited exposure to these private libraries. To address this issue, Zan et al. [108] propose a novel
framework to simulate the process of programmers writing private code. The framework consists
of two modules: APIFinder, which retrieves potentially useful APIs from API documentation,
and APICoder, which utilizes these retrieved APIs to generate private code. Furthermore, Zan et
al. [108] introduce an enhanced version of the APICoder called CodeGenAPI, developed based on
GodeGen, an LLM. The paper also creates four benchmark metrics for private libraries, including
TorchDataEval, TorchDataComplexEval, MonkeyEval, and BeatNumEval. Test cases are written for
each benchmark test to support the comprehensive evaluation of CodeGenAPI.
Zhang et al. [112] focuses on the task of code translation, which involves converting code changes
from one programming language to another. A language model called Codeeditor is designed and
implemented for this purpose. Codeeditor explicitly models code changes as an editing sequence
and learns cross-language associations for the changes. To evaluate Codeeditor, the paper collects
6,613 consistent code changes from 8 pairs of open-source software projects that have similar
functionality in two programming languages: Java and C#. The results of the study show that
Codeeditor outperforms both Codex and CodeT5 by a significant margin across various commonly
used metrics. The performance of Codeeditor on a single software engineering task provides new
insights for the development of Code LLMs and complements the capabilities of existing Code
LLMs. The combination of these models can ensure higher performance overall.
3.3
Research teams & Open source communities-led LLMs
EleutherAI: EleutherAI has conducted equivalent reproductions of the non-open-source GPT-3
N71 model and released two open-source equivalents: GPT-Neo (125M 2.7B) N59 and GPT-J (6B)
N60, corresponding to GPT-3 Ada (2.7B) and GPT-3 Babbage (6.7B), respectively. To reproduce
GPT-3, EleutherAI first proposed a 825GB English text corpus called “the Pile"" N72, which includes
95GB of code data sourced from GitHub. GPT-Neo is a GPT2-like causal language model trained on
the Pile dataset and utilizes the mesh-tensorflow library for parallelism. GPT-J is an autoregressive
text generation model with 6 billion parameters trained on the Pile using Mesh Transformer JAX.
Although GPT-Neo and GPT-J outperform GPT-3 models with equivalent parameters on the tested
NLP reasoning benchmarks, they still have a significant gap when compared to the largest GPT-3
model, GPT-3 Davinci (175B). Additionally, due to the inclusion of code data in the training set, both
GPT-Neo and GPT-J show some performance in code-related tasks. GPT-NeoX [8], an open-source
LLM by EleutherAI, is a 20B model and currently the largest publicly available model in terms of
weight. It is trained on the Pile dataset with minor modifications to the GPT-J architecture. However,
the paper does not provide experimental evaluations of the programming capabilities of GPT-NeoX.
Instead, it primarily showcases the impressive performance of GPT-NeoX on mathematical tasks.
OpenAI: In 2021 and 2022, OpenAI introduced two LLMs, Codex [17] and FIM [7], designed to
assist with programming tasks. Codex [17] is a GPT-3 model fine-tuned using publicly available code
from GitHub, with a maximum parameter count of 12B. The research paper also introduces a new
A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends
13
evaluation benchmark called HumanEval, which involves synthesizing programs from docstrings.
HumanEval consists of 164 handwritten programming questions and has become an important
tool for evaluating the performance of large models on software engineering tasks. Experimental
results show that in a single sample from HumanEval, Codex solved 28.8% of the problems, while
GPT-3 solved 0% and GPT-J solved 11.4%.
Building upon Codex, GitHub collaborated with OpenAI to develop a code generation tool
called GitHub Copilot. GitHub Copilot leverages context and prompts to automatically generate
code snippets, comments, and more. It provides programming suggestions and code completion
functionality to assist developers in their coding tasks.
Indeed, all model classes, including Codex and GPT-3, have limitations when it comes to infilling,
which involves generating text at a specific location within a prompt while conditioning on both a
prefix and a suffix [17]. Left-to-right models can only condition on the prefix and cannot handle this
type of task effectively. To address this limitation, OpenAI proposed the addition of fill-in-the-middle
(FIM) capability within the paradigm of LLMs [7].
The FIM framework allows for simple modifications to the training data without altering the
model structure, enabling the models to handle fill-in-the-middle tasks more effectively. The research
paper showcases eight causal transformer decoder models trained on the FIM framework, which
have architectures similar to Codex and GPT-3. To prevent contamination from the HumanEval
dataset, which was used for evaluation, FIM utilized the same 159GB code dataset as Codex for
pre-training. The models were initialized with random parameters similar to GPT-3. Due to the
efficiency of the FIM training framework, subsequent models such as Incoder and SantaCoder also
adopted this approach.
Salesforce Research: Salesforce Research has released three LLM models in the field of software
engineering: CodeT5 [100], CodeRL [45], and CodeGen [67].
CodeT5 [100] is a pre-trained encoder-decoder model based on the T5 architecture. The developers
of CodeT5 recognized that previous code models treated source code as token sequences similar
to natural language, overlooking the rich structural information present in code. To address this,
the authors proposed a novel identifier-aware mechanism that enables the model to distinguish
which tokens are identifiers and recover them when they are masked. CodeT5 was trained on the
CodeSearchNet dataset and further fine-tuned on the CodeXGLUE [56] benchmark. Experimental
results demonstrate that CodeT5 outperforms previous works on most tasks within the CodeXGLUE
benchmark, showcasing its improved performance in the software engineering domain.
CodeRL [45] introduces a training framework that leverages reinforcement learning to improve
the performance of pre-trained language models in program synthesis tasks. Based on this frame-
work, the authors used CodeT5 [100] as the base model and trained the CodeRL model on the GCPY
dataset, which focuses on Python code. During training, the code generation language model is
treated as an actor-network, and a critic network is introduced to predict the functional correct-
ness of the generated programs and provide dense feedback signals to the actor. Experimental
results presented in the paper demonstrate that CodeRL surpasses CodeT5 not only on the APPS
benchmark test and MBPP benchmark test but also exhibits strong zero-shot transfer learning
capabilities. This indicates that CodeRL can effectively generalize its learning to new tasks and
datasets without explicit training on them.
By increasing the model size and data scale, the language modelling capability can be improved,
enabling LLMs to understand long contexts and generate coherent responses. Therefore, utilizing
this capability can lead to a better understanding of user intent and achieve improved program
synthesis results [67]. To validate this approach’s effectiveness, the authors propose a multi-step
program synthesis method and introduce a new LLM called CodeGen [67] for verification. CodeGen
14
Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen
is also used for program synthesis tasks, adopting the form of an autoregressive transformer.
CodeGen is trained on three datasets: the Pile, BigQuery, and BigPython.
The authors initially trained CodeGen on the Pile dataset. The Pile dataset is a large-scale
English text corpus open-sourced by EleutherAI, which includes a substantial amount of GitHub
code data. This training results in the first version of CodeGen-NL. Subsequently, developers
utilize the parameters of CodeGen-NL as initial parameters for training on BigQuery. BigQuery
is a publicly available multi-programming language code dataset provided by Google, and six
programming languages are selected for this purpose, leading to the second version, CodeGen-
MULTI. Finally, developers employ the parameters of CodeGen-MULTI as initial parameters for
training on BigPython. BigPython is a single-language code dataset focused on Python. Upon
completion of training, CodeGen-MONO is obtained.
The experiments in the article demonstrate that multi-step program synthesis can enhance the
model’s program synthesis capability. Furthermore, the multi-step program synthesis capability
exhibits a linear relationship with the model size and data scale.
Salesforce Research also explored improving the efficiency of LLMs for program synthesis training
by unifying four key components: model architecture, learning methods, infill sampling, and data
distribution. In this exploration, researchers trained four models with parameter sizes of 1B, 3.7B,
7B, and 16B, referred to as CodeGen2 [66]. Regarding model architecture, CodeGen2 attempted
to unify the encoder and decoder models into a single prefix-based language modelling (Prefix-
LM) [73]. This approach aims to unify bidirectional encoder representations and unidirectional
decoder representations.
In terms of learning methods, CodeGen2 employed an algorithm that combines a causal language
modelling objective with span corruption. And it also explored the “free lunch"" hypothesis. In the
aspect of data distribution, CodeGen2 investigated the impact of mixing programming language
and natural language distributions on model performance.
The experimental results showed the following findings:
• The Prefix-LM architecture did not demonstrate measurable improvements in this task.
• Training a model with infill sampling did not provide a free lunch.
• A simple mixture of causal language modeling and span corruption, limited to within-file
spans, was sufficient.
• A mixture distribution of programming and natural languages showed promising results.
Salesforce Research also introduced an upgraded version of CodeT5 called CodeT5+ [98]. CodeT5+
allows flexible combinations of its component modules to adapt to various downstream code tasks.
The authors of the article proposed the use of mixed pretraining objectives to alleviate the dis-
crepancy between pretraining and fine-tuning stages. They covered multiple pretraining tasks,
including span denoising, contrastive learning, text-code matching, and causal language modeling,
across both single-modal and multimodal code corpora. Furthermore, the authors suggested ini-
tializing CodeT5+ with frozen pre trained LLMs, without the need for training from scratch. This
approach effectively scales up these models and explores instruction conditioning to align with
natural language instructions.
CodeT5Mix [99] is based on CodeT5. Building upon CodeT5, CodeT5Mix introduces an encoder
component, RoBERTa, and two decoder components, GPT-2 and CodeGPT. By combining these
components (or using them individually), CodeT5Mix is able to handle various code-related tasks.
The training incorporates diverse pre-training tasks and employs an efficient weight-sharing
strategy. CodeT5Mix comes in two variants: base (220M) and large (770M). The results of the
study demonstrate that it can fully support a semi-parametric retrieval-augmented code generation
approach, as the model effectively performs in both code retrieval and generation tasks.
A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends
15
CodedotAI: CodedotAI attempted to replicate GitHub Copilot and proposed the GPT-CC (GPT-
Code-Clippy) [65] model. GPT-CC utilized the GPT-Neo model as its base model and was trained
on Code Clippy Data, with fine-tuning performed on APPS and CodeSearchNet Challenge Data.
Code Clippy Data is a 132GB dataset created by developers by deduplicating and merging code
data collected from GitHub with code data from the Pile. However, GPT-CC did not provide a
comparison with other LLMs in terms of performance on software engineering tasks, nor did it
further analyze the effectiveness and superiority of Code Clippy Data.
Huaggingface: Huaggingface has made significant contributions to the open-source landscape
of LLMs in the field of software engineering. They have provided a training tutorial similar to Codex,
allowing users to build a large model called CodeParrot) [35] from scratch. CodeParrot is based
on the GPT-2 architecture, and users can train both 110M and 1.5B versions of CodeParrot using
just 150 lines of code. Huaggingface has also introduced the BigCode project, which is a scientific
collaborative effort dedicated to developing large code models. Within the BigCode project, they
proposed SantaCoder [3], an LLM based on the FIM (Filling in the Missing) model. SantaCoder’s
Tokenizer draws inspiration from InCoder and uses the Hugging Face Tokenizer [63]. It employs
the Byte-Pair Encoding (BPE) algorithm to train the original bytes. The Stack v1.1 is a dataset
released by Huaggingface in the BigCode community, containing 6.4TB of source code data from
384 programming languages. This dataset has been filtered to remove sensitive information such
as usernames while generating it.
In addition, the BigCode community has also released an open-sourced StarCoder [50], claiming
it to be one of the most advanced LLMs for assisted programming currently available. StarCoder
is divided into two versions: StarCoder and StarCoderBase. The architecture of StarCoderBase
is similar to SantaCoder, with a parameter size of 15.5B and the utilization of techniques such
as FIM (Filling in the Missing) and MQA (Multi-Question Answering). StarCoderBase is trained
on 1 trillion tokens from the Stack dataset and then fine-tuned on 35B Python code data to
obtain StarCoder. StarCoderBase outperforms other existing open-source code LLMs in popular
programming benchmarks. Additionally, it has a context length of over 8,000 tokens, allowing
StarCoder to handle larger inputs than any other open-source LLM.
In Muennighoff et al. [64], the BigCode community introduced COMMITPACK, a 4TB Git commit
dataset covering 350 programming languages. This dataset includes paired information between
code changes and human instructions, which aids in better instruction-based fine-tuning of LLMs.
The article also introduces HUMANEVALPACK, which extends the HumanEval benchmark to three
coding tasks (code fixing, code interpretation, and code synthesis) across six languages (Python,
JavaScript, Java, Go, C++, Rust). Additionally, the article presents two Code LLM models, OctoCoder
and OctoGeeX. Experimental results demonstrate that these two LLMs achieve optimal performance
on HUMANEVALPACK and outperform GPT-4 in terms of performance.
BigScience Workshop: The BigScience Workshop introduced an open-source 176B language
model called BLOOM [79]. It is a decoder-only Transformer language model trained on the ROOTS
corpus. ROOTS is a composite collection consisting of 498 Hugging Face datasets [46] with a
total text size of 1.61TB, covering 46 natural languages and 13 programming languages. BLOOM
achieves competitive performance in various benchmark metrics and shows stronger results after
multi-task prompt fine-tuning. Unlike considering a mixture of expert models (MoE) or the state-
space model [30], BLOOM employs causal decoder-only models. However, the article does not
provide a comprehensive evaluation of BLOOM on software engineering tasks. Only a few test
results indicate that BLOOM performs weaker than Codex in code generation. This suggests that
multitasking fine-tuned BLOOMZ models do not significantly improve over BLOOM models.
OpenChat: OpenCoderPlus [96]is a code-oriented large language model in the OpenChat series,
fine-tuned on a dataset of code. The base model for OpenCoderPlus is StarCoderPlus. According to
16
Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen
the data provided by the authors, OpenCoderPlus achieves 102.5% of the ChatGPT score on the
Vicuna GPT-4 evaluation and has a win rate of 78.7% on AlpacaEval.
CodeFuse: CodeFuse-MFTCoder [62]is an open-source project by CodeFuse that focuses on
multi-task code large language models (Code-LLMs). It includes models, datasets, training code
repositories, and inference guidelines. MFTCoder supports most mainstream open-source large mod-
els, particularly those related to Code-LLMs, such as Code-LaMA and Starcoder. Currently, CodeFuse
has released two high-quality code-related instruction fine-tuning datasets: Evol-instruction-66k
and CodeExercise-Python-27k. They have also introduced two models: CodeFuse-13B and CodeFuse-
CodeLlama-34B. According to the results presented in MFTCoder [62], CodeFuse-CodeLlama-34B
outperforms GPT-4 in terms of performance on HumanEval.
Phind: Indeed, there are several Code LLMs that have been fine-tuned based on Code Llama,
including Phind-CodeLlama [72]. Phind-CodeLlama offers three versions that are fine-tuned from
Code Llama: Phind-CodeLlama-34B-v1, Phind-CodeLlama-34B-v2, and CodeLlama-34B-Python.
Among them, CodeLlama-34B and CodeLlama-34B-Python achieved 67.6% and 69.5% pass@1 on
the HumanEval benchmark using an internal Phind dataset. Additionally, Phind-CodeLlama-34B-v2
achieved an even higher score of 73.8% pass@1 on the HumanEval benchmark. These scores surpass
the performance of GPT-4, which achieved 67% pass@1 on the same benchmark.
3.4
Individuals & Anonymously-led LLMs
CodeAlpaca [13] is an instruction-guided LLaMA [94] model trained based on code generation
instructions. LLaMA is a set of efficient open-source large language models trained on publicly
available datasets using the standard transformer architecture. Alpacaon the other hand, combines
the Self-Instruct [97] method with LLaMA to create a dataset for instruction-based fine-tuning.
LLaMA is then fine-tuned on this instruction-based dataset to create an LLM. Furthermore, CodeAl-
paca focuses on code generation, editing, optimization tasks, and similar code-related tasks by
placing emphasis on instruction-based fine-tuning using the Alpaca model. The LLaMA model is
fine-tuned on instruction-based data in these code domains, resulting in a large model capable of
following code instructions.
We have also mapped out the relationships between these Code LLMs as shown in Fig. 4. The
arrows indicate that the model being pointed to is derived or developed from the model pointing to
it. This development can include, but is not limited to, improvements, fine-tuning, or borrowing of
techniques.
4
DO CODE LLMS REALLY PERFORM BETTER THAN GENERAL LLMS IN SOFTWARE
ENGINEERING TASKS?
In this section, we primarily focus on addressing and summarizing RQ2. First, we will filter out the
papers from our collection that evaluate Code LLMs, introduce new evaluation benchmarks, or
propose new Code LLMs. Then, we will thoroughly examine all the literature and identify papers
that compare Code LLMs with general LLMs in their evaluation sections. We will organize this
information and focus on presenting the evaluation results.
Indeed, it is important to consider the parameter scale when comparing different LLMs, as
variations in parameter size can lead to significant performance differences [110]. When comparing
LLMs, it is crucial to take into account their specific parameter sizes.
Jiang et al. [38] compares the performance of code-davinci-002-175B and text-davinci-002 in three
metrics: Pass@1 of HumanEval, CodeBLEU, and AvgPassRatio. Since text-davinci-002is based on
code-davinci-002 (175B), there is no significant performance difference between the two. However,
compared to text-davinci-003, the performance difference becomes more apparent. After RLHF,
text-davinci-003 outperforms code-davinci-002 in all aspects. From the results of N2, it may be
A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends
17
Phi-1.5
GPT-2
GPT-3
T5
CodeGen
VeriGen
CodeT5
Llama 2
CodeGeeX
CodeGeeX2
SantaCoder
StarCoder
OpenCod
erPlus
MultiCoder
WizardCoder
Phi-1
Llama
Alpaca
Code Alpaca
Code 
Llama
CodeFuse-
Codellama
Phind-
Codellama
GPT-C
GPT-Neo
GPT-
NeoX
GPT-CC
PyCodeGPT
Code
GPT
CodeP
arrot
PolyCoder
Codex
PyMT5
JuPyT5
ERNIE-
Code
CodeRL
CodeT5Mix
CodeT5+
PaLM-Coder
PanGu-
Coder
PanGu-Coder2
CodeGenAPI
Replit-Code
AquilaCode
CodeGen2
UniXco
der
Fig. 4. The relationships between Code LLMs
difficult to draw useful conclusions. We can conclude that the current code generation capability of
text-davinci-003 (175B) is stronger than code-davinci-002.
Maddigan et al. [60] evaluate the performance of ChatGPT, Codex, and GPT-3 in the task of
natural language generation data visualization. The article evaluates them through five case studies,
and in all five case studies, ChatGPT performs better than Codex and GPT-3.
Shirafuji et al. [82] evaluate the robustness of source code, Codex, CodeGen, InstructGPT, and
ChatGPT, popular LLMs, in solving programming problems by changing prompt formats and
modifying problem descriptions. The results of the article show that the new models combined
with RLHF techniques have stronger robustness towards the format of problem descriptions. In
other words, ChatGPT’s ability exceeds the other three models. Li et al. [48] mainly compare Codex
and ChatGPT in three code generation benchmark metrics, SCoT prompts, and Pass@k of the
benchmarks. The results also show that ChatGPT is more powerful than Codex in code generation
tasks.
Li et al. [47] introduce BIRD, a benchmark test for English large-scale cross-domain text-to-SQL.
T5, ChatGPT, and Codex are tested on this benchmark. The experimental results show that ChatGPT
and Codex perform better than T5 in both The Valid Efficiency Score and The Execution Accuracy.
However, there is not much difference in performance between ChatGPT and Codex, and relatively
speaking, ChatGPT is slightly better. However, they still fall far short of human performance.
Yang et al. [105] present the performance of CodeParrot-1.5B, CodeParrot-small-110M, PolyCoder-
160M, PolyCoder-400M, GPT-Neo-125M, and GPT-Neo-1.3B on OpenAI’s HumanEval benchmark.
We can see that different parameter sizes of the same model do have an impact on performance but
to varying degrees. Among the six models, GPT-Neo-1.3B performs the best, even surpassing the
larger parameter size of CodeParrot-1.5B. However, GPT-Neo-125M performs poorly, even worse
than the smaller parameter size of CodeParrot-small-110M.
Thakur et al. [91] include an experimental section evaluating the ability of LLMs to generate
high-quality Verilog code. The experimental results show that the fine-tuned CodeGen-16B model
outperforms the GPT-3.5-Turbo model, with an overall improvement of 1.1%. However, the article
also mentions that GPT-4 performs exceptionally well on these tasks, especially in advanced
18
Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen
problem-solving. Due to limited API access to GPT-4, the article does not provide a detailed
comparative evaluation of GPT-4.
Siddiq et al. [83] includes an experiment that quantifies the percentage of compilable code
snippets in Java and Python code generation tasks for 11 LLMs. It is observed that CodeGen-2B-
multi performs better for Java code, while GPT-3.5 performs relatively better for Python code
generation. The article also shows the performance of the 11 LLMs on NDCG@10. Interestingly,
GPT-3.5 achieves a higher score in the Java task, while CodeGen-2B-mono obtains a higher score
in the Python code generation task. From this article, we can not determine which model performs
better between CodeGen-2B and GPT-3.5.
Sun et al. [86] presents a performance comparison of several LLMs on the Spider Dev Split test
suite, where GPT-4 performs relatively the best.
Sun et al. [87] tests the performance of ChatGPT against NCS, CodeBERT, and CodeT5 on the
CSN-Python dataset for code summarization. The results show that ChatGPT performs relatively
poorly, while CodeT5 achieves the best performance.
Wang et al. [98] introduce CodeT5+ and compare it to current major LLMs. Compared to
current major Code LLMs, CodeT5+ demonstrates better performance in pass@k(%) on HumanEval.
However, there is still a performance gap between CodeT5+ and models like GPT-4, GPT-3.5, and
code-davinci-002.
Li et al. [50] compare StarCoder with most mainstream Code LLMs. The results of the article
show that StarCoder outperforms general LLMs, PaLM, and various versions of LaMDA in Python
program metrics, demonstrating better performance.
Li et al. [49] compare the performance of Code LLMs (Codex) and NL LLMs (GPT-3) on the task
of information extraction (IE). The article primarily metrics named entity recognition (NER) and
relation extraction (RE). The experimental results show that Code LLMs outperform GPT-3 in both
tasks.
Zhang et al. [113] focuses on the performance of LLMs when executing API operations. The
experiments in the article are conducted on three Public library benchmarks and two Private
library benchmarks. The results show that CodeGen-2B performs worse than GPT-3.5 in all five
metrics. The article also fine-tunes the ToolCoder model based on CodeGen-2B to better handle
API operations during code generation. Although ToolCoder achieves results close to GPT-3.5 in
some metrics, overall performance still falls short of GPT-3.5.
Siddiq et al. [84] validate the performance of LLMs in generating unit test cases. The article
conducts experiments with different prompts and compares the results based on compilation rate,
test correctness, coverage, and test smells. The experimental results show that CodeGen performs
the worst, while Codex performs the best among the LLMs. This could be attributed to CodeGen
being a smaller model with only 350 million parameters, much smaller in terms of training data
and parameter size compared to Codex and ChatGPT-3.5, which have 12 billion and 175 billion
parameters respectively. ChatGPT-3.5’s performance falls between the two, but in most cases, it is
noticeably inferior to Codex.
Zhuo et al. [124] introduce a novel testing framework for code generation tasks and evaluate
multiple models on two different aspects (human preferences and execution success) and four
programming languages. The results show that the method based on GPT-3.5-turbo performs better.
However, there are two caveats to consider regarding the results of this article: (1) The article
primarily focuses on example-level Kendall-Tau and Spearman (rs) correlation tests with HumanEval
based on executable functional correctness, which may not directly indicate the performance of
different language models. (2) The other language models compared to GPT-3.5-turbo in the article,
such as CodeBERTScore, are far behind in terms of parameter size and cannot even be considered
A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends
19
as LLMs. Therefore, while the work in this article is interesting, we cannot draw useful conclusions
from it.
Zheng et al. [116] introduce CodeGeeX-13B and extensively evaluate its performance on code
generation tasks in different programming languages using HumanEval-X. The experimental results
of the article show that CodeGeeX-13B and CodeGen-Multi-16B outperform general LLMs, GPT-
J-6B, and GPT-NeoX-20B in almost all metrics. The article also provides a comparison between
PaLM-540B and PaLMCoder-540B, showing that PaLMCoder-540B performs better than PaLM-540B
to some extent on the MBPP dataset.
Rozière et al. [78] introduce Code Llama and compare it in detail with SOTA LLMs on the
benchmarks of HumanEval, MBPP, and APPS. The article’s results demonstrate that Code Llama
achieves the best results in almost every evaluation metric. By observing the experimental settings
and results, we can draw the following conclusions: (1) The evaluation of GPT-3.5 and GPT-4 in the
article is not comprehensive, but based on the evaluations involving GPT-3.5 and GPT-4, they still
remain highly competitive and outperform Code LLMs like StarCoder. (2) When comparing Llama
2 with Code Llama and Code Llama with Code Llama Python, Code Llama Python performs better.
This also indicates that specialized instruction fine-tuning improves code generation capabilities
for the same model. (3) Model size is crucial, as larger parameter models like Code Llama exhibit
stronger performance.
Fu et al. [27] introduce CodeApex, a bilingual benchmark dataset focused on testing LLMs’
programming comprehension and code generation abilities. CodeApex consists of three types of
multiple-choice questions: conceptual understanding, commonsense reasoning, and multi-hop
reasoning. The results of the article show that GPT-3.5-turbo performs better in most of the metrics.
Zan et al. [110] categorize LLMs based on model size and present their performance accordingly.
The data results indicate that Code LLMs have better performance when the parameter sizes are
similar. At the same time, the impact of parameter size on the model’s capabilities may vary among
different LLMs. It is observed that the three versions of CodeGen-Mono with different parameter
sizes (2.7B, 6.1B, and 16.1B) show minimal differences on the MBPP and HumanEval benchmarks.
On the other hand, Codex exhibits relatively significant differences between its two versions: 2.5B
and 12B. The marginal effect of model parameter size on performance and the sensitivity of different
models to parameter size are important factors worth discussing.
Lu et al. [56] introduce CodeGPT, which has the same model architecture and training objective
as GPT-2. CodeGPT performs better than GPT-2 in tasks such as code completion and code search.
Similarly, Clement et al. [20] compare the proposed PyMT5 with GPT-2 and reach the same
conclusion. CodeT5 also outperforms GPT-2 with better performance [100].
Xu et al. [103] claim that PolyCoder achieves lower complexity than all models, including Codex.
However, based on the HumanEval benchmark results provided in the article, PolyCoder falls behind
Codex by a considerable margin. It also performs worse than GPT-Neo with similar parameter
sizes.
Li et al. [53] introduce AlphaCode, which already surpasses GPT-Neo (APPS test) on all difficulty
levels with a small 1B parameter model, and it outperforms Codex-12B on interview and contest
difficulty levels. The article further analyzes the test results and finds that all encoder-decoder
models, including the final AlphaCode model, perform significantly worse in HumanEval com-
pared to decoder-only models. The article attributes this performance difference to the fact that
the encoder-decoder models align well with competitive programming settings. In competitive
programming settings, there is typically a dataset with clear inputs (programming contest problem
descriptions) and outputs (solution code), along with example tests for effective filtering. Therefore,
encoder-decoder models can learn effectively and sample efficiently. However, encoder-decoder
models are not aligned with the HumanEval setting, where the only training data is GitHub code,
20
Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen
which cannot be easily split into meaningful inputs and outputs. Thus, the fact that decoder-only
models compute loss for all tokens (instead of only one-third of the tokens) enables them to achieve
stronger results in the HumanEval setting. Therefore, more diverse benchmark tests may be needed
to quantify the true capabilities of Code LLMs, as a single benchmark may not fully reflect their
performance.
Fried et al. [26] evaluated the performance of INCODER-6.7B, as proposed in the article, on
the HumanEval and MBPP benchmarks. We can see that INCODER outperforms GPT-J-6B, which
has a similar number of parameters, by a significant margin, and even performs better than the
larger model GPT-NeoX-20B. However, there is still a notable gap between INCODER-6.7B and
CodeGen-Mono, Codex, and other Code LLMs.
Wang et al. [99] extensively evaluated CodeT5Mix on seven code-related tasks across 20 datasets
and demonstrated that it achieves state-of-the-art performance on most tasks. However, CodeT5Mix
was not directly compared to general-purpose LLMs in these metrics. In some metrics, the smaller-
scale CodeT5Mix outperformed larger models like LaMDA and GPT-2. Unfortunately, the article did
not provide a comparison between CodeT5Mix and GPT-3.5/GPT-4. On the other hand, WizardCoder
proposed by Luo et al. [57] was compared to GPT-3.5 and GPT-4 on HumanEval (pass@1(%)) and
MBPP. The experimental results showed that WizardCoder outperformed GPT-3.5 but was inferior
to GPT-4. However, it still performed significantly better than larger models like PaLM-540B and
PaLM-Coder-540B.
Li et al. [52] also demonstrated impressive performance in leapfrogging, performing well on
Common Sense Reasoning Benchmarks and surpassing general LLMs like Llama2-7B. However, it
was outperformed by general LLMs on Language Understanding and Knowledge Benchmarks. The
article presents results that make it difficult to assess the superiority of Code LLMs over general
LLMs. It challenges the popular notion that the ability of LLMs is solely determined by their scale,
indicating that data quality plays a more important role than previously believed. The article further
demonstrates the feasibility of achieving high-level functionality in smaller LLMs. Perhaps the
results presented in the article are far more important than assessing the performance of Code
LLMs compared to general LLMs.
Some recent Code LLMs have shown better performance. OpenCoderPlus [96] claims to achieve
102.5% of ChatGPT score on Vicuna GPT-4 evaluation and a 78.7 win rate on AlpacaEval. CodeFuse-
CodeLlama-34B [62] also outperformed GPT-4 in HumanEval (Pass@1). Additionally, MFTCoder [62]
published the performance of WizardCoder-Python-34B-V1.0 in HumanEval (Pass@1), which
is slightly better than GPT-4. CodeGeeX2-6B [93] also surpassed LLaMA2-70B in HumanEval
(Pass@1).
Some evaluation work has focused on analyzing the performance of Code LLMs. Among them,
some work has compared the performance differences between Code LLMs and general LLMs. Tang
et al. [90] evaluate results on BIOCODER showed that ChatGPT has a better understanding of the
application of imported toolkits or functions contained in other files. Xia et al. [101] explored the
vulnerability repair ability of LLMs and found that as the model size increases, there is a scaling
effect of performance improvement. Codex exhibited a more prominent performance. Palacio et
al. [68] evaluated LLMs’ features and their corresponding AsC-Eval performance, and in all the
test results, CodeGen demonstrated better performance compared to GPT-Neo and CodeParrot.
Pan et al. [69] evaluated the performance of LLMs on code translation tasks and found that, except
for GPT-4 and StarCoder, all other LLMs performed poorly. Among the two, GPT-4 performed
better. Kou et al. [43] assessed the attention differences between LLMs and programmers in code
generation. The article indicated that compared to models like GPT-J-6B and INCODER, CodeGen’s
attention is closer to that of human programmers. However, the article also found that LLMs pay
A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends
21
more attention to different types of keywords, while programmers are more sensitive to adjectives
and numbers in task descriptions.
Du et al. [25] provided Pass@k with Nucleus Sampling on ClassEval. ClassEval is a class-level
code generation benchmark test. The experimental results showed that GPT-4 and GPT-3.5 achieved
better performance compared to Code LLMs like WizardCoder, Instruct-StarCoder, and CodeGen.
We organize the results from the aforementioned paper into Table 3.
From the Table 3, we can see that out of the 38 papers, 11 experiments from various works
showed that general LLMs perform better on software engineering tasks, while in 20 experiments,
Code LLMs demonstrated superior performance. There are 7 papers where we couldn’t obtain a
clear conclusion regarding whether Code LLMs or general LLMs perform better based on their
experimental sections or conclusions.
Based on the above findings, it may be challenging to arrive at a definitive conclusion regarding
whether Code LLMs or general LLMs are better. Firstly, the current state-of-the-art general LLMs,
such as GPT-4 and GPT-3.5-turbo, are not open-sourced, which limits the ability to compare them
in many metrics. Additionally, there are restrictions when accessing the APIs of these models.
Secondly, due to the varying publication dates of each work, the choice of Code LLMs and general
LLMs differs among them [87], leading to variations in the evaluation results. For example, earlier
works often used GPT-2 and GPT-J as baselines for general LLMs [43], while more recent works
typically use Llama and GPT-3.5 as baselines [83]. Thirdly, the selection of benchmarks and software
engineering tasks differs among these studies [25, 27]. Lastly, different works may select different
versions of the same model with varying parameter sizes, leading to significant performance
differences for that model across different studies [105].
Based on the results from the aforementioned literature, we can still draw the following conclu-
sions:
• Model parameters have a significant impact on performance, with larger-scale models often
exhibiting better performance within the same model architecture [78, 101, 110].
• For the same model, fine-tuning the model specifically for software engineering tasks gener-
ally leads to improved performance compared to the base model [50, 113].
• When parameter sizes are comparable, Code LLMs tend to outperform general LLMs [26, 99].
• Currently, the state-of-the-art Code LLMs (such as CodeFuse-CodeLlama-34B and Open-
CoderPlus) outperform the current state-of-the-art general LLMs (GPT-4) in code generation
tasks [75, 96].
Furthermore, we can derive an interesting finding from the literature:
• The impact of parameter size on model performance may vary among different LLMs. In Zan
et al. [110], it is observed that different versions of CodeGen-Mono with parameter sizes of
2.7B, 6.1B, and 16.1B exhibit relatively small differences in performance on the MBPP and
HumanEval benchmarks. On the other hand, the variations between the two versions of
Codex, 2.5B and 12B, are relatively larger. Therefore, for Code LLMs, the marginal effect of
model parameter size on performance and the sensitivity of different models to parameter
size could be a topic worth discussing.
• All encoder-decoder models perform significantly worse in HumanEval compared to decoder-
only models. Li et al. [53] suggest that this discrepancy arises from the fact that encoder-
decoder models are not well-aligned with the HumanEval setup, whereas they align well
with competitive programming settings. In competitive programming, there is typically a
well-defined input (programming competition problem description) and output (solution
code). Encoder-decoder models may struggle to split the input and output into meaningful
segments for HumanEval, thus calculating the loss for all tokens in the decoder-only models
22
Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen
Table 3. The performance of Code LLMs and general LLMs in various literatures.
Work
General LLMs
Code LLMs
Unknown
Task
Jiang et al. [38]
✓
Code Generation
Maddigan et al. [60]
✓
Data visualization
Shirafuji et al. [82]
✓
Robustness of solving programming problems
Li et al. [48]
✓
Code Generation
Li et al. [47]
✓
Text-to-SQL
Yang et al. [105]
✓
Code Generation
Thakur et al. [91]
✓
Verilog Code Generation
Siddiq et al. [83]
✓
Code Generation
Sun et al. [86]
✓
Text-to-SQL
Sun et al. [87]
✓
Code Summarization
Wang et al. [98]
✓
Code Generation
Li et al. [50]
✓
Code Generation
Li et al. [49]
✓
Information extraction
Zhang et al. [113]
✓
API-related Tasks
Siddiq et al. [84]
✓
Unit test Generation
Zhuo et al. [124]
✓
Code Generation
Zheng et al. [116]
✓
Code Generation
Rozière et al. [78]
✓
Code Generation
Fu et al. [27]
✓
Code Generation& Understanding
Zan et al. [110]
✓
Code Generation
Lu et al. [56]
✓
Code Completion & Code Search
Clement et al. [20]
✓
Code Generation
Yue et al. [100]
✓
Code Generation
Xu et al. [103]
✓
Code Generation
Li et al. [53]
✓
Code Generation
Fried et al. [26]
✓
Code Generation
Wang et al. [99]
✓
Seven code-related tasks
Luo et al. [57]
✓
Code Generation
Li et al. [52]
✓
Code Generation
Guan et al. [96]
✓
Code Generation
MFTCoder [62]
✓
Code Generation
THUDM [93]
✓
Code Generation
Tang et al. [90]
✓
Understanding functions
Xia et al. [101]
✓
Vulnerability Remediation
Palacio et al. [68]
✓
Code Syntax Understanding
Pan et al. [69]
✓
Code Translation
Kou et al. [43]
✓
Attention to Programming Problems
Du et al. [25]
✓
Code Generation
Total Number
11
20
7
enables them to achieve stronger results in the HumanEval setting. Therefore, a more diverse
set of evaluation benchmarks may be needed to accurately quantify the true capabilities of
Code LLMs, as a single benchmark may not fully capture their performance.
A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends
23
• The abilities of LLMs may not be solely determined by their model size [52], and data quality
can play a crucial role. This is exemplified by phi-1.5-1.3B, which has been able to outperform
LLMs with parameter sizes five times larger.
5
WHICH SOFTWARE ENGINEERING TASKS ARE DIFFERENT LLMS PARTICULARLY
GOOD AT?
In this section, we primarily focus on addressing and summarizing RQ3. First, we will gather the
relevant papers and extract the sections that evaluate LLMs in software engineering tasks. Next, we
will classify these works according to different software engineering tasks and further categorize
them based on the evaluation benchmarks used. Our goal is to compile a relatively comprehensive
list for each benchmark.
5.1
Code Generation
Code generation is currently one of the most prominent software engineering tasks of interest
for Code LLMs. Consequently, there is a rich variety of benchmarks available to evaluate LLMs in
code generation tasks. Some commonly used benchmarks include HumanEval [16], DS-1000 [44],
and MBPP [121]. Additionally, there have been efforts to propose new benchmarks from different
perspectives to assess the code generation capabilities of LLMs. For instance, Muennighoff et al.[64]
introduced HUMANEVALPACK to evaluate the multilingual code generation ability of LLMs, and
Du et al.[25] proposed ClassEval, a class-level code generation benchmark.
However, to the best of our knowledge, there is currently no organization or individual actively
maintaining these benchmarks. Therefore, it is challenging to find a comprehensive list that
showcases the performance of LLMs on a specific benchmark. In this section, we will organize
LLMs according to different benchmarks to facilitate a better comparison of their performance.
HumanEval: HumanEval consists of 164 handwritten Python programming problems. Each
problem provides a prompt that includes a description of the function to be generated, the function
signature, and example test cases in the form of assertions. The models are required to complete
the functionality based on the prompt, aiming to pass all the provided test cases, thereby measuring
their performance in terms of functional correctness. HumanEval is currently the most commonly
used benchmark to test the code generation capabilities of LLMs. We have organized the results
from the collected papers on HumanEval and obtained Table 4. In the case of *, this result is sourced
from Shen et al. [81].
We have marked the top five scores from each evaluation data using underlines and bold
formatting. From the table, we can see that LATS( GPT-4 based ) [121] performs the best in Pass@1.
In Pass@10, the best performer is Unnatural-Code-LLaMA-34B [78], and it remains the best in
Pass@100 as well. Considering the results from all three criteria, Unnatural-Code-LLaMA-34B is
currently the top-performing LLM (Language Model) according to HumanEval. To a certain extent,
GPT-4 and Unnatural-Code-LLaMA-34B is also the best-performing LLM for code generation tasks
at the moment. The next best performers are Code-LLaMA-Python-34B and Code-LLaMA-Python-
13B.
It is important to note that there may be variations in the data reported for the same model
across different papers. We have selected data from more recent publications. Additionally, for
cases where there are differences in the reported data, we have included them in Table 5 as well.
There are also some benchmarks such as APPS [32] and CodeXGLUE [56] where the available
data is limited, making it difficult to showcase performance differences among different LLMs.
Therefore, we won’t be presenting them here. Additionally, there are more complex benchmarks
that require different settings for different tasks, such as HumanEvalX. As a result, each paper
24
Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen
Table 4. Performance of LLMs in HumanEval Benchmark.
LLMs
HumanEval
LLMs
HumanEval
Pass@1 Pass@10 Pass@100
Pass@1
Pass@10 Pass@100
LATS(GPT-4 based)
94.4
-
-
LLaMA2-13B
20.1
34.8
61.2
Reflexion(GPT-4 based)
91
-
-
CodeGen-multi-16.1B
19.22
34.64
55.17
LATS(GPT-3.5 based)
86.9
-
-
CodeGen-16B-multi
19.2
34.6
55.2
Parsel
85.1
-
-
CodeGen2-7B
18.83
31.78
50.41
GPT-4(*)
82
-
-
CodeGen-multi-16B
18.3
-
-
MetaGPT
81.7
-
-
CodeGen-multi-6.1B
18.16
27.81
44.85
CodeFuse-CodeLlama-34B
74.4
-
-
SantaCoder-1.1B
18
29
49
Phind-CodeLlama-34B-v2
73.8
-
-
AlphaCode-1.1B
17.1
28.2
45.3
WizardCoder-Python-34B
73.2
-
-
PanGu-Coder-317M
17.07
24.05
34.55
Phind-CodeLlama-Python-34B-v1 69.5
-
-
Codex-679M
16.22
25.70
40.95
GPT-3.5(*)
68.9
-
-
PaLM-62B
15.9
-
46.3
Phind-CodeLlama-34B-v1
67.6
-
-
LLaMA-13B
15.8
-
52.5
GPT-4(OpenAI)
67
-
-
BLOOM-176B
15.52
32.20
55.45
Unnatural-Code-LLaMA-34B
62.2
85.2
95.4
CodeT5+-770M
15.5
27.2
42.7
PanGu-Coder2-15B
61.64
79.55
91.75
GPT-NeoX-20B
15.4
25.6
41.2
WizardCoder-15B
57.3
73.2
90.46
InCoder-6.7B
15.2
27.8
47.0
Code-LLaMA-Python-34B
53.7
82.8
94.7
InCoder-multi-6.7B
15.2
27.8
47.0
Phi-1-1.3B
50.6
-
-
InCoder-6B
15.2
27.8
47.0
Code-LLaMA-34B
48.8
76.8
93.0
CodeGen-multi-2.7B
14.51
24.67
38.56
GPT-3.5(OpenAI)
48.1
-
-
Codegen-NL-16.1B
14.24
23.46
38.33
code-davinci-002
47.0
74.9
92.1
AlphaCode(dec)-685M
14.2
24.4
38.8
OctoCoder
46.2
-
-
LaMDA-137B
14.0
-
47.3
Code-LLaMA-Python-13B
43.3
77.4
94.1
Codex-300M
13.17
20.37
36.27
Code-LLaMA-Instruct-13B
42.7
71.6
91.6
CodeGen-Mono-350M
12.76
23.11
35.19
Code-LLaMA-Instruct-34B
41.5
77.2
93.5
CodeGen-mono-350M
12.76
23.11
35.19
StarCoder-Prompted-15B
40.8
-
-
LLaMA2-7B
12.2
25.2
44.4
code-davinci-001
39
60.6
84.1
CodeT5-770M
12.09
19.24
30.93
Code-LLaMA-Python-7B
38.4
70.3
90.6
CodeT5+-220M
12.0
20.7
31.6
PaLM-2-S
37.6
-
88.4
GPT-J-6B
11.62
15.74
27.74
PaLM-Coder-540B
36
-
88.4
AlphaCode-302M
11.6
18.8
31.8
Code-LLaMA-13B
36.0
69.4
89.8
LLaMA-7B
10.5
-
36.5
CodeGeeX2-6B
35.9
62.6
88.3
CODEGEN-NL-6.1B
10.43
18.36
29.85.
InstructCodeT5+-16B
35.0
54.5
77.9
InCoder-1.3B
8.9
16.7
25.6
Code-LLaMA-Instruct-7B
34.8
64.3
88.1
PyCodeGPT-110M
8.33
13.36
19.13
CodeGen-16.1B
34.6
-
-
Codex-85M
8.22
12.81
22.40
StarCoder-Python-15B
33.6
-
-
BLOOM-7.1B
7.73
17.38
29.47
StarCoder-15B
33.60
45.78
79.82
CODEGEN-NL-2.7B
6.7
14.15
22.84
code-cushman-001
33.5
54.3
77.4
CODEGEN-MULTI-350M 6.67
10.61
16.84
Code-LLaMA-7B
33.5
59.6
85.9
BLOOM-3B
6.48
11.35
20.43
CodeGen2.5-7B-mono
33.4
58.4
82.7
GPT-NEO-2.7B
6.41
11.27
21.37
CodeT5+-16B-mono
30.9
51.6
76.7
PolyCoder-2.7B
5.59
9.84
17.68
MIM-2.7B
30.7
48.22
69.6
JuPyT5-300M
5.4
15.46
25.60
Replit-Finetuned-2.7B
30.5
-
-
Codex-42M
5.06
8.8
15.55
LLaMA2-70B
30.5
59.4
87.0
GPT-Neo-1.3B(1.5B)
4.79
7.47
16.30
StarCoderBase-15B
30.4
-
-
GPT-Neo-1.3B
4.79
7.47
16.3
CodeGen-mono-16B(16.1B)
29.28
49.86
75.00
AlphaCode(dec)-89M
4.3
12.2
20.0
Codex-12B
28.81
46.81
72.31
AlphaCode(dec)-55M
4.2
8.2
16.9
CodeGen2.5-7B
28.36
47.46
75.15
BLOOM-1.7B
4.03
7.45
12.75
CodeT5+-6B
28.0
47.2
69.8
CodeParrot-multi-1.5B
4.0
8.7
17.9
PaLM-540B
26.2
-
76.2
CodeParrot-1.5B
3.99
8.69
17.88
CodeGen-Mono-6.1B
26.13
42.29
65.82
CodeParrot-110M
3.8
6.57
12.78
CodeGen-mono-6B
26.1
42.3
65.8
CodeParrot-1.5B
3.8
6.57
12.78
CodeT5+-2B
24.2
38.2
57.8
PaLM-8B
3.6
-
18.7
PanGu-Coder-2.6B
23.78
35.36
51.24
CodeParrot-small-110M
3.58
8.03
14.96
CodeGen-Mono-2.7B
23.7
36.64
57.01
AlphaCode(dec)-29M
3.4
5.8
11.2
LLaMA-65B
23.7
-
79.3
Codex-25M
3.21
7.1
12.89
CodeGen-mono-2B
23.7
36.6
57
PolyCoder-400M
2.96
5.29
11.59
CodeGeeX-13B
22.89
39.57
60.92
BLOOM-1.1B
2.48
5.93
9.62
LLaMA2-34B
22.6
47.0
79.5
PolyCoder-160M
2.13
3.35
4.88
MIM-1.3B
22.4
41.7
53.8
CODEGEN-NL-350M
2.12
4.10
7.38
Replit-3B
21.9
-
-
Codex-12M
2
3.62
8.58
Replit-2.7B
21.9
-
-
AlphaCode-13M
1.5
3.6
8.6
LLaMA-33B
21.7
-
70.7
GPT-NEO-350M
0.85
2.55
5.95
Codex-2.5B
21.36
35.42
59.50
BLOOM-560M
0.82
3.02
5.91
CodeGen2-16B
20.46
36.5
56.71
GPT-Neo-125M
0.75
1.88
2.97
A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends
25
Table 5. Performance inconsistency reported in different works on HumanEval.
LLMs
HumanEval
Pass@1
Pass@10
Pass@100
InCoder-1.3B [98]
8.9
16.7
25.6
InCoder-1.3B [110]
11.09
16.14
24.20
CodeGen-multi-16.1B [116]
19.22
34.64
55.17
CodeGen-multi-16.1B [67]
18.32
32.07
50.80
Table 6. Percentage of compilable suggestions (code snippets) in [83].
Code Par-
rot small
CodeParrot
regular
InCoder
1B
CodeGen
350M
mono
CodeGen
350M
multi
CodeGen
2B mono
CodeGen
2B multi
PolyCoder
160M
PolyCoder
0.4B
PolyCoder
2.7B
GPT 3.5
Java
-
0.15%
-
5.86%
-
7.43%
0.13%
0.29%
0.19%
0.89%
Python
22.22%
25.98%
34.56%
37.06%
34.29%
40.49%
19.73%
21.33%
22.72%
57.75%
Table 7. NDCG@10 scores for the original model ranking in [83].
Code Par-
rot small
CodeParrot
regular
InCoder
1B
CodeGen
350M
mono
CodeGen
350M
multi
CodeGen
2B mono
CodeGen
2B multi
PolyCoder
160M
PolyCoder
0.4B
PolyCoder
2.7B
GPT 3.5
Java
-
-
0.0979
0.2330
-
-
0.3019
0.1385
0.1525
0.2264
0.5775
Python
0.3297
0.4021
0.2012
0.3944
0.3640
0.4745
0.4738
0.3740
0.4200
0.4541
0.3742
provides different experimental setups and results. It is also challenging to observe the performance
of these LLMs in the context of HumanEvalX.
There have been various studies that have explored the performance of LLMs in code generation
from different perspectives. Zan et al. [108], focused on evaluating LLMs’ performance in generating
code tailored to private libraries. The study proposed four benchmark metrics for private libraries,
including TorchDataEval, TorchDataComplexEval, MonkeyEval, and BeatNumEval. Based on the
data provided in the paper, we can observe that the proposed CodeGenAPI-6B performs the best in
this task. CodeGenAPI-6B is developed based on CodeGen. If we exclude CodeGenAPI, then the
best performer is code-davinci-002 (Codex), followed by CodeGen-6B.
Siddiq et al. [83] introduced a lightweight framework for recommending more secure source
code derived from LLMs. The article also provides performance results for several LLMs, as shown
in Tables 6and Tables 7. We can see that GPT-3.5 performs better than other LLMs. However, it’s
important to note that the evaluated LLMs in the article may not be state-of-the-art LLMs, so the
obtained results have limited value as a comparison to current top-performing models. However,
from the tables, we can also observe that the performance of the same LLM can vary significantly
across different programming languages.
Zhang et al. [113] also evaluated the performance of several LLMs on PublicLibrary Benchmark
and Private Library Benchmark, as shown in Tables 8 and Tables 9. Although the primary focus of
Zhang et al [113] was to introduce the proposed ToolCoder, we can also observe the differences
in LLMs’ ability to call APIs during code generation from Tables A and B. Based on the results
provided in the article, ToolCoder is undoubtedly the best performer, followed by GPT-3.5. However,
the performance of CodeGenAPI, which is CodeGen fine-tuned for this task, did not surpass that of
CodeGen.
Zhong et al. [119] also evaluated the issue of API misuse in code generation by LLMs. Table 10
in the article presents the test results for GPT-3.5/GPT-4/Llama/Vicunad on the ROBUSTAPI
26
Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen
Table 8. Pass rates of models on public library benchmarks in [113].
Model
NumpyEval
PandasEval
TorchDataEval
pass@1
pass@10
pass@1
pass@10
pass@1
pass@10
CodeT5-220M
0
0.1
0
0
0
0
PyCodeGPT-11M
18.04
38.61
12.75
37.62
3.80
14.00
CodeGen-350M
18.51
43.56
16.73
29.70
4.60
14.00
CodeGen2B-2B
29.10
53.46
30.69
42.57
7.00
18.00
GPT-3.5
58.41
66.21
30.09
33.16
6.00
24.00
CERT-numpy-220M
31.47
46.42
16.03
27.72
2.20
14.00
CERT-pandas-220M
18.81
33.66
28.42
48.04
2.80
6.00
CodeGenAPI-350M
16.55
29.48
13.58
34.95
7.19
16.93
CodeGenAPI-retrieval-475M
12.67
27.32
11.25
28.61
10.41
23.50
CodeGen-retrieval-475M
18.30
35.12
9.54
29.02
7.52
16.36
ToolCoder-OnlineTool-350M
35.64
50.50
22.77
37.62
7.40
20.00
ToolCoder-OnlineTool-2B
41.58
55.44
31.68
47.52
11.80
24.00
Table 9. Pass rates of models on private library benchmarks in [113].
Model
MonkeyEval
BeatNumEval
pass@1
pass@10
pass@1
pass@10
CodeT5-220M
0
0
0
0
CodeGen-350M
0.95
4.90
5.15
11.96
CodeGen-2B
1.59
5.94
5.94
11.88
GPT-3.5
2.47
8.91
6.68
17.82
CodeGenAPI-350M
1.19
4.68
4.44
8.24
CodeGenAPI-retrieval-475M
3.41
8.33
5.90
11.79
CodeGen-retrieval-475M
2.46
6.35
6.65
13.68
ToolCoder-DocTool-350M
2.98
5.94
6.73
12.87
ToolCoder-DocTool-2B
3.02
7.92
6.93
13.86
Table 10. Performance of LLMs on ROBUSTAPI in [119].
LLMs
Zero-shot
One-shot-irrelevant
One-shot-relevant
Misuse
Rate
Exec.
Sample
Overall
Misuse
Misuse
Rate
Exec.
Sample
Overall
Misuse
Misuse
Rate
Exec.
Sample
Overall
Misuse
GPT-3.5
62.97%
79.14%
49.83%
68.09%
91.06%
62.00%
38.56%
80.71%
31.13%
GPT-4
68.81%
90.23%
62.09%
70.38%
91.39%
64.32%
54.40%
90.40%
49.17%
Llama-2
7.34%
9.02%
0.66%
61.36%
80.13%
49.17%
64.47%
72.93%
47.02%
Vicuna-
1.5
45.66%
37.17%
16.97%
57.85%
83.86%
48.51%
42.53%
64.24%
27.32%
benchmark. The misuse rate refers to the proportion of misuse cases among executable cases, exec.
The sample represents the proportion of executable cases among all questions, and the overall
misuse percentage is the proportion of misuse cases among all questions. However, from Table 10,
it is difficult to determine which LLM performs better. We can only conclude that LLMs commonly
exhibit API misuse issues, even when generating code that is executable and aligns with the user’s
intent.
Tu et al. [24] introduce and define the problem of error code completion, where given a problem
statement and partially coded program with potential errors, the task is to complete the coding
A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends
27
Table 11. Pass@1 of completion methods on buggy-HumanEval and buggy-FixEval datasets in [24].
Prefix
Method
buggy-HumanEval
buggy-FixEval
CodeGen
InCoder
CodeGen
InCoder
350M
2 B
1 B
6 B
350M
2 B
1 B
6 B
Clean
completion
43.0
54.9
41.1
50.6
27.6
37.8
24.1
32.3
Buggy
completion
0.7
3.1
0.5
1.0
2.4
4.3
1.2
1.8
Table 12. Pass@k with Nucleus Sampling on ClassEval in [25].
Model
Class-level
Method-level
Pass@1
Pass@3
Pass@5
Pass@1
Pass@3
Pass@5
GPT-4
37.6%
41.3%
42.0%
62.8%
67.4%
68.5%
GPT-3.5
29.6%
34.9%
36.0%
50.4%
59.0%
61.1%
WizardCoder
12.2%
20.0%
23.0%
35.2%
47.1%
51.1%
Instruct-StarCoder
10.2%
12.7%
14.0%
23.1%
26.5%
27.7%
SantaCoder
8.6%
9.9%
10.0%
27.7%
33.0%
34.9%
Instruct-CodeGen
8.2%
12.3%
13.0%
24.9%
34.3%
37.1%
CodeGeeX
7.2%
9.4%
10.0%
21.2%
27.1%
29.5%
InCoder
6.2%
7.6%
8.0%
21.1%
26.5%
29.1%
Vicuna
3.0%
3.6%
4.0%
11.0%
15.8%
18.4%
ChatGLM
1.4%
2.6%
3.0%
8.2%
11.2%
12.4%
PolyCoder
1.4%
2.2%
3.0%
13.2%
17.5%
19.6%
program. The article selects InCoder and CodeGen as the compared Code LLMs in the experiments.
According to the experimental data provided in Table 11, CodeGen-2B consistently achieves better
results across various metrics. However, it’s important to note that the article only compares
InCoder and CodeGen, which limits the scope of the results presented in the article.
Fu et al. [27] introduce CodeApex, a benchmark focused on evaluating LLMs’ programming
understanding and code generation capabilities. In terms of programming understanding, GPT-3.5-
turbo consistently ranks first in almost all tasks, followed by InternLM-Chat-7B. The experimental
conclusions for code generation are similar, with GPT-3.5-turbo performing the best. Additionally,
WizardCoder-15B also achieves very good results. It’s worth noting that the article’s evaluation of
LLMs is limited, and there is relatively little work that has tested LLMs using CodeApex.
Du et al. [25] attempted to evaluate LLMs’ class-level code generation capabilities and introduced a
class-level code generation benchmark called ClassEval. The article also presents the performance of
11 state-of-the-art LLMs on ClassEval, as shown in Table 12. We can observe that GPT-4 achieves the
best performance in almost all metrics, followed by GPT-3.5. Among the Code LLMs, WizardCoder
achieves the best scores.
Yu et al. [106] introduce a benchmark called CoderEval to evaluate the performance of models in
generating practical code. Compared to the HumanEval benchmark, CoderEval includes program-
ming tasks from various open-source projects and provides full coverage testing to assess models’
performance in practical code generation. The article does not conduct large-scale evaluation ex-
periments on existing LLMs but compares the performance of CodeGen, Codex, and PanGu-Coder
on CoderEval. The results indicate that Codex performs better in various testing scenarios.
Based on the information provided, we can see that the Code-LLaMA series of LLMs performs
well in several commonly used code generation benchmarks. Among them, Unnatural-Code-LLaMA-
34B stands out with outstanding performance. For API-related code generation tasks, ToolCoder
performs better. Additionally, GPT-4 and GPT-3.5 (GPT-3.5-turbo) also exhibit good performance.
28
Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen
Table 13. Average solved rates (%) for each type of problem formatting in [82].
Formatting
CodeGen
Codex
InstructGPT
ChatGPT
Raw HTML
10.9
30.9
75.9
90.1
Parsed plain
9.2
34.7
73.5
89.0
AlphaCode-inspired
9.7
35.2
72.0
88.7
APPS-inspired
11.7
39.3
73.7
88.6
Fully Markdown-formatted
9.9
39.9
74.5
89.0
Average
10.3
36.0
73.9
89.1
Variance
1.01
13.61
2.04
0.36
Table 14. Performance comparison on Test Suite accuracy on Spider Dev Split in [82].
LLMs
Execution Accuracy
Test-suite Accuracy
GPT-3 ada (0-shot)
2.3
0.3
GPT-3 babbage (0-shot)
5.7
3.9
GPT-3 curie (0-shot)
12.6
8.3
GPT-3 davinci (0-shot)
26.3
21.7
CodeX cushman (0-shot)
63.7
53.0
CodeX davinci (0-shot)
67.0
55.1
CodeX davinci (few-shot)
71.0
61.5
ChatGPT (w/ OpenAI-default Prompt)
70.1
60.1
GPT-4 (Zero-shot)
72.9
64.9
GPT-4 (Few-shot)
76.8
67.4
5.2
Test Case Generation
In Schäfer et al. [80], GPT-3.5-turbo, StarCoder, and code-cushman-002 were tested for the number
of generated tests and the percentage of passed generated tests. The experimental results in the
article showed that the code-Cushman-002 model had a test coverage rate comparable to that
of GPT-3.5-turbo, with the latter having slightly higher median statement and branch coverage.
StarCoder exhibited relatively poorer performance in comparison.
Shirafuji et al. [82] aims to demonstrate the high capability of LLMs in solving a wide range of
programming problems, specifically their robustness in solving programming problems. Therefore,
the article assigns each LLM to generate 100 programs for each of the 40 questions and tests their
performance, as shown in Table 13. On average, we can observe that Codex performs three times
better than CodeGen. Codex’s successor, InstructGPT, improves the average success rate by twofold,
while ChatGPT exhibits even greater improvement. The article suggests that these significant
performance differences also reflect the impact of fine-tuning on programming problems, as Codex
is fine-tuned while CodeGen is not.
According to Sun et al. [86], a performance comparison of GPT-4, GPT-3.5, and Codex was
conducted on the Spider Dev Split test suite to evaluate their accuracy. The results are presented in
Table 14. It can be observed that on the Spider Dev Split, GPT-4 outperforms Codex, while Codex’s
performance is better than that of GPT-3.5.
In Siddiq et al. [84], ChatGPT, codegen-350M-multi, and Codex (2K and 4K) were evaluated for
their ability to generate unit tests. Although the article provides a detailed test analysis for these
three LLMs, it does not provide definitive results. Even when considering the compilation success
rate, it is not possible to draw deterministic conclusions, as shown in Table 15.
A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends
29
Table 15. Compilation status of the generated unit tests in [9].
Benchmark
LLM
Compilable
Compilable
(after fix)
Test
Methods
Test
Files
HumanEval
ChatGPT
43.1%
81.3%
1,117
130
HumanEval
CodeGen
23.8%
33.1%
844
529
HumanEval
Codex (2K)
37.5%
100%
697
160
HumanEval
Codex (4K)
44.4%
99.4%
774
159
SF110
ChatGPT
9.7%
85.9%
194
87
SF110
CodeGen
21.0%
58.5%
83
139
SF110
Codex (2K)
2.7%
74.5%
1,406
222
SF110
Codex (4K)
3.4%
83.5%
1,039
152
Table 16. Performance comparison on Test Suite accuracy on Spider Dev Split in [87].
Method
CSN-Python
BLEU
METEOR
ROUGE-L
NCS
15.8
10.6
31.3
CodeBERT
18.7
12.4
34.8
CodeT5
20.0
14.7
37.7
ChatGPT (one sentence)
10.28
14.40
20.81
Table 17. Performance (smoothed BLEU-4) on code summarization on CodeSearchNet in [98].
LLMs
Ruby
JS
Go
Python
Java
PHP
Overall
RoBERTa-125M
11.17
11.90
17.72
18.14
16.47
24.02
16.57
CodeBERT-125M
12.16
14.90
18.07
19.06
17.65
25.16
17.83
UniXcoder-125M
14.87
15.85
19.07
19.13
20.31
26.54
19.30
CodeGen-multi-350M
13.48
16.54
18.09
18.31
19.41
24.41
18.37
PLBART-140M
14.11
15.56
18.91
19.30
18.45
23.58
18.32
CodeT5-220M
15.24
16.16
19.56
20.01
20.31
26.03
19.55
CodeT5+-220M
15.51
16.27
19.60
20.16
20.53
26.78
19.81
CodeT5+-770M
15.63
17.93
19.64
20.47
20.83
26.39
20.15
Based on the available information, it can be concluded that in the task of test case generation,
GPT-4 and GPT-3.5 (GPT-3.5-turbo) show better performance.
5.3
Code Summarization
Sun et al. [87] present the overall performance of ChatGPT (one sentence), NCS, CodeBERT, and
CodeT5 on the CSN-Python dataset. In Table 16, We can see that CodeT5 performs the best among
the four models in the code summarization task, outperforming ChatGPT. However, it is important
to note that the article’s evaluation includes a limited selection of LLMs, which still imposes
limitations on the presented results.
Wang et al. [98] also presented the performance of several LLMs on CSN and released CodeT5+.
The details of their performance can be found in Table 17. Furthermore, Wanget al. [99] demonstrates
the performance of CodeT5Mix on smoothed BLEU-4. CodeT5Mix is an improved version of CodeT5.
However, based on the results, CodeT5Mix performs almost on par with CodeT5 and does not show
significant performance improvement.
30
Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen
Table 18. Performance of code translation task in HumanEval-X in [116].
Model
Target Language
Python
C++
Java
JavaScript
Go
@1
@10 @100 @1
@10 @100 @1
@10 @100 @1
@10 @100 @1
@10 @100
Py
InCoder-6.7B
-
-
-
26.11 41.00 54.25 26.74 42.66 61.20 37.05 58.85 78.91 15.69 27.57 43.67
CodeGen-Multi-16B -
-
-
35.94 47.81 59.37 29.27 45.70 64.45 43.40 66.26 82.55 28.87 41.01 57.72
CodeGeeX-13B
-
-
-
26.54 43.56 56.48 25.84 41.52 59.72 23.22 47.33 65.87 9.56 23.83 33.56
CodeGeeX-13B-FT
-
-
-
34.16 46.86 61.22 41.98 58.17 72.78 34.81 53.05 66.08 16.41 30.76 46.37
C + +
InCoder-6.7B
34.37 58.41 78.57 -
-
-
34.04 57.02 68.70 37.05 65.05 79.61 25.54 39.11 58.02
CodeGen-Multi-16B 33.83 55.37 76.64 -
-
-
43.20 69.84 88.82 54.51 71.50 83.14 27.94 49.73 68.32
CodeGeeX-13B
27.18 49.02 67.69 -
-
-
22.56 40.91 64.08 30.23 55.68 75.58 8.64 18.79 31.76
CodeGeeX-13B-FT
62.79 80.39 87.10 -
-
-
71.68 81.62 85.84 50.83 64.55 74.57 16.71 34.18 52.98
Java
InCoder-6.7B
42.76 65.55 80.43 40.01 55.17 70.39 -
-
-
43.20 68.24 84.39 21.58 35.20 54.97
CodeGen-Multi-16B 52.73 69.30 82.74 41.42 54.68 65.50 -
-
-
57.65 67.90 79.22 34.00 48.49 67.94
CodeGeeX-13B
43.41 68.46 84.03 39.33 58.48 72.36 -
-
-
44.19 64.22 82.89 17.17 32.74 47.71
CodeGeeX-13B-FT
75.03 87.71 95.13 49.67 65.65 75.40 -
-
-
49.95 62.82 79.64 18.85 32.92 48.93
JS
InCoder-6.7B
23.18 50.47 67.26 35.47 54.48 70.71 30.67 50.90 71.03 -
-
-
25.79 42.96 61.47
CodeGen-Multi-16B 35.52 52.23 69.78 35.41 53.12 64.47 33.79 56.06 74.00 -
-
-
33.38 49.08 64.14
CodeGeeX-13B
31.15 54.02 72.36 30.32 51.63 69.37 24.68 48.35 69.03 -
-
-
11.91 26.39 39.81
CodeGeeX-13B-FT
67.63 81.88 89.30 46.87 60.82 73.18 56.55 70.27 80.71 -
-
-
16.46 32.99 50.29
Go
InCoder-6.7B
34.14 54.52 70.88 30.45 48.47 62.81 34.52 53.95 69.92 39.37 63.63 80.75 -
-
-
CodeGen-Multi-16B 38.32 50.57 68.65 32.95 45.88 59.56 36.55 59.12 78.70 38.93 56.68 70.68 -
-
-
CodeGeeX-13B
35.92 56.02 77.32 29.83 41.98 58.15 22.89 41.04 61.46 25.24 46.50 69.93 -
-
-
CodeGeeX-13B-FT
57.98 79.04 93.57 38.97 53.05 63.92 54.22 69.03 79.40 43.07 59.78 74.04 -
-
-
Based on the available information, it can be concluded that in the task of code summarization,
CodeT5+ demonstrates better performance compared to GPT-3.5 (GPT-3.5-turbo).
5.4
Code Translation
As shown in Tables 18, the performance of CodeGeeX on code translation tasks and compared with
two other models., it can be observed that CodeGeeX-13B-FT exhibits relatively better performance,
while CodeGen-Multi-16B also performs exceptionally well. However, their strengths lie in different
multilingual scenarios. Nonetheless, the non-fine-tuned CodeGeeX-13B does not perform as well
as CodeGen-Multi-16B in code translation. On the XLCoST benchmark, CodeGeeX outperforms
CodeT5, although the difference in scores between the two is not significant.
Pan et al. [69] also provide the performance of seven LLMs, including GPT-4 and StarCoder,
on code translation tasks across seven datasets. From the test results presented in the article (as
shown in Table 19, for detailed experimental settings please refer to Pan et al. [69]), it can be
observed that, except for GPT-4 and StarCoder, the other models perform poorly. The article also
points out a strong correlation between the average number of test attempts per translation sample
and unsuccessful translations. Additionally, unsuccessful translations do not exhibit consistent
patterns between the source and target languages, but due to stricter GO syntax constraints, code
translations related to GO perform poorly.
In the task of code translation, GPT-4 performs better. This is supported by Pan et al. [69], who
found that GPT-4 outperforms CodeGeeX significantly in terms of performance.
A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends
31
Table 19. Performance of subject LLMs in translating code.
Dataset
% Unsuccessful Translations
CodeGen
CodeGeeX
StarCoder
GPT-4
Llama 2
TB-Airoboros
TB-Vicuna
CodeNet
76.6%
85.1%
58.0%
17.0%
85.1%
81.2%
95.6%
86.0%
96.4%
60.9%
20.0%
90.5%
91.7%
96.6%
85.7%
94.1%
58.0%
14.5%
83.1%
93.4%
99.1%
78.7%
89.7%
69.7%
18.7%
86.1%
93.5%
99.9%
82.5%
92.7%
66.7%
20.1%
89.0%
93.5%
99.0%
Total/Average (CodeNet)
81.9%
91.6%
62.7%
18.0%
86.8%
90.7%
98.0%
AVATAR
91.9%
98.2%
88.1%
29.2%
98.2%
94.9%
100%
96.2%
98.4%
85.8%
47.8%
95.3%
99.1%
99.1%
Total/Average (AvATAR)
94.1%
98.3%
87.0%
38.5%
96.8%
97.0%
99.6%
EvalPlus
83.5%
96.3%
78.0%
20.7%
98.8%
86.0%
92.1%
Commons CLI
100%
100%
100%
86.4%
100%
100%
100%
Click
100%
100%
100%
100%
100%
100%
100%
Total/Average (All)
91.9%
97.2%
85.5%
52.7%
96.5%
94.7%
97.9%
5.5
Vulnerability Repair
MBPP: MBPP is also one of the important benchmarks for evaluating the code generation capabili-
ties of LLMs. MBPP, which stands for Massively Bugs and Performance Problems, is a benchmark
that consists of a large number of code snippets with defects and performance issues. The models
are required to generate the correct repair code that is relevant to the given problem. The benchmark
aims to assess the models’ ability to identify and resolve software errors and performance problems.
By using the MBPP benchmark, the practicality and robustness of the models in real-world software
engineering scenarios can be evaluated. We have organized the results from the collected papers
on MBPP and obtained Table 20.
Similarly, while organizing the data for MBPP, we have also noticed variations in the reported
data across different literature sources. We have compiled and included these variations in Table 21
as well. We have followed the principle of selecting the most recent literature to obtain the data.
We have also marked the top five performing data with underlines and bold for each metric (except
for pass@80). We can see that the Code-LLaMA series continues to exhibit strong performance,
carrying forward its excellent performance on HumanEval. Among them, Unnatural-Code-LLaMA
shows the best overall performance, followed by Code-LLaMA-Python-34B. However, it should
be noted that, unlike HumanEval, it is challenging to find scores for many LLMs on MBPP. We
attempted to search using a snowballing approach but did not make significant progress. Based on
the currently available data, Unnatural-Code-LLaMA-34B is the top-performing LLM on MBPP. To
some extent, Unnatural-Code-LLaMA-34B is also the best-performing LLM for code generation
tasks currently available.
Pearce et al. [71] explore the ability of LLMs to fix software vulnerabilities in a zero-shot
setting. The experimental section of the article mainly utilizes the following LLMs: code-cushman-
001, code-davinci-001, code-davinci-002, j1-large, j1-jumbo, and polycoder. Overall, based on the
combined evaluation datasets, code-davinci-002 performs relatively well. However, the article
does not explicitly provide performance differences between the LLMs. The study finds that LLMs
can generate fix programs for security vulnerabilities when provided with carefully constructed
32
Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen
Table 20. Performance of LLMs in MBPP benchmark.
LLMs
MBPP
LLMs
MBPP
Pass@1Pass@10Pass@80Pass@100
Pass@1Pass@10Pass@80Pass@100
WizardCoder-16B
51.8
-
-
-
CodeParrot-110M
0.48
3.89
-
15.93
Unnatural-Code-LLaMA-34B61.2
76.6
-
86.7
CodeParro-1.5B
1.29
8.66
-
27.17
StarCoder-Python-15B
52.7
-
-
-
Code-LLaMA-Python-7B
47.6
70.3
-
84.8
StarCoder-Prompted-15.5B
49.5
-
-
-
Code-LLaMA-Python-34B 56.2
76.4
-
88.2
StarCoderBase-15B
49.0
-
-
-
Code-LLaMA-Python-13B 49
74
-
87.6
StarCoder-5.5B
52.7
-
-
-
Code-LLaMA-Instruct-7B 44.4
65.4
-
76.8
SantaCoder-1.1B
3.65
21.33
-
41.92
Code-LLaMA-Instruct-34B57
74.6
-
85.4
SantaCoder-1.1B
35
-
-
-
Code-LLaMA-Instruct-13B49.4
71.2
-
84.1
PyCodeGPT-110M
9.39
28.37
-
48.71
Code-LLaMA-7B
41.4
66.7
-
82.5
PolyCoder-400M
1.31
7.98
-
21.55
Code-LLaMA-34B
55
76.2
-
86.6
PolyCoder-2.7B
4.39
17.99
-
38.17
Code-LLaMA-13B
47
71.7
-
87.1
PolyCoder-160M
1.08
6.67
-
18.97
CodeGen-NL 6.1B
8.15
31.21
-
55.27
phi-1-1.3B
55.5
-
-
-
CodeGen-NL 350M
0.96
6.37
-
19.91
PaLM-Coder-540B
47
-
80.8
-
CodeGen-NL 2.7B
5.34
24.63
-
48.95
PaLMCoder-540B
47
-
80.8
-
CodeGen-NL 16.1B
10.92
38.43
-
62.76
PaLM-540B
36.8
-
75
-
CodeGen-Multi-16B
20.9
-
-
-
PaLM-2-S
50
-
-
-
CodeGen-Multi-6.1B
18.35
47.27
-
67.92
LLaMA-7B
17.7
-
-
-
CodeGen-Multi-350M
7.46
24.18
-
46.37
LLaMA-65B
37.7
-
-
-
CodeGen-Multi-2.7B
18.06
45.80
-
65.34
LLaMA-33B
30.2
-
-
-
CodeGen-Multi-16.1B
20.94
51.61
-
70.02
LLaMA2-7B
20.8
41.8
-
65.5
CodeGen-Mono-6.1B
33.70
62.70
-
70.25
LLaMA2-70B
45.4
66.2
-
83.1
CodeGen-Mono-350M
15.44
42.50
-
64.40
LLaMA2-34B
33.8
56.9
-
77.6
CodeGen-Mono-350M
-
-
-
-
LLaMA2-13B
27.6
48.1
-
69.5
CodeGen-Mono-2.7B
28.80
60.73
-
75.41
LLaMA-13B
22
-
-
-
CodeGen-Mono-16B
35.3
-
-
-
LaMDA-137B
14.8
-
62.4
-
CodeGen-Mono-16.1B
35.28
67.32
-
80.09
JuPuT5-300M
-
-
52.2
-
CodeGen-Mono-16.1B
35.3
-
-
-
InstructCodeT5+-16B
-
-
-
-
CODEGEN-Mono-6.1B
32.48
64.20
-
76.81
InCoder-6.7B
19.4
-
-
-
CODEGEN-Mono-350M
14.59
41.49
-
63.00
InCoder-6.7B
21.3
46.5
-
66.2
CODEGEN-Mono-2.7B
27.31
59.19
-
74.24
InCoder-1.3B
10.00
34.02
-
55.50
CODEGEN-Mono-16.1B
35.28
67.32
-
80.09
InCoder-6B
21.30
46.50
-
66.20
CodeGen2-1B
-
-
-
-
GPT-Neo-2.7B
5.89
23.09
-
44.26
CodeGeeX-13B
24.4
48
68.5
-
GPT-Neo-125M
0.26
2.15
-
7.96
code-davinci-002
58.10
76.70
-
84.50
GPT-Neo-1.3B
3.77
16.26
-
29.51
code-davinci-001
51.80
72.80
-
84.10
GPT-J-6B
11.30
35.62
-
53.63
code-cushman-001
45.90
66.90
-
79.90
GPT-4
-
-
-
-
CodcGen2-7B
-
-
-
-
GPT-3.5
-
-
-
-
BLOOM-7.1B
1.01
7.91
-
24.12
GPT-3.5-turbo
52.2
-
-
-
BLOOM-560M
0.26
2.04
-
8.90
CodeT5-770M
15.78
38.63
50.35
BLOOM-3B
2.25
13.58
-
32.08
CodeT5+-2B
-
-
-
-
BLOOM-1.7B
3.16
14.23
-
31.38
CodeT5+-16B
-
-
-
-
BLOOM-1.1B
1.90
9.20
-
23.42
Table 21. Data in the literature that differ for MBPP.
LLMs
MBPP
Pass@1
Pass@10
Pass@80
Pass@100
InCoder-6.7B [105]
19.4
-
-
-
InCoder-6.7B [67]
21.3
46.5
-
66.2
CodeGen-Mono-16.1B [67]
35.28
67.32
-
80.09
CodeGen-Mono-16.1B [50]
35.3
-
-
-
prompts. However, the evaluation of LLM performance indicates that the current state of the
technology is not sufficient to deliver true value in the context of program repair frameworks.
Xia et al. [101] evaluates LLMs used for direct program repair. The article reveals the scaling
effects of increasing model size on various crucial factors in APR, such as the number of fixed
bugs, patch generation speed, and compilation rate. LLMs are also tested on widely used APR
A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends
33
Table 22. Number of samples generated per minute for different PLMs on Defects4J 1.2 and QuixBugs with
the 3 repair generation settings in [101].
Tools / Models
Single func (255 bugs)
Patch func
Correct hunk
Single line
AlphaRepair
67
-
-
-
RewardRepair
48
-
-
-
Recoder
61
-
-
-
TBar
54
-
-
-
CURE
52
-
-
-
GPT-Neo 125M
9
6
-
5
GPT-Neo 1.3B
18
7
-
12
GPT-Neo 2.7B
20
10
-
13
GPT-J
28
14
-
16
GPT-NeoX
34
18
-
21
CodeT5
6
-
6
-
INCODER 1.3B
32
-
32
-
INCODER 6.7B
37
-
37
-
Codex
99
63
62
32
Total
109
69
74
40
Table 23. Performance on Defects4J 2.0, QuixBugs-Java and -Python in [101]
Tools / Models
Defects4J 2.0 (78 bugs)
QuixBugs Java (40 bugs)
QuixBugs Python (40 bugs)
AlphaRepair
35
28
27
RewardRepair
25
20
-
DeepDebug
-
-
21
Recoder
11
17
-
CURE
-
21
-
TBar
8
-
-
CoCoNuT
-
13
19
GPT-Neo 125M
10
8
9
GPT-Neo 1.3B
11
20
17
GPT-Neo 2.7B
19
18
24
GPT-J
16
22
29
GPT-NeoX
24
21
31
CodeT5
9
10
7
INCODER 1.3B
15
21
25
INCODER 6.7B
21
26
27
Codex
45
38
40
Total
52
38
40
benchmarks, resulting in Table 22 and Table 23 (Columns CF, CI, SL refer to complete function,
correct infilling and single line generation, respectively). It can be observed that many models
achieve similar (or even better) performance through carefully designed APR tools. Additionally, in
the Defects4J 2.0, QuixBugs-Java, and QuixBugs-Python benchmarks, all nine LLMs outperform
TBar (state-of-the-art template-based APR tool). Among these nine LLMs, Codex demonstrates the
best performance, followed by InCoder-6.7B and GPT-NeoX.
In summary, based on the limited available results, Codex demonstrates better performance in
the task of vulnerability repair. However, the information available for this task is still limited, and
there is a lack of performance comparison between state-of-the-art (SOTA) Code LLMs and GPT-4.
34
Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen
Table 24. Average solved rate (%) for each type of problem in [107].
LLM
Defect Detection (%) Clone Detection (%) Assert Generation (%)
Code Summarization (%)
Zero-shot
One-shot
Zero-shot
One-shot
Zero-shot
One-shot
Zero-shot
One-shot
CodeGen-6B
0.3
43.6
1.4
23.4
0.0
56.2
0.0
13.0
ChatGLM-6B
7.1
54.2
17.5
12.8
1.7
46.2
45.0
54.0
Vicuna-7B
54.0
54.1
13.2
-
10.1
31.2
48.0
37.0
Alpaca-7B
45.8
55.4
22.1
-
5.3
41.4
32.0
6.0
Dolly-7B
33.1
49.9
21.3
23.5
1.9
51.0
12.0
14.0
StableLM-7B
44.3
43.4
24.3
-
1.1
44.4
30.0
19.0
CodeAlpaca-7B
51.9
50.3
1.4
10.3
4.4
35.1
9.0
34.0
Dolly-12B
33.8
52.7
23.5
22.6
1.0
51.7
5.0
8.0
Vicuna-13B
49.8
53.0
14.1
6.5
12.0
44.0
63.0
24.0
WizardCoder-15B
54.4
53.8
23.8
7.3
19.4
63.3
71.0
50.0
Instruct-CodeGen-16B
47.8
54.6
14.2
20.7
8.4
55.0
9.0
41.0
5.6
Other Evaluation or New Benchmarks
Yuan et al. [107] provide a detailed evaluation of 10 open-source guided LLMs on four representative
code understanding and generation tasks: defect detection, clone detection, assertion generation,
and code summarization. While the main focus of the article is on the impact of instruction fine-
tuning on Code LLMs, it also provides valuable insights. Table 24 showcases the performance of
instruction-tuned LLMs on software engineering tasks under zero-shot and one-shot settings. We
can observe that WizardCoder-15B performs relatively well, particularly in the assertion generation
task. The paper also presents several interesting findings: (1) For zero-shot settings, guided LLMs
sometimes outperform small-scale SOTA models fine-tuned specifically for each downstream task
in code understanding and generation tasks. (2) For few-shot settings, the addition of demonstration
examples can significantly improve the performance of guided LLMs on most code understanding
and generation tasks. (3) For fine-tuning settings, further performance enhancement on downstream
code understanding and generation tasks can be achieved through fine-tuning.
Zan et al. [110] conducted a comprehensive investigation of Code LLMs on 27 existing LLMs and
reviewed widely used benchmarks and metrics, as shown in Table 25. In the table, P.NL represents the
Problem description’s Natural Language, S.PL denotes the code Solution’s Programming Language,
and T.N. denotes the average Number of Test cases. P.C. and P.L. (S.C. and S.L.) stand for the average
number of Characters and Lines in the Problem description (code Solution), respectively.
Athiwaratkun et al. [6] introduce new benchmarks for evaluating code generation models:
MBXP, Multilingual HumanEval, and MathQA-X. The article provides a detailed comparison of the
performance of the CodeGen, OPT, and BLOOM models in multilingual code generation and code
translation tasks. In each test, CodeGen-Mono-16B achieves higher scores. The article also highlights
some important findings: (1) Given the same model size, multilingual models generally outperform
the best monolingual models trained with equivalent training resources, especially when the
model is large enough. This observation suggests that training a single model on all programming
languages is beneficial, and as long as the model has sufficient capacity, its performance will
surpass the best monolingual models. (2) LLMs have the potential to learn from their uncurated
programming languages through unit tests. (3) Few-shot prompts can effectively help the model
acquire knowledge of new languages not seen during training, significantly improving out-of-
domain code generation capabilities. Through error analysis, it is observed that fewer prompts
help reduce compilation or parsing errors, which are the main source of errors when dealing with
programming languages the model is unfamiliar with. (4) Language models possess zero-shot code
A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends
35
Table 25. Average solved rate (%) for each type of problem formatting in [110]. The asterisk (*) indicates the
number of instances per programming language.
Benchmark
Number of instances
P.NL
S.PL
Data Statistics
Scenario
T.N.
P.C.
P.L.
S.C.
S.L.
HumanEval
164
English
Python
7.8
450.6
13.7
180.9
6.8
Code Exercise
MBPP
974
English
Python
3.1
78.6
1.0
181.1
6.7
Code Exercise
APPS
5,000
English
Python
21.0
1743.4
41.6
473.8
21.4
Competitions
CodeContests
165
English
Multi.
203.7
1989.2
66.4
2239.3
92.1
Competitions
DS-1000
1,000
English
Python
1.6
879.1
31.6
137.4
5.0
Data Science
DSP
1,119
English
Python
2.1
756.9
17.8
226.3
7.6
Data Science
MBXP
974∗
English
Multi.
3.1
419.9
14.8
-
-
Multilingual
MBXP-HumanEval
164∗
English
Multi.
7.8
825.6
30.0
-
-
Multilingual
HumanEval-X
164∗
English
Multi.
7.8
468.4
15.5
264.6
12.1
Multilingual
MultiPL-HumanEval
164∗
English
Multi.
7.8
453.9
13.0
-
-
Multilingual
MultiPL-MBPP
974∗
English
Multi.
3.1
181.2
5.4
-
-
Multilingual
PandasEval
101
English
Python
6.5
244.5
7.2
46.2
1.3
Public Library
NumpyEval
101
English
Python
3.5
222.9
7.0
29.9
1.1
Public Library
TorchDataEval
50
English
Python
1.1
329.0
8.6
50.7
1.3
Private Library
MTPB
115
English
Python
-
72.7
1.0
-
-
Multi-Turn
ODEX 2022 c
945
Multi.
Python
1.8
26.6
2.0
50.4
1.9
Open-Domain
BIG-Bench
32
English
Python
4.7
341.8
3.0
-
-
Code Exercise
translation capabilities, and this translation ability extends to monolingual models. (5) Multilingual
models are more robust to prompt perturbations and can better summarize code.
Tang et al. [90] introduce BIOCODER, a benchmark for evaluating LLMs’ ability to generate
bioinformatics code. The article presents the performance of InCoder, CodeGen, CodeGen2, Santa-
Coder, StarCoder, StarCoder+, InstructCodeT5+, and ChatGPT on BIOCODER, as shown in Table 26.
Notably, StarCoder+ is the result of fine-tuning StarCoder on Java for 2000 steps, while all others
are zero-shot results. It can be observed that ChatGPT performs the best on all tasks. The article
also highlights an interesting phenomenon: despite InstructCodeT5+, CodeGen, and CodeGen2
having larger parameter sizes than InCoder and SantaCoder, their performance is significantly
worse. The authors attribute this to InstructCodeT5+, CodeGen, and CodeGen2 being trained on
single-line completions rather than function completions. Additionally, InstructCodeT5+, CodeGen,
and CodeGen2 have relatively smaller context constraints. The authors further note that context
constraints have a significant impact on how different models perform under different prompts.
Kou et al. [43] evaluated the differences between LLMs and human model attention in pro-
gramming based on keyword coverage and Cohen’s Kappa consistency level. The article shows
that among the five models evaluated, CodeGen exhibited the highest consistency with human
programmers. However, the results presented in the article are limited, and there was no testing
conducted on state-of-the-art (SOTA) models.
In conclusion, we can summarize the following points:
• The current evaluation of LLMs focuses more on code generation tasks, with less emphasis on
evaluating or researching other tasks such as vulnerability repair. There is a lack of relevant
evaluation work, and when new models are released, there is limited attention given to tasks
like vulnerability repair.
• Code generation tasks have well-known benchmarks like HumanEval. However, other tasks
lack such widely recognized benchmarks.
36
Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen
Table 26. Performance with five prompt versions of BIOCODER [90].
Model
Prompt
Java
Python
Pass@1 Pass@5 5ass@10 Pass@20 Pass@1 Pass@5 Pass@10 Pass@20
InCoder-6B
Summary at Top
0
0
0
0
0.828
2.016
3.006
4.459
Uncommented
0
0
0
0
0.032
0.159
0.318
0.637
Summary Only
0
0
0
0
1.688
5.320
8.332
12.006
Summary at Bottom -
-
-
-
0.610
2.587
4.303
6.274
Necessary Only
0
0
0
0
0.032
0.159
0.318
0.637
SantaCoder-1.1B
Summary at Top
0
0
0
0
0.637
1.338
1.844
2.548
Uncommented
0
0
0
0
0.287
0.764
0.955
1.274
Summary Only
0
0
0
0
2.965
9.848
14.227
18.181
Summary at Bottom -
-
-
-
0.510
1.949
3.013
4.459
Necessary Only
0
0
0
0
0.032
0.159
0.318
0.637
StarCoder-15.5B
Summary at Top
0
0
0
0
3.694
13.197
19.359
24.554
Uncommented
0
0
0
0
0.318
1.062
1.591
2.548
Summary Only
0
0
0
0
4.682
15.225
21.200
27.166
Summary at Bottom -
-
-
-
6.465
13.824
16.746
19.076
Necessary Only
0
0
0
0
0.127
0.603
1.123
1.911
StarCoder-15.5B
(finetuned)
Summary at top
0
0
0
0
-
-
-
-
Uncommented
0
0
0
0
-
-
-
-
Summary Only
0.200
1.000
2.000
4.000
-
-
-
-
Summary at bottom -
-
-
-
-
-
-
-
Necessary Only
3.300
12.097
19.545
30.000
-
-
-
-
StarCoder+
Summary at Top
0
0
0
0
2.675
9.133
14.019
19.650
Uncommented
0
0
0
0
0.510
0.955
1.274
1.911
Summary Only
1.300
5.031
8.042
12.000
2.548
8.279
12.864
18.057
Summary at Bottom -
-
-
-
4.172
11.772
14.933
17.197
Necessary Only
0
0
0
0
0.127
0.457
0.609
0.637
InstructCodeT5+
All prompt types
0
0
0
0
0
0
0
0
CodeGen-6B-mono
Summary at Top
0
0
0
0
0.637
0.637
0.637
0.637
Uncommented
0
0
0
0
0
0
0
0
Summary Only
0
0
0
0
0.637
0.637
0.637
0.637
Summary at Bottom -
-
-
-
2.070
4.535
5.896
7.006
Necessary Only
0
0
0
0
0
0
0
0
CodeGen-16B-mono
Summary at Top
0
0
0
0
0.637
0.637
0.637
0.637
Uncommented
0
0
0
0
0
0
0
0
Summary Only
0
0
0
0
0.637
0.637
0.637
0.637
Summary at Bottom -
-
-
-
2.166
5.137
6.022
6.369
Necessary Only
0
0
0
0
0
0
0
0
CodeGen2-7B
Summary at Top
0
0
0
0
0.637
0.637
0.637
0.637
Uncommented
0
0
0
0
0.510
0.637
0.637
0.637
Summary Only
0
0
0
0
0.860
2.494
3.962
6.242
Summary at Bottom -
-
-
-
0.510
1.019
1.207
1.274
Necessary Only
0
0
0
0
0
0
0
0
GPT-3.5-Turbo
Summary at Top
4.100
7.235
8.989
11.600
22.771
33.461
36.551
39.490
Uncommented
6.300
11.563
14.436
18.000
11.019
19.075
21.680
24.204
Summary Only
17.400
33.199
37.878
42.000
24.682
33.997
37.132
40.127
Summary at Bottom -
-
-
-
13.439
20.040
22.460
25.478
Necessary Only
43.500
52.582
53.995
55.400
28.758
39.529
44.029
47.771
A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends
37
• In code generation tasks, the Code-LLaMA series of LLMs perform the best, especially
with Unnatural-Code-LLaMA-34B showing outstanding performance. In API-related code
generation tasks, ToolCoder performs better. GPT-4 and GPT-3.5 (GPT-3.5-turbo) also exhibit
good performance in code generation.
• For test case generation tasks, GPT-4 and GPT-3.5 (GPT-3.5-turbo) demonstrate better perfor-
mance.
• In code summarization tasks, CodeT5+ outperforms GPT-3.5 (GPT-3.5-turbo).
• In code translation tasks, GPT-4 performs better.
• For vulnerability repair tasks, based on limited results, Codex shows better performance.
However it should be noted that, except for the relatively accurate results in code generation
tasks, the results in other tasks are not precise enough. For example, in tasks such as code translation
and code summarization, there is a lack of comparative evaluation work between SOTA Code LLMs
such as Unnatural-Code-LLaMA-34B and GPT-4. Furthermore, apart from code generation, we also
lack relevant benchmarks to measure the differences in capabilities among various models in other
software engineering tasks. An interesting thing to note is that different literature records vary in
terms of LLMs’ performance in HumanEval, especially for ChatGPT and GPT-3.5 models, and these
inconsistencies are not simply due to errors in quoting. The reasons for this occurrence are not yet
clear, and it is worth discussing.
6
RELATED WORK
Artetxe et al. [5] conducted a comprehensive analysis and summary of 27 code-based large models
released before December 2022. These 27 models were tested and evaluated on the HumanEval
benchmark using a zero-shot approach to provide intuitive comparative results. The article identifies
the key factors for the success of code-based large models as model parameters, data quality,
and expert tuning. However, there are still many open challenges in code testing benchmarks
for large language models (LLMs). For example, most of these benchmarks only have problem
descriptions in English and Python code solutions, which cannot cover multiple natural languages
and programming languages. The article suggests that current code-based large models still face
challenges in terms of comprehension ability, inference ability, explanatory ability, adaptive learning
ability, and multitasking ability.
Zheng et al. [118] discuss the applications of LLMs in the field of software engineering. The
article organizes and categorizes 123 selected works and literature on the intersection of software
engineering and LLMs. It classifies them according to software engineering tasks, revealing the
research focuses and potential directions for combining various software engineering tasks with
LLMs. Additionally, the article reveals the performance of LLMs in these tasks, along with their
limitations, and provides directions for future research and optimization.
Some efforts aim to enhance the capabilities of existing LLMs in software engineering tasks. Gong
et al. [29] introduces CODETF, an open-source library based on transformers for code LLMs and
code intelligence. CODETF is designed with a unified interface to enable fast access and development
across different types of models, datasets, and tasks. Strictly speaking, CODETF is not a conventional
LLM in the traditional sense but more of a methodology. Lu et al. [55] addresses the phenomenon of
training code LLMs on large, uncleaned source code corpora scraped from the internet. The paper
discusses the security, privacy, and licensing issues associated with generated LLMs and provides
four feasible recommendations to address these concerns. Le et al. [45] presents a Cross-language
Code representation with a large-scale pre-training (XCode) method, which utilizes several abstract
syntax trees and ELMo-enhanced variational autoencoders to train multiple pre-trained source
code language models on approximately 1.5 million code snippets. Maddigan et al. [60] enhances
38
Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen
the robustness of existing pre-trained models by designing nine PL-NL enhancement operators to
group semantically equivalent variants.
Some works have utilized the capabilities of LLMs to design automated tools or frameworks.
Gao et al. [28] introduces CollabCoder, a system that supports users in completing qualitative
coding through multiple stages using LLMs. Tanaka et al. [89] proposes a new learning approach
called Inductive Bias Learning (IBL), which combines Inductive Concept Learning (ICL) and code
generation techniques. The article suggests that the prediction models generated by IBL have the
potential to replace existing machine learning models in terms of interpretability and inference
speed. Das et al. [22] addresses the cumbersome and time-consuming process of generating and
integrating code views for each programming language and presents a tool called COMEX. COMEX
allows researchers and developers to create and harvest multiple code views that can be used by
LLMs for various software engineering tasks.
Karmakar et al. [39] evaluated the code synthesis ability of the Codex model using a set of
115 Python problem statements from HackerRank and found clear evidence of Codex’s ability
to generate code from memory. Ding et al. [23] proposed a static evaluation framework for code
completion generated by large language models and performed error analysis on the CodeGen model
using a large-scale real-world Python evaluation set. Martínez et al. [61] explored the application
of law graduates in code detection and evaluated the code detection capability of GPT-3.5 using
matrix multiplication. N41 conducted an empirical study to assess ChatGPT’s unit test generation
capability and proposed a method called CHATTESTER, which utilizes ChatGPT itself to improve
the quality of generated tests. The results showed that GPT-3.5 achieved an accuracy rate close
to 100% in the evaluation. Furthermore, there are several related works that have developed new
benchmarks for testing the code generation capabilities of LLMs, such as CODETASKCL [104],
CodeBLEU [75], CodeSearchNet [36], and Galeras [77].
7
CONCLUSION
After a comprehensive review, this paper explores the performance and value of specialized LLMs
in the field of software engineering. Firstly, we collected and screened 134 works related to Code
LLMs. Next, we organized Code LLMs based on the types of institutions to which their main
developers belong, revealing the relationships between Code LLMs, general LLMs, and among
Code LLMs themselves. Furthermore, we conducted a comprehensive analysis and compilation
of the performance of general LLMs and Code LLMs in software engineering tasks. We provided
statistical results and analyzed interesting phenomena. Lastly, we maintained the scores of 126
Code LLMs on major benchmarks and conducted a detailed analysis of their performance across
different software engineering tasks. The contribution of this paper lies in the comprehensive
overview of Code LLMs and their performance. This work can assist Code LLM developers in
making informed decisions regarding base models and fine-tuning approaches, and it also provides
key improvement directions for Code LLMs.
REFERENCES
[1] Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021. Unified Pre-training for Program
Understanding and Generation. In Proceedings of the 2021 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021. Association
for Computational Linguistics, 2655–2668. https://doi.org/10.18653/v1/2021.naacl-main.211
[2] Toufique Ahmed and Premkumar T. Devanbu. 2022. Few-shot training LLMs for project-specific code-summarization.
In 37th IEEE/ACM International Conference on Automated Software Engineering, ASE 2022, Rochester, MI, USA, October
10-14, 2022. ACM.
[3] Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Muñoz Ferrandis, Niklas
Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian
A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends
39
Zi, Joel Lamy-Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel Romero, Michael Lappert,
Francesco De Toni, Bernardo García del Río, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu,
Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia Li,
Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried, Arjun Guha, Harm de Vries, and Leandro von Werra.
2023. SantaCoder: don’t reach for the stars! CoRR abs/2301.03988 (2023). https://doi.org/10.48550/arXiv.2301.03988
arXiv:2301.03988
[4] Miltiadis Allamanis and Charles Sutton. 2013. Mining source code repositories at massive scale using language
modeling. In Proceedings of the 10th Working Conference on Mining Software Repositories, MSR ’13, San Francisco,
CA, USA, May 18-19, 2013, Thomas Zimmermann, Massimiliano Di Penta, and Sunghun Kim (Eds.). IEEE Computer
Society, 207–216. https://doi.org/10.1109/MSR.2013.6624029
[5] Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, et al. 2022. Efficient Large Scale Language
Modeling with Mixtures of Experts. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language
Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022. Association for Computational
Linguistics, 11699–11732. https://aclanthology.org/2022.emnlp-main.804
[6] Ben Athiwaratkun, Sanjay Krishna Gouda, Zijian Wang, Xiaopeng Li, et al. 2023. Multi-lingual Evaluation of Code
Generation Models. arXiv:2210.14868 [cs.LG]
[7] Mohammad Bavarian, Heewoo Jun, Nikolas Tezak, John Schulman, Christine McLeavey, Jerry Tworek, and Mark
Chen. 2022. Efficient Training of Language Models to Fill in the Middle. CoRR abs/2207.14255 (2022).
https:
//doi.org/10.48550/arXiv.2207.14255
[8] Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, et al. 2022. GPT-NeoX-20B: An Open-Source
Autoregressive Language Model. In Proceedings of the ACL Workshop on Challenges & Perspectives in Creating Large
Language Models. https://arxiv.org/abs/2204.06745
[9] Nghi D. Q. Bui, Hung Le, Yue Wang, Junnan Li, Akhilesh Deepak Gotmare, and Steven C. H. Hoi. 2023. CodeTF:
One-stop Transformer Library for State-of-the-art Code LLM. arXiv:2306.00029 [cs.SE]
[10] Federico Cassano, John Gouwar, Francesca Lucchetti, Claire Schlesinger, Carolyn Jane Anderson, Michael Greenberg,
Abhinav Jangda, and Arjun Guha. 2023. Knowledge Transfer from High-Resource to Low-Resource Programming
Languages for Code LLMs. arXiv:2308.09895 [cs.PL]
[11] Aaron Chan, Anant Kharkar, Roshanak Zilouchian Moghaddam, Yevhen Mohylevskyy, Alec Helyar, Eslam Kamal,
Mohamed Elkamhawy, and Neel Sundaresan. 2023. Transformer-based Vulnerability Detection in Code at EditTime:
Zero-shot, Few-shot, or Fine-tuning? arXiv:2306.01754 [cs.CR]
[12] Shubham Chandel, Colin B. Clement, Guillermo Serrato, and Neel Sundaresan. 2022. Training and Evaluating a Jupyter
Notebook Data Science Assistant. CoRR abs/2201.12901 (2022). arXiv:2201.12901 https://arxiv.org/abs/2201.12901
[13] Sahil Chaudhary. 2023. Code Alpaca: An Instruction-following LLaMA model for code generation. https://github.
com/sahil280114/codealpaca.
[14] Jiachi Chen, Xin Xia, David Lo, John Grundy, and Xiaohu Yang. 2021. Maintenance-related concerns for post-deployed
Ethereum smart contract development: issues, techniques, and future challenges. Empirical Software Engineering 26,
6 (2021), 1–44.
[15] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards,
Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv
preprint arXiv:2107.03374 (2021).
[16] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards,
Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv
preprint arXiv:2107.03374 (2021).
[17] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards,
Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv
preprint arXiv:2107.03374 (2021).
[18] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, et al. 2022. PaLM: Scaling Language Modeling
with Pathways. CoRR abs/2204.02311 (2022). https://doi.org/10.48550/arXiv.2204.02311 arXiv:2204.02311
[19] Fenia Christopoulou, Gerasimos Lampouras, Milan Gritta, Guchun Zhang, Yinpeng Guo, Zhongqi Li, Qi Zhang, Meng
Xiao, Bo Shen, Lin Li, et al. 2022. Pangu-coder: Program synthesis with function-level language modeling. arXiv
preprint arXiv:2207.11280 (2022).
[20] Colin B. Clement, Dawn Drain, Jonathan Timcheck, Alexey Svyatkovskiy, and Neel Sundaresan. 2020. PyMT5:
multi-mode translation of natural language and Python code with transformers. In Proceedings of the 2020 Conference
on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, Bonnie Webber,
Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, 9052–9065. https://doi.org/
10.18653/v1/2020.emnlp-main.728
[21] CodedotAl. 2021. GPT-Code-Clippy. https://github.com/CodedotAl/gpt-code-clippy
40
Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen
[22] Debeshee Das, Noble Saji Mathews, Alex Mathai, Srikanth Tamilselvam, Kranthi Sedamaki, Sridhar Chimalakonda, and
Atul Kumar. 2023. COMEX: A Tool for Generating Customized Source Code Representations. arXiv:2307.04693 [cs.SE]
[23] Hantian Ding, Varun Kumar, Yuchen Tian, Zijian Wang, Rob Kwiatkowski, Xiaopeng Li, Murali Krishna Ramanathan,
Baishakhi Ray, Parminder Bhatia, Sudipta Sengupta, Dan Roth, and Bing Xiang. 2023. A Static Evaluation of Code
Completion by Large Language Models. arXiv:2306.03203 [cs.CL]
[24] Tuan Dinh, Jinman Zhao, Samson Tan, Renato Negrinho, Leonard Lausen, Sheng Zha, and George Karypis. 2023.
Large Language Models of Code Fail at Completing Code with Potential Bugs. arXiv:2306.03438 [cs.LG]
[25] Xueying Du, Mingwei Liu, Kaixin Wang, Hanlin Wang, Junwei Liu, Yixuan Chen, Jiayi Feng, Chaofeng Sha, Xin
Peng, and Yiling Lou. 2023. ClassEval: A Manually-Crafted Benchmark for Evaluating LLMs on Class-level Code
Generation. arXiv:2308.01861 [cs.CL]
[26] Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Scott Yih, Luke
Zettlemoyer, and Mike Lewis. 2023. InCoder: A Generative Model for Code Infilling and Synthesis. In The Eleventh
International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.
https://openreview.net/pdf?id=hQwb-lbM6EL
[27] Lingyue Fu, Huacan Chai, Shuang Luo, Kounianhua Du, Weiming Zhang, Longteng Fan, Jiayi Lei, Renting Rui,
Jianghao Lin, Yuchen Fang, Yifan Liu, Jingkuan Wang, Siyuan Qi, Kangning Zhang, Weinan Zhang, and Yong Yu. 2023.
CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models. arXiv:2309.01940 [cs.CL]
[28] Jie Gao, Yuchen Guo, Gionnieve Lim, Tianqin Zhang, Zheng Zhang, Toby Jia-Jun Li, and Simon Tangi Perrault. 2023.
CollabCoder: A GPT-Powered Workflow for Collaborative Qualitative Analysis. arXiv:2304.07366 [cs.HC]
[29] Zi Gong, Yinpeng Guo, Pingyi Zhou, Cuiyun Gao, Yasheng Wang, and Zenglin Xu. 2022. MultiCoder: Multi-
Programming-Lingual Pre-Training for Low-Resource Code Completion. arXiv:2212.09666 [cs.CL]
[30] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Ré. 2020. HiPPO: Recurrent Memory with Optimal
Polynomial Projections. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural
Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, Hugo Larochelle, Marc’Aurelio
Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (Eds.).
[31] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, et al. 2023.
Textbooks Are All You Need. CoRR abs/2306.11644 (2023). https://doi.org/10.48550/arXiv.2306.11644 arXiv:2306.11644
[32] Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir
Puranik, Horace He, Dawn Song, and Jacob Steinhardt. 2021. Measuring Coding Challenge Competence With APPS.
NeurIPS (2021).
[33] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long Short-Term Memory. Neural Comput. 9, 8 (1997), 1735–1780.
https://doi.org/10.1162/neco.1997.9.8.1735
[34] Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo, David Lo, John Grundy, and Haoyu
Wang. 2023. Large Language Models for Software Engineering: A Systematic Literature Review. arXiv preprint
arXiv:2308.10620 (2023).
[35] Huaggingface. 2021. Training CodeParrot from Scratch. https://huggingface.co/blog/codeparrot.
[36] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. 2019. CodeSearchNet
Challenge: Evaluating the State of Semantic Code Search. CoRR abs/1909.09436 (2019). arXiv:1909.09436 http:
//arxiv.org/abs/1909.09436
[37] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016. Summarizing Source Code using a
Neural Attention Model. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics,
ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers. The Association for Computer Linguistics.
https://doi.org/10.18653/v1/p16-1195
[38] Xue Jiang, Yihong Dong, Lecheng Wang, Zheng Fang, Qiwei Shang, Ge Li, Zhi Jin, and Wenpin Jiao. 2023. Self-planning
Code Generation with Large Language Models. arXiv:2303.06689 [cs.SE]
[39] Anjan Karmakar, Julian Aron Prenner, Marco D’Ambros, and Romain Robbes. 2022. Codex Hacks HackerRank:
Memorization Issues and a Framework for Code Synthesis Evaluation. arXiv:2212.02684 [cs.SE]
[40] Mohammad Abdullah Matin Khan, M Saiful Bari, Xuan Long Do, Weishi Wang, Md Rizwan Parvez, and Shafiq Joty.
2023. xCodeEval: A Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation
and Retrieval. arXiv:2303.03004 [cs.CL]
[41] B. A. Kitchenham. 2007. Kitchenham, B.: Guidelines for performing Systematic Literature Reviews in software
engineering. EBSE Technical Report EBSE-2007-01. IEEE Computer Society (2007).
[42] Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, et al. 2022. The Stack: 3 TB of permissively
licensed source code. CoRR abs/2211.15533 (2022). https://doi.org/10.48550/arXiv.2211.15533 arXiv:2211.15533
[43] Bonan Kou, Shengmai Chen, Zhijie Wang, Lei Ma, and Tianyi Zhang. 2023. Is Model Attention Aligned with Human
Attention? An Empirical Study on Large Language Models for Code Generation. arXiv:2306.01220 [cs.SE]
A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends
41
[44] Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Scott Wen tau Yih, Daniel
Fried, Sida Wang, and Tao Yu. 2022. DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation.
arXiv:2211.11501 [cs.SE]
[45] Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Chu-Hong Hoi. 2022. CodeRL: Mastering
Code Generation through Pretrained Models and Deep Reinforcement Learning. In NeurIPS. http://papers.nips.cc
[46] Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, et al. 2021.
Datasets: A Community Library for Natural Language Processing. In Proceedings of the 2021 Conference on Empirical
Methods in Natural Language Processing: System Demonstrations, EMNLP 2021, Online and Punta Cana, Dominican
Republic, 7-11 November, 2021, Heike Adel and Shuming Shi (Eds.). Association for Computational Linguistics, 175–184.
https://doi.org/10.18653/v1/2021.emnlp-demo.21
[47] Jinyang Li, Binyuan Hui, Ge Qu, Binhua Li, Jiaxi Yang, Bowen Li, Bailin Wang, Bowen Qin, Rongyu Cao, Ruiying Geng,
Nan Huo, Xuanhe Zhou, Chenhao Ma, Guoliang Li, Kevin C. C. Chang, Fei Huang, Reynold Cheng, and Yongbin Li.
2023. Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs.
arXiv:2305.03111 [cs.CL]
[48] Jia Li, Ge Li, Yongmin Li, and Zhi Jin. 2023.
Structured Chain-of-Thought Prompting for Code Generation.
arXiv:2305.06599 [cs.SE]
[49] Peng Li, Tianxiang Sun, Qiong Tang, Hang Yan, Yuanbin Wu, Xuanjing Huang, and Xipeng Qiu. 2023. CodeIE: Large
Code Generation Models are Better Few-Shot Information Extractors. arXiv:2305.05711 [cs.CL]
[50] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone,
Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier
Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel
Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang,
Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang,
Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov,
Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan
Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish
Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis, Sean Hughes,
Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. 2023. StarCoder: may the source be with you!
arXiv:2305.06161 [cs.CL]
[51] Tsz-On Li, Wenxi Zong, Yibo Wang, Haoye Tian, Ying Wang, Shing-Chi Cheung, and Jeff Kramer. 2023. Finding
Failure-Inducing Test Cases with ChatGPT. arXiv:2304.11686 [cs.SE]
[52] Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. 2023. Textbooks
Are All You Need II: phi-1.5 technical report. arXiv:2309.05463 [cs.CL]
[53] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, et al. 2022.
Competition-level
code generation with AlphaCode. Science 378, 6624 (2022), 1092–1097.
https://doi.org/10.1126/science.abq1158
arXiv:https://www.science.org/doi/pdf/10.1126/science.abq1158
[54] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer,
and Veselin Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. CoRR abs/1907.11692
(2019). arXiv:1907.11692 http://arxiv.org/abs/1907.11692
[55] Junyi Lu, Lei Yu, Xiaojia Li, Li Yang, and Chun Zuo. 2023. LLaMA-Reviewer: Advancing Code Review Automation
with Large Language Models through Parameter-Efficient Fine-Tuning. arXiv:2308.11148
[56] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, et al. 2021. CodeXGLUE: A Machine Learning Benchmark Dataset for
Code Understanding and Generation. In Proceedings of the Neural Information Processing Systems Track on Datasets
and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, Joaquin Vanschoren and Sai-Kit
Yeung (Eds.). https://datasets-benchmarks-proceedings.neurips.cc
[57] Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and
Daxin Jiang. 2023. WizardCoder: Empowering Code Large Language Models with Evol-Instruct. CoRR abs/2306.08568
(2023). https://doi.org/10.48550/arXiv.2306.08568 arXiv:2306.08568
[58] Thang Luong, Hieu Pham, and Christopher D. Manning. 2015. Effective Approaches to Attention-based Neural
Machine Translation. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing.
Association for Computational Linguistics, Lisbon, Portugal, 1412–1421. https://doi.org/10.18653/v1/D15-1166
[59] Wei Ma, Shangqing Liu, Wenhan Wang, Qiang Hu, Ye Liu, Cen Zhang, Liming Nie, and Yang Liu. 2023. The Scope of
ChatGPT in Software Engineering: A Thorough Investigation. arXiv:2305.12138 [cs.SE]
[60] Paula Maddigan and Teo Susnjak. 2023. Chat2VIS: Generating Data Visualisations via Natural Language using
ChatGPT, Codex and GPT-3 Large Language Models. arXiv:2302.02094 [cs.HC]
[61] Pablo Antonio Martínez, Gregorio Bernabé, and José Manuel García. 2023. Code Detection for Hardware Acceleration
Using Large Language Models. arXiv:2307.10348 [cs.SE]
42
Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen
[62] MFTCoder. 2023. CodeFuse-MFTCoder: Multitask Fine-Tuned Code LLMs. https://github.com/codefuse-ai/MFTCoder
[63] Anthony MOI, Nicolas Patry, Pierric Cistac, Pete, et al. 2022. huggingface/tokenizers: Rust 0.13.2. https://doi.org/10.
5281/zenodo.7298413
[64] Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru
Tang, Leandro von Werra, and Shayne Longpre. 2023. OctoPack: Instruction Tuning Code Large Language Models.
arXiv:2308.07124 [cs.CL]
[65] Artashes Arutiunian Nathan Coooper et al. 2022. Code Clippy Data: A large dataset of code data from Github for
research into code language models. https://github.com/ncoop57/gpt-code-clippy
[66] Erik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Silvio Savarese, and Yingbo Zhou. 2023. CodeGen2: Lessons for
Training LLMs on Programming and Natural Languages. arXiv:2305.02309 [cs.LG]
[67] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong.
2023. CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis. In The Eleventh
International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.
https://openreview.net/pdf?id=iaYcJKpY2B_
[68] David N Palacio, Alejandro Velasco, Daniel Rodriguez-Cardenas, Kevin Moran, and Denys Poshyvanyk. 2023. Evalu-
ating and Explaining Large Language Models for Code Using Syntactic Structures. arXiv:2308.03873 [cs.SE]
[69] Rangeet Pan, Ali Reza Ibrahimzada, Rahul Krishna, Divya Sankar, Lambert Pouguem Wassi, Michele Merler, Boris
Sobolev, Raju Pavuluri, Saurabh Sinha, and Reyhaneh Jabbarvand. 2023. Understanding the Effectiveness of Large
Language Models in Code Translation. arXiv:2308.03109 [cs.SE]
[70] Hammond Pearce, Benjamin Tan, Baleegh Ahmad, Ramesh Karri, and Brendan Dolan-Gavitt. 2022. Examining
Zero-Shot Vulnerability Repair with Large Language Models. arXiv:2112.02125 [cs.CR]
[71] Hammond Pearce, Benjamin Tan, Baleegh Ahmad, Ramesh Karri, and Brendan Dolan-Gavitt. 2023. Examining
Zero-Shot Vulnerability Repair with Large Language Models. In 44th IEEE Symposium on Security and Privacy, SP
2023, San Francisco, CA, USA, May 21-25, 2023. IEEE, 2339–2356. https://doi.org/10.1109/SP46215.2023.10179420
[72] Phind. 2023. Phind-CodeLlama. https://huggingface.co/Phind/Phind-CodeLlama-34B-v1
[73] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J. Liu. 2020. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. J. Mach. Learn.
Res. 21 (2020), 140:1–140:67. http://jmlr.org/papers/v21/20-074.html
[74] Veselin Raychev, Pavol Bielik, and Martin T. Vechev. 2016. Probabilistic model for code with decision trees. In
Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages,
and Applications, OOPSLA 2016, part of SPLASH 2016, Amsterdam, The Netherlands, October 30 - November 4, 2016,
Eelco Visser and Yannis Smaragdakis (Eds.). ACM, 731–747. https://doi.org/10.1145/2983990.2984041
[75] Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu Tang, Neel Sundaresan, Ming Zhou, Ambrosio Blanco,
and Shuai Ma. 2020. CodeBLEU: a Method for Automatic Evaluation of Code Synthesis. CoRR abs/2009.10297 (2020).
arXiv:2009.10297 https://arxiv.org/abs/2009.10297
[76] replit. 2023. replit-code-v1-3b. https://huggingface.co/replit/replit-code-v1-3b
[77] Daniel Rodriguez-Cardenas, David N. Palacio, Dipin Khati, Henry Burke, and Denys Poshyvanyk. 2023. Benchmarking
Causal Study to Interpret Large Language Models for Source Code. arXiv:2308.12415 [cs.SE]
[78] Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu,
Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer,
Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin,
Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. 2023. Code Llama: Open Foundation Models for Code.
arXiv:2308.12950 [cs.CL]
[79] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexan-
dra Sasha Luccioni, François Yvon, Matthias Gallé, et al. 2022. Bloom: A 176b-parameter open-access multilingual
language model. arXiv preprint arXiv:2211.05100 (2022).
[80] Max Schäfer, Sarah Nadi, Aryaz Eghbali, and Frank Tip. 2023. An Empirical Evaluation of Using Large Language
Models for Automated Unit Test Generation. arXiv:2302.06527 [cs.SE]
[81] Bo Shen, Jiaxin Zhang, Taihong Chen, Daoguang Zan, Bing Geng, An Fu, Muhan Zeng, Ailun Yu, Jichuan Ji, Jingyang
Zhao, Yuenan Guo, and Qianxiang Wang. 2023. PanGu-Coder2: Boosting Large Language Models for Code with
Ranking Feedback. arXiv:2307.14936 [cs.CL]
[82] Atsushi Shirafuji, Yutaka Watanobe, Takumi Ito, Makoto Morishita, Yuki Nakamura, Yusuke Oda, and Jun Suzuki. 2023.
Exploring the Robustness of Large Language Models for Solving Programming Problems. arXiv:2306.14583 [cs.CL]
[83] Mohammed Latif Siddiq, Beatrice Casey, and Joanna C. S. Santos. 2023. A Lightweight Framework for High-Quality
Code Generation. arXiv:2307.08220 [cs.SE]
[84] Mohammed Latif Siddiq, Joanna C. S. Santos, Ridwanul Hasan Tanvir, Noshin Ulfat, Fahmid Al Rifat, and Vini-
cius Carvalho Lopes. 2023.
Exploring the Effectiveness of Large Language Models in Generating Unit Tests.
A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends
43
arXiv:2305.00418 [cs.SE]
[85] Giriprasad Sridhara, Ranjani H. G., and Sourav Mazumdar. 2023. ChatGPT: A Study on its Utility for Ubiquitous
Software Engineering Tasks. arXiv:2305.16837 [cs.SE]
[86] Ruoxi Sun, Sercan O. Arik, Hootan Nakhost, Hanjun Dai, Rajarishi Sinha, Pengcheng Yin, and Tomas Pfister. 2023.
SQL-PaLM: Improved Large Language Model Adaptation for Text-to-SQL. arXiv:2306.00739 [cs.CL]
[87] Weisong Sun, Chunrong Fang, Yudu You, Yun Miao, Yi Liu, Yuekang Li, Gelei Deng, Shenghan Huang, Yuchen Chen,
Quanjun Zhang, Hanwei Qian, Yang Liu, and Zhenyu Chen. 2023. Automatic Code Summarization via ChatGPT:
How Far Are We? arXiv:2305.12865 [cs.SE]
[88] Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, and Neel Sundaresan. 2020. Intellicode compose: Code generation
using transformer. In Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and
Symposium on the Foundations of Software Engineering. 1433–1443.
[89] Toma Tanaka, Naofumi Emoto, and Tsukasa Yumibayashi. 2023. Inductive-bias Learning: Generating Code Models
with Large Language Model. arXiv:2308.09890 [cs.LG]
[90] Xiangru Tang, Bill Qian, Rick Gao, Jiakang Chen, Xinyun Chen, and Mark Gerstein. 2023. BioCoder: A Benchmark
for Bioinformatics Code Generation with Contextual Pragmatic Knowledge. arXiv:2308.16458 [cs.LG]
[91] Shailja Thakur, Baleegh Ahmad, Hammond Pearce, Benjamin Tan, Brendan Dolan-Gavitt, Ramesh Karri, and Siddharth
Garg. 2023. VeriGen: A Large Language Model for Verilog Code Generation. arXiv:2308.00708 [cs.PL]
[92] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, et al. 2022. LaMDA: Language Models for Dialog
Applications. CoRR abs/2201.08239 (2022). arXiv:2201.08239 https://arxiv.org/abs/2201.08239
[93] THUDM. 2023. CodeGeeX2: A More Powerful Multilingual Code Generation Model.
https://github.com/THUDM/
CodeGeeX2
[94] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, et al. 2023. LLaMA: Open
and Efficient Foundation Language Models. CoRR abs/2302.13971 (2023). https://doi.org/10.48550/arXiv.2302.13971
arXiv:2302.13971
[95] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and
Illia Polosukhin. 2017. Attention is All you Need. In Advances in Neural Information Processing Systems 30: Annual
Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA. 5998–6008.
[96] Guan Wang, Sijie Cheng, Qiying Yu, and Changling Liu. 2023. opencoderplus. https://huggingface.co/openchat/
opencoderplus
[97] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi.
2022. Self-Instruct: Aligning Language Model with Self Generated Instructions. CoRR abs/2212.10560 (2022). https:
//doi.org/10.48550/arXiv.2212.10560 arXiv:2212.10560
[98] Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi D. Q. Bui, Junnan Li, and Steven C. H. Hoi. 2023. CodeT5+:
Open Code Large Language Models for Code Understanding and Generation. arXiv:2305.07922 [cs.CL]
[99] Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Junnan Li, and Steven Hoi. 2023. CodeT5Mix: A Pretrained Mixture
of Encoder-decoder Transformers for Code Understanding and Generation.
https://openreview.net/forum?id=
VPCi3STZcaO
[100] Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. 2021. Codet5: Identifier-aware unified pre-trained encoder-
decoder models for code understanding and generation. arXiv preprint arXiv:2109.00859 (2021).
[101] Chunqiu Steven Xia, Yuxiang Wei, and Lingming Zhang. 2022. Practical Program Repair in the Era of Large Pre-trained
Language Models. arXiv:2210.14179 [cs.SE]
[102] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023.
WizardLM: Empowering Large Language Models to Follow Complex Instructions. CoRR abs/2304.12244 (2023).
https://doi.org/10.48550/arXiv.2304.12244 arXiv:2304.12244
[103] Frank F. Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn. 2022. A Systematic Evaluation of Large
Language Models of Code. In Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming
(San Diego, CA, USA) (MAPS 2022). Association for Computing Machinery, New York, NY, USA, 1–10.
https:
//doi.org/10.1145/3520312.3534862
[104] Prateek Yadav, Qing Sun, Hantian Ding, Xiaopeng Li, Dejiao Zhang, Ming Tan, Xiaofei Ma, Parminder Bhatia, Ramesh
Nallapati, Murali Krishna Ramanathan, Mohit Bansal, and Bing Xiang. 2023. Exploring Continual Learning for Code
Generation Models. arXiv:2307.02435 [cs.LG]
[105] Zhou Yang, Zhipeng Zhao, Chenyu Wang, Jieke Shi, Dongsun Kim, DongGyun Han, and David Lo. 2023. What Do
Code Models Memorize? An Empirical Study on Large Language Models of Code. arXiv:2308.09932 [cs.SE]
[106] Hao Yu, Bo Shen, Dezhi Ran, Jiaxin Zhang, Qi Zhang, Yuchi Ma, Guangtai Liang, Ying Li, Tao Xie, and Qianxi-
ang Wang. 2023. CoderEval: A Benchmark of Pragmatic Code Generation with Generative Pre-trained Models.
arXiv:2302.00288 [cs.SE]
44
Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen
[107] Zhiqiang Yuan, Junwei Liu, Qiancheng Zi, Mingwei Liu, Xin Peng, and Yiling Lou. 2023. Evaluating Instruction-Tuned
Large Language Models on Code Comprehension and Generation. arXiv:2308.01240 [cs.CL]
[108] Daoguang Zan, Bei Chen, Yongshun Gong, Junzhi Cao, Fengji Zhang, Bingchao Wu, Bei Guan, Yilong Yin, and Yongji
Wang. 2023. Private-Library-Oriented Code Generation with Large Language Models. arXiv:2307.15370 [cs.SE]
[109] Daoguang Zan, Bei Chen, Dejian Yang, Zeqi Lin, et al. 2022. CERT: Continual Pre-training on Sketches for Library-
oriented Code Generation. In Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI
2022, Vienna, Austria, 23-29 July 2022, Luc De Raedt (Ed.). ijcai.org, 2369–2375. https://doi.org/10.24963/ijcai.2022/329
[110] Daoguang Zan, Bei Chen, Fengji Zhang, Dianjie Lu, Bingchao Wu, Bei Guan, Yongji Wang, and Jian-Guang Lou. 2023.
Large Language Models Meet NL2Code: A Survey. arXiv:2212.09420 [cs.SE]
[111] Wei Zeng, Xiaozhe Ren, Teng Su, Hui Wang, et al. 2021. PanGu-𝛼: Large-scale Autoregressive Pretrained Chinese
Language Models with Auto-parallel Computation. CoRR abs/2104.12369 (2021). arXiv:2104.12369 https://arxiv.org/
abs/2104.12369
[112] Jiyang Zhang, Pengyu Nie, Junyi Jessy Li, and Milos Gligoric. 2023. Multilingual Code Co-Evolution Using Large
Language Models. arXiv:2307.14991 [cs.SE]
[113] Kechi Zhang, Huangzhao Zhang, Ge Li, Jia Li, Zhuo Li, and Zhi Jin. 2023. ToolCoder: Teach Code Generation Models
to use API search tools. arXiv:2305.04032 [cs.SE]
[114] Yuntong Zhang, Xiang Gao, Gregory J. Duck, and Abhik Roychoudhury. 2022. Program vulnerability repair via
inductive inference. In ISSTA ’22: 31st ACM SIGSOFT International Symposium on Software Testing and Analysis,
Virtual Event, South Korea, July 18 - 22, 2022, Sukyoung Ryu and Yannis Smaragdakis (Eds.). ACM, 691–702. https:
//doi.org/10.1145/3533767.3534387
[115] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie
Zhang, Zican Dong, et al. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223 (2023).
[116] Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li,
Teng Su, Zhilin Yang, and Jie Tang. 2023. CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual
Evaluations on HumanEval-X. arXiv:2303.17568 [cs.LG]
[117] Zibin Zheng, Kaiwen Ning, Jiachi Chen, Yanlin Wang, Wenqing Chen, Lianghong Guo, and Weicheng Wang. 2023.
Towards an Understanding of Large Language Models in Software Engineering Tasks. arXiv preprint arXiv:2308.11396
(2023).
[118] Zibin Zheng, Kaiwen Ning, Jiachi Chen, Yanlin Wang, Wenqing Chen, Lianghong Guo, and Weicheng Wang. 2023.
Towards an Understanding of Large Language Models in Software Engineering Tasks. arXiv:2308.11396 [cs.SE]
[119] Li Zhong and Zilong Wang. 2023. A Study on Robustness and Reliability of Large Language Model Code Generation.
arXiv:2308.10335 [cs.CL]
[120] Maosheng Zhong, Gen Liu, Hongwei Li, Jiangling Kuang, Jinshan Zeng, and Mingwen Wang. 2022. CodeGen-Test:
An Automatic Code Generation Model Integrating Program Test Information. arXiv:2202.07612 [cs.SE]
[121] Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. 2023. Language Agent Tree
Search Unifies Reasoning Acting and Planning in Language Models. arXiv:2310.04406 [cs.AI]
[122] Shuyan Zhou, Uri Alon, Sumit Agarwal, and Graham Neubig. 2023. CodeBERTScore: Evaluating Code Generation
with Pretrained Models of Code. arXiv:2302.05527 [cs.SE]
[123] Terry Yue Zhuo. 2023.
Large Language Models Are State-of-the-Art Evaluators of Code Generation.
arXiv:2304.14317 [cs.AI]
[124] Terry Yue Zhuo. 2023.
Large Language Models Are State-of-the-Art Evaluators of Code Generation.
arXiv:2304.14317 [cs.AI]
[125] Terry Yue Zhuo, Zhuang Li, Yujin Huang, Fatemeh Shiri, Weiqing Wang, Gholamreza Haffari, and Yuan-Fang Li. 2023.
On Robustness of Prompt-based Semantic Parsing with Large Pre-trained Language Model: An Empirical Study on
Codex. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics.
Association for Computational Linguistics, Dubrovnik, Croatia, 1090–1102.
"
3,4,"A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications","Wenbo Shang, Xin Huang","A graph is a fundamental data model to represent various entities and their
complex relationships in society and nature, such as social networks,
transportation networks, financial networks, and biomedical systems. Recently,
large language models (LLMs) have showcased a strong generalization ability to
handle various NLP and multi-mode tasks to answer users' arbitrary questions
and specific-domain content generation. Compared with graph learning models,
LLMs enjoy superior advantages in addressing the challenges of generalizing
graph tasks by eliminating the need for training graph learning models and
reducing the cost of manual annotation. In this survey, we conduct a
comprehensive investigation of existing LLM studies on graph data, which
summarizes the relevant graph analytics tasks solved by advanced LLM models and
points out the existing remaining challenges and future directions.
Specifically, we study the key problems of LLM-based generative graph analytics
(LLM-GGA) with three categories: LLM-based graph query processing (LLM-GQP),
LLM-based graph inference and learning (LLM-GIL), and graph-LLM-based
applications. LLM-GQP focuses on an integration of graph analytics techniques
and LLM prompts, including graph understanding and knowledge graph (KG) based
augmented retrieval, while LLM-GIL focuses on learning and reasoning over
graphs, including graph learning, graph-formed reasoning and graph
representation. We summarize the useful prompts incorporated into LLM to handle
different graph downstream tasks. Moreover, we give a summary of LLM model
evaluation, benchmark datasets/tasks, and a deep pro and cons analysis of LLM
models. We also explore open problems and future directions in this exciting
interdisciplinary research area of LLMs and graph analytics.","A Survey of Large Language Models on Generative
Graph Analytics: Query, Learning, and Applications
Wenbo Shang
Department of Computer Science
Hong Kong Baptist University
Hong Kong, China
cswbshang@comp.hkbu.edu.hk
Xin Huang
Department of Computer Science
Hong Kong Baptist University
Hong Kong, China
xinhuang@comp.hkbu.edu.hk
Abstract—A graph is a fundamental data model to represent
various entities and their complex relationships in society and
nature, such as social networks, transportation networks, finan-
cial networks, and biomedical systems. Recently, large language
models (LLMs) have showcased a strong generalization ability
to handle various NLP and multi-mode tasks to answer users’
arbitrary questions and specific-domain content generation.
Compared with graph learning models, LLMs enjoy superior
advantages in addressing the challenges of generalizing graph
tasks by eliminating the need for training graph learning models
and reducing the cost of manual annotation. In this survey, we
conduct a comprehensive investigation of existing LLM studies
on graph data, which summarizes the relevant graph analytics
tasks solved by advanced LLM models and points out the
existing remaining challenges and future directions. Specifically,
we study the key problems of LLM-based generative graph
analytics (LLM-GGA) with three categories: LLM-based graph
query processing (LLM-GQP), LLM-based graph inference and
learning (LLM-GIL), and graph-LLM-based applications. LLM-
GQP focuses on an integration of graph analytics techniques
and LLM prompts, including graph understanding and knowledge
graph (KG) based augmented retrieval, while LLM-GIL focuses
on learning and reasoning over graphs, including graph learning,
graph-formed reasoning and graph representation. We summarize
the useful prompts incorporated into LLM to handle different
graph downstream tasks. Moreover, we give a summary of LLM
model evaluation, benchmark datasets/tasks, and a deep pro and
cons analysis of LLM models. We also explore open problems
and future directions in this exciting interdisciplinary research
area of LLMs and graph analytics.
Index Terms—Graph, LLMs, GNNs, Prompt, Survey
I. INTRODUCTION
Large language models (LLMs) possess billions of parame-
ters and have been trained on extensive corpora using training
strategies like instruction tuning [1] [2] and Direct Preference
Optimization(DPO) [3], enabling them to exhibit powerful
reasoning and semantic representation capabilities, thereby
advancing AI intelligence closer to human levels. Undoubt-
edly, LLMs currently serve as the foundation model for NLP
tasks [4] [5] [6], showcasing strong generalization abilities to
handle various NLP tasks such as question answering [7] [8],
machine translation [9], code generation [10] [11], etc. LLMs
have demonstrated extensive common knowledge and robust
semantic comprehension abilities, fundamentally transforming
existing text-processing workflows. While initially designed
for text data, LLMs are increasingly being utilized for tasks
LLM-GGA
LLM-GQP
LLM-GIL
Graph-LLM-based 
applications
Graphs
LLMs
+
Graph 
Queries
LLMs
Answers
LLMs
Graphs
Graph representation
Graph learning tasks
Graph reasoning
KGs
Fig. 1: Illustration of the LLM-GGA domain. LLM-GGA do-
main includes three principal components: LLM-based graph
query processing (LLM-GQP), which necessitates the melding
of graph analytics techniques and LLM prompts for query pro-
cessing; LLM-based graph inference and learning (LLM-GIL),
focusing on learning and reasoning over graphs; Graph-LLM-
based applications that employ the graph-LLM framework to
address non-graph tasks, such as recommendation systems.
beyond language processing, aiming to leverage the robust ca-
pabilities of LLMs across different tasks, showcasing superior
performance.
Graphs, as structured data, play a crucial role in various real-
world application scenarios, including the citation networks
[12], social networks [13], molecular graphs [14], web links
[15], and to name a few. Various graph analytics tasks have
been studied to show their usefulness, e.g., node classification,
link prediction, subgraph mining, influence maximization, and
so on. Their versatility and ability to capture complex rela-
tionships have made graphs indispensable tools in academic
research and industry platforms. Recently, one kind of graph-
based learning model, graph neural network (GNN) [16] [17],
has been widely studied and applied to solve challenging graph
tasks. The GNN models utilize recursive message passing
[18] and aggregation mechanisms [19] among nodes to derive
representations of nodes, edges, or entire graphs, which have
been used for various downstream tasks. This is thanks to
the strong ability of GNN models to capture both graph
structure and node features. However, GNNs exhibit weak
generalization capabilities [20] [21] [22], requiring retraining
for different graph tasks and showing limited transfer ability.
arXiv:2404.14809v1  [cs.CL]  23 Apr 2024
In other words, no universal graph foundation model could be
easily generalized to handle various types of graph tasks.
Therefore, whether LLMs’ powerful reasoning, semantic
representation, and generalization capabilities can be applied
to address graph tasks, leading to the inspiration of a graph
foundation model, is the core of current efforts in leveraging
existing large language models for graph-related tasks. In one
word, can LLMs solve graph data tasks? More specifically,
we study three detailed questions: (a) what specific graph
tasks can LLMs answer? (b) How do LLMs tackle these
tasks? (c) What is the effectiveness of LLM-based methods
in solving these tasks compared with the existing graph-based
approaches?
To address the above question, this survey conducts a
comprehensive study of existing relevant work on graph an-
alytics and LLMs, focusing on exploring the key issue of
the LLM-based generative graph analytics (LLM-GGA) field.
Drawing from a thorough investigation of the LLM-GGA
domain, we offer a structured and methodical analysis that
delineates the field into three principal components: LLM-
based graph query processing (LLM-GQP), which necessitates
the melding of graph analytics techniques and LLM prompts
for query processing; LLM-based graph inference and learning
(LLM-GIL), focusing on learning and reasoning over graphs;
and lastly, graph-LLM-based applications that employ the
graph-LLM framework to address non-graph tasks, such as
recommendation systems. The framework is shown in Figure
1.
We categorize these three main components into a total
of six directions to provide a guideline for researchers to
conduct more in-depth studies. LLM-GQP includes graph
understanding and KG-based augmented retrieval directions.
LLM-GIL covers graph learning, graph-formed reasoning, and
graph representation directions. The sixth direction is graph-
LLM-based applications. The following section details these
six directions:
• Graph understanding tasks. This research direction is
studying whether LLMs can solve graph algorithm prob-
lems, exploring whether LLMs can comprehend graph
structures to conduct graph mining and graph search. Cur-
rent methods have primarily explored LLMs’ understand-
ing of graph structures, such as shortest path, clustering
coefficient computation [23] [24], and more complex
problems like maximum flow and Hamilton path [25] [26]
[27]. Two main methods are introduced: prompting and
supervised fine-tuning (SFT). The prompting methods
explore the LLM’s current structural understanding abil-
ity through query processing. Meanwhile, SFT methods
enhance LLMs’ structure understanding capability by
tuning it on specific graph datasets. However, many more
tasks are yet to be explored, such as the community
search, keyword search, subgraph pattern mining, and
other NP-hard complex graph problems [28] [29].
• Graph learning tasks. This direction explores whether
LLMs can combine graph structure and attributes for
learning, extracting features of nodes, edges, and graphs,
and understanding the semantic information of graphs,
for example, tasks like node classification, graph classi-
fication, and GQL generation [30] [31] [32] [33]. There
are two main pipelines: LLM-GNN pipelines and LLM
pipelines. LLMs can leverage their powerful reasoning
ability and vast knowledge repository to enhance GNNs
and also can predict results directly.
• Graph-formed reasoning. This direction explores how
LLMs use graph structures to simulate human thinking
during reasoning [34] [35] [36], enabling them to solve
more complex reasoning problems such as algorithmic,
logical, and mathematical tasks. Graph-formed reasoning
involves two types of reasoning: think on the graph and
verify on the graph. Think on the graph refers to LLMs
deriving the final conclusion through the graph structure.
Verify on the graph refers to verifying the correctness of
the LLMs’ intermediate or final outputs through the graph
structure.
• Graph representation. This direction explores enhanc-
ing graph representation with LLMs, particularly for Text
Attribute Graphs (TAGs). LLMs’ strong text representa-
tion capabilities allow text embeddings to capture deeper
semantic nuances. However, the key challenge in this
area remains how to capture and integrate graph structure
into graph representation effectively [37] [38] [39]. There
are three forms of graph representation: graph embed-
ding, graph-enhanced text embedding, and graph-encoded
prompts. Graph embedding methods transform a graph
into a sequential format for LLM processing. Graph-
enhanced text embedding methods integrate structure into
text embedding, where the integration method can be
concatenation. Graph-encoded prompts focus on the way
a graph is described within prompts.
• Knowledge Graph (KG) based augmented retrieval.
This direction investigates the relationship between LLMs
and Knowledge Graphs (KGs). With the emergence of
LLMs, discussions have arisen regarding the potential
replacement of KGs [40] [41] [42] [43]. Consequently,
this paper discusses the limitations of LLMs in processing
factual knowledge, evaluates strategies for improving
LLM efficacy via KG-based augmented retrieval, and
investigates potential avenues for future advancements in
this field.
• Graph-LLM-based applications. This part explores the
tasks where graph-LLM-based methods can be applied
for useful downstream application [44] [45] [46], such as
recommendation systems, conversational understanding,
and so on.
We comprehensively analyze these six research directions
of LLM-GGA to provide valuable definitions and highlighted
methodologies. We also highlight the pros and cons of these
methods and showcase future directions. To further explore the
capabilities of LLMs reliably, this paper uses the prompting
method to test the effectiveness of LLMs in tasks such as
graph structure understanding, graph learning, and graph-
formed reasoning. Details of the prompts and results obtained
during testing are also provided. Additionally, we refine and
compile commonly used and effective prompts for graph-
related tasks, assisting researchers in conducting experiments.
Furthermore, this paper also organizes and introduces the
code for existing popular methods, benchmarks for LLM-GGA
tasks, and evaluations measuring LLM performance in graph
tasks to facilitate future research.
Our contributions and the identified challenges for future
research. In this paper, we provide a comprehensive survey of
the state-of-the-art work on LLMs applied to graph data. We
begin by delineating six critical directions in the field of LLM-
GGA: graph structure understanding, graph learning, graph-
formed reasoning, graph representation, KG-based augmented
retrieval, and graph-LLM-based applications. This categoriza-
tion clarifies the current work and offers a guideline for future
research endeavors. In each direction, we propose a structured
introduction and summarization using vivid examples and
offer suitable specific pipelines. We analyze the advantages
and limitations of current methodologies and suggest avenues
for future research. Furthermore, we organize resources related
to benchmarks, evaluations, and code links within the LLM-
GGA domain to facilitate further investigation by researchers.
Lastly, we identify the fundamental challenges in the LLM-
GGA field, which are the primary obstacles to advancing LLM
in solving graph tasks, including the fundamental issue of how
sequential LLM handles structural graph data, the efficiency
issue of large-scale graph data, and the NP-hard problems of
complex graph analytics. This clarification guides the research
direction for future work on LLM-GGA.
Roadmaps. The organization of this paper is as follows. We
first present the fundamental preliminaries and summarize
the graph description language, which converts graphs into
sequences before inputting them into LLMs in Section II.
Then, we introduce six tasks of LLM-based graph analytics
one by one. We present the graph structure understanding
direction in Section III, graph learning direction in Section IV,
graph-formed reasoning in Section V, graph representation in
Section VI, KG-based augmented retrieval in Section VII and
graph-LLM-based applications in Section VIII. In the above
six directions, we clarify the tasks that LLMs can perform,
discuss the methodologies, conduct a comparative analysis,
and propose guidelines and principles in this direction. Fol-
lowing this, Section IX introduces the popular datasets and
new datasets for solving the above tasks and also provides
metrics for evaluating LLMs or tasks in different directions. In
Section X, we identify and discuss the current and upcoming
challenges that LLM-GGA faces and future directions. Finally,
our conclusions are presented in Section XI.
II. PRELIMINARY
In the subsequent section, we will initially introduce graph
data, proceed to discuss GNNs as a paradigm of graph-
based learning models, then introduce LLMs and distinguish
LLMs and PLMs, and ultimately introduce graph description
language, which can transform the graph into sequential data
as the input of LLMs.
A. Graph
Graph data represents complex relationships through nodes
and edges, where nodes represent entities and edges represent
their interconnections. This structure excels at modeling intri-
cate networks such as social, biological, and transportation
systems. It enables analyses like community detection and
shortest path calculations, offering critical insights into the
dynamics of various systems. Formally, a general graph can
be represented as G = (V, E), where V and E denote the set
of nodes and edges. V = {v1, v2, ..., vn} where the number
of nodes is |V| and |V| = n. E = {eij} where the number of
edges is |E| and eij is an edge from vi to vj.
B. Graph Neural Network
Graph Neural Networks (GNNs) [16] [17] are a type of deep
learning model that can handle graph-structured data. The goal
of these GNNs is to learn representations for each node, which
are computed based on the node’s own features, the features of
the edges connected to it, the representations of its neighbors,
and the features of its neighboring nodes,
hl
v = AGGR(hl−1
v
, {hl
u −1 : u ∈Nv}; θl)
(1)
where hl
v represents the representation of node v in the l-th
layer. AGGR denotes the aggregation function that aggregates
the representations of neighboring nodes from the previous
layer. For the tasks that focus on individual nodes, e.g.,
node classification, the learned representations can be used
directly to accomplish specific objectives. However, for the
tasks that consider the entire graph, e.g., graph classification,
a global representation can be obtained by pooling or applying
other methods to the representations of all nodes. This global
representation can then be used to perform the corresponding
tasks.
C. Large Language Models
Currently, there is no precise definition for Large Language
Models (LLMs). However, according to the pioneering surveys
[47] [48] on LLMs, a distinction can be made between LLMs
and Pre-trained Language Models (PLMs). LLMs are large
language models with billion-level parameters that are pre-
trained on massive amounts of data, such as Llama [5] and
ChatGPT. Conversely, PLMs are pre-trained language models
with million-level parameters that can be more easily fine-
tuned on task-specific data. While LLMs and PLMs share
similarities in their pre-training process, the former is char-
acterized by its larger size and ability to generate human-like
text. Thus, it is essential to consider the potential implications
of using LLMs in various applications.
D. Graph Description Language
Graphs are represented in the structured data in arbitrary
shapes, while LLMs typically process sequential data, such
as the text as a sequence of words. To bridge this gap,
Graph Structure Understanding Tasks
1
4
3
2
0
Graph  Size Calculation
Given <graph>, what is the number 
of nodes and edges in this graph? 
Please answer with the number of 
nodes: X, number of edges: X. 
1
4
3
2
0
Degree Calculation
Given <graph>, what is the degree of 
node 4?  Or, like, find the node degree 
of node [given node] in the given graph.
1
4
3
2
0
Connected Nodes Search
Given <graph>. Is node 3 the 1-hop 
neighbor of node 4? List the answers 
after “Ans:” in the format of [Yes, No,]. 
1
4
3
2
0
Edge Validation
Given <graph>. Is there an edge 
between node 1 and node 2?
1
4
3
2
0
Path Search
Given <graph>. Simple path: Find a 
single path from node 0 to node 4 
connected by edges in the given graph. 
Shortest path: Give the shortest path 
from node 0 to node 4.
1
4
3
2
0
Attribute Retrieval
Given <graph>, what is the title of node 0?
Abstract: Text in curve orientation, 
despite being one of the common…
Title: Total Text A Comprehensive 
Dataset For Scene Text Detection 
And Recognition.
1
4
3
2
0
Graph Density
Given <graph>, what is the density 
of the given graph?
1
4
3
2
0
Eccentricity
Given <graph>, what is the eccentricity 
of the node 0?
1
4
3
2
0
Pattern matching
Given <graph>, in the given graph, the 
triangle must be connected by three edges, 
list the triangle after ”Ans:” in the format of 
[0-1-2]
1
4
3
2
0
Topological Sorting
In a directed graph with 5 nodes numbered 
from 0 to 4: node 0 should be visited before 
node 1, ... Q: Can all the nodes be visited? 
Give the solution.
1
2
1
0
0
Bipartite Graph Matching
There are 2 job applicants numbered from 0 to 1, and 3 jobs numbered from 
0 to 2. Each applicant is interested in some of the jobs. Each job can only 
accept one applicant and a job applicant can be appointed for only one job. 
Applicant 0 is interested in job 1, ... Q: Find an assignment of jobs to 
applicants in such that the maximum number of applicants find the job they 
are interested in.
1
4
3
2
0
Hamilton Path
Given <graph>, is there a path in this graph that 
visits every node exactly once? If yes, give the 
path. Note that in a path, adjacent nodes must 
be connected with edges. 
1
4
3
2
0
Maximum Flow
In a directed graph with 5 nodes numbered 
from 0 to 4, and the edges are: an edge from 
node 0 to node 1 with capacity 10... Q: What 
is the maximum flow from node 0 to node 3?
10
1
5
5
9
(a)
(b)
(c) 
(d) 
(e)
(f)
(g)
(h)
(j)
(k)
(l) 
(m)
(n) 
1
4
3
2
0
Graph Diameter
Given <graph>, what is the diameter 
of the given graph?
(i)
Fig. 2: Graph Structure Understanding tasks.
the graph description language (GDL) transforms the graph
into sequential data, which can be inputted into an LLM.
Specifically, GDL aims to convert graphs into sequential data
while retaining the structure and unique attributes of the graph.
This conversion allows the graph’s information to be fed into
an LLM for processing. There are several graph description
languages:
• Text description. Graph structure can be described using
words such as ‘Node 1 is connected to Node 2’ and
‘There are three nodes connected to Node 1’.
• Adjacency list. An adjacency list represents each vertex
in the graph with the collection of its neighbouring
vertices or edges. Node A is connected with node B and
node C can be denoted as N(v) = {B, C}.
• Edge list. An edge list represents the edge connections
between two nodes in the graph. (A, B) indicates a
connection between nodes A and B.
• GML. Graph Modelling Language [49] consists of an
unordered sequence of node and edge elements enclosed
within ‘[·]’.
• GraphML. Graph Markup Language [50] consists of
XML containing a graph element and an unordered
sequence of node and edge elements.
• SQL. Several specialized SQL languages are designed
specifically for working with graph data. These languages
are also capable of serving as graph description lan-
guages. Some notable examples include Cypher [51], a
query language developed by Neo4j, and Gremlin [52],
SPARQL [53], and GSQL [54]. They combine SQL-
like syntax with graph-specific constructs and algorithms,
making them suitable for complex graph analytics tasks.
• Multi-modality encoding. Except for text description,
graph structure can also be represented using image
description and motif description. The graph can be visu-
alized as an image and inputted into an LLM to process
images. Alternatively, motifs such as stars, triangles, or
clique patterns can represent the graph structure as input
into an LLM.
• Encode as a story. The graph can be encoded within
a specific context, such as a friendship, co-authorship,
social network, politician, or expert. For example, the
connections between nodes can represent friendship re-
lationships. We can assign names to the nodes, such as
‘David’ and ‘Alice’.
Notably, (1) different graph description languages can yield
different results of LLMs. Therefore, it is suggested to test
with multiple GDLs and select the one with the best experi-
mental results. (2) If needed, the LLM’s output form can be
specified along with GDLs in the prompt. LLMs often generate
excessive reasoning processes that may be unnecessary, so
standardizing the LLM’s output can be beneficial.
III. GRAPH STRUCTURE UNDERSTANDING TASKS
Graph structure understanding tasks evaluate whether LLMs
can comprehend graph structures. Simple tasks include the
queries of neighbors, shortest paths, connectivity, the calcu-
lation of graph radius, and the clustering coefficient. More
complex tasks include solving maximum flow problems and
performing topological sorting. These tasks need LLMs to
comprehend graph structures locally and globally, as shown in
Figure 2. In this section, we present 21 graph understanding
tasks along with their definitions. Subsequently, we elaborate
on the two main methods currently used to address graph
structure understanding tasks: prompting and supervised fine-
tuning LLMs.
Task
Prompts
Graph Data Loading
The structure of the [file path] molecular graph of the benzene ring contains a hexagon.
Graph Size Detection
Given [graph], what is the number of nodes and edges in this graph? Please answer with the number of nodes:
X, number of edges: X.
Degree Detection
Given [graph], what is the degree of node 4? Or, find the node degree of node [given node] in the given graph.
Connected Nodes
Given [graph]. Is node 5 the 1-hop neighbor of node 4? List the answers after “Ans:” in the format of [Yes, No,].
Edge Detection
Given [graph]. Is there an edge between node 1 and node 2?
Path
Simple path: Given the undirected graph with the specified nodes and edges, nodes: [0, 1, 2, 3, 4], edges: [(0,
1), (1, 4), (1, 3), (4, 3), (3, 2)], find a single path from node 1 to node 2 connected by edges in the given graph.
Shortest path: Given the directed graph with the specified nodes and edges, nodes: [0, 1, 2, 3, 4], edges: [(0, 1),
(1, 4), (1, 3), (4, 3), (3, 2)], give the shortest path from node 0 to node 4.
Attribute Retrieval
Given [graph]. What is the title of node 0?
Graph Density
Given [graph]. What is the density of the given graph?
Eccentricity
Given [graph]. What is the eccentricity of the given graph?
Graph Radius
Given [graph]. What is the radius of the given graph?
Graph Diameter
Given [graph]. What is the diameter of this graph?
Graph Periphery
Given [graph]. What is the periphery of this graph? Or What are the nodes included by the periphery of the given
graph?
Clustering Coefficient Computing
Given [graph]. What is the clustering coefficient of [given node]?
TABLE I: Prompts for Graph Structure Understanding Tasks, where [graph] is the input of the data.
A. Task Introduction
1) Graph size calculation: Graph size refers to the number
of nodes and edges in a graph. Given a general graph G =
(V, E), the graph size detection task is to detect the |V| and |E|
in G. Through this task, LLMs are expected to understand the
fundamental structure of a graph accurately. Given a prompt
describing the graph and asking related queries, LLMs are
supposed to determine |V| and |E|, as shown in Figure 2 (a).
2) Degree calculation: The degree detection task involves
determining the degree of a specific node in a graph. The
neighbors of node v can be denoted as N(v) = {u|(u, v) ∈
E(v)}, where E(v) is the edge set including edges connected to
v. The degree of vi is the number of its neighbors in G, which
can be denotes as degG(vi) = |N(vi)|. Through this task,
LLMs are expected to comprehend the context surrounding vi
and identify N(vi) accurately. By inputting a prompt about
vi and G, LLMs are expected to calculate the degree of the
node. This task is shown in Figure 2 (b).
3) Connected nodes search: The connected nodes detection
task involves finding all the nodes in NG(vi) of vi in G. Given
the prompt about G, LLMs are expected to analyze the local
structure of the given node vi and determine NG(vi), as shown
in Figure 2 (c).
4) Edge validation:
The edge detection task refers to
whether there exists an edge eij or eij between vi and vi.
Through this task, LLMs are expected to accurately identify
the connectivity between nodes and understand the local
structure of nodes. Given the prompt about the neighbors of
vi to the LLMs, LLMs will likely indicate whether eij or eij
exists, as shown in Figure 2 (d).
5) Path search: We consider two types of paths, including
the simple path and the shortest path, as shown in Figure 2
(e). Given a graph G = {V, E}, the simple path task involves
detecting whether there exists a path (vi, ..., vj) between a
source node vi and a target node vj in G. In other words, it
is about finding a simple path (vi, ..., vj) between vi and vj
without specific requirements. This task evaluates the ability of
LLMs to traverse a graph and understand its structure. Given
the prompt about G to LLMs, the goal is to return a simple
path from vi to vj.
Given a weighted directed acyclic graph G = {V, E} with
each edge e ∈E has a non-negative weight w(e), the shortest
paths task involve finding a path p = (e1, e2, . . . , en) from a
source node to a target node in G such that the sum of the
weights of edges w(p) = Pn
i=1 w(ei) is minimized. LLMs
can evaluate the length of the shortest path and identify the
qualified paths. This task can be further divided into three
objectives: 1. Finding the shortest path between two nodes. 2.
Finding all the shortest paths for all paired nodes. 3. Finding
the average length of all the shortest paths. This task assesses
whether the LLM can effectively determine the shortest route
between two specified nodes within the graph.
6) Attribute retrieval: The attribute retrieval task involves
retrieving detailed information related to nodes, such as the
Task
Prompts
Graph Partition
In the academic collaboration network dblp, scholar #355233 is involved in [TBR] local community formed by his/her
collaborators.
Graph Searching
According to the Freebase knowledge graph, the relation between entity /m/027rn and entity /m/06cx9 is [TBR].
Pattern matching
Triangle: find a single triangle containing node X. Or in the given graph, the triangle must be connected by three edges,
list the triangle after ”Ans:” in the format of [0-1-2]. Cliques: find all the cliques with N nodes in the given graph, list all
the cliques after ”Ans:” in the format of [0-1-2] and separate the answers by a comma. Wedge Centering find a single
wedge containing node X in the given graph, node X must be the center of this wedge, list the wedge after ”Ans:” in the
format of [0-1-2].
Cycle Check
In an undirected graph, (i,j) means that node i and node j are connected with an undirected edge. The nodes are numbered
from 0 to 5, and the edges are: (3,4) (3,5) (1,0) (2,5) (2,0) Q: Is there a cycle in this graph?
Topological Sort
In a directed graph with 5 nodes numbered from 0 to 4: node 0 should be visited before node 4, ... Q: Can all the nodes
be visited? Give the solution.
Maximum Flow
In a directed graph with 5 nodes numbered from 0 to 4, and the edges are: an edge from node 0 to node 1 with capacity
10... Q: What is the maximum flow from node 0 to node 3?
Bipartite Graph Matching
There are 2 job applicants numbered from 0 to 1, and 3 jobs numbered from 0 to 2. Each applicant is interested in some
of the jobs. Each job can only accept one applicant and a job applicant can be appointed for only one job. Applicant 0 is
interested in job 1, ... Q: Find an assignment of jobs to applicants in such that the maximum number of applicants find
the job they are interested in.
Hamilton Path
Given [graph], is there a path in this graph that visits every node exactly once? If yes, give the path. Note that in a path,
adjacent nodes must be connected with edges.
Graph Neural Networks
Given [graph]. Embeddings: node 0: [1,1], ... In a simple graph convolution layer, each node’s embedding is updated by
the sum of its neighbors’ embeddings. Q: What’s the embedding of each node after one layer of simple graph convolution
layer?
Dynamic Graph
In an undirected dynamic graph, (u, v, t) means that node u and node v are linked with an undirected edge at time t. Your
task is to answer when two nodes are first connected in the dynamic graph. Two nodes are connected if there exists a
path between them. Given an undirected dynamic graph with the edges [(0, 1, 0), (1, 2, 1), (0, 2, 2)]. When are node 0
and node 2 first connected?
TABLE II: Prompts for Graph Structure Understanding Tasks, where [graph] is the input of the data. [TBR] means to be
reasoned by LLMs.
attributes of a node. For example, in a citation network, LLMs
are tasked with retrieving specific attributes of a node, such
as the title, abstract, or author of a paper. Given the prompt
about G and detailed attribute information, LLMs are expected
to retrieve the required information, as shown in Figure 2 (f).
7) Graph density: Graph density represents the ratio be-
tween the number of edges present in a graph and the
maximum number of edges that the graph can have. For an
undirected simple graph G = {V, E}, the graph density is
defined as:
D =
2|E|
|V|(|V| −1)
(2)
For a directed simple graph, the graph density is defined as:
D =
|E|
|V|(|V| −1)
(3)
This task requires LLM to calculate the density of a given
graph and assess its understanding of the entire graph, as
shown in Figure 2 (g).
8) Eccentricity: The eccentricity of a node in a graph is
defined as the length of the longest shortest path starting at that
node. The eccentricity of one node: this task requires LLMs
to answer the eccentricity of a given node. The eccentricity of
many nodes: this task requires LLMs to answer the eccentricity
of a subset of nodes or all the nodes in the graph, as shown
in Figure 2 (h).
9) Graph radius: Based on the eccentricity of nodes, the
radius of a graph is the minimum eccentricity of any vertex in
the graph. LLMs can calculate the radius of the given graph
with the description of the graph.
10) Graph center: The center of a graph is the set of
vertices of graph eccentricity equal to the graph radius. Based
on the eccentricity task and graph radius task, LLMs should be
given the graph information and asked to calculate the graph
center.
11) Graph diameter: Based on the shortest path, the diam-
eter of a graph is the length of the shortest path between the
most distant nodes. LLMs can calculate the graph’s diameter
with the given graph information, as shown in Figure 2 (i).
12) Graph periphery: Based on the graph eccentricities and
graph diameter, the graph periphery is a set of vertices that
have graph eccentricities equal to the graph diameter. LLMs
can answer questions related to the graph periphery using the
given graph information.
Fig. 3: Examples for Path Task with GPT3.5 - Graph Structure
Understanding Tasks.
Fig. 4: Examples for Maximum Flow Task with GPT3.5 -
Graph Structure Understanding Tasks.
Fig. 5: Examples for Bipartite Graph Matching Task with
GPT3.5 - Graph Structure Understanding Tasks.
13) Clustering coefficient computing: The clustering coef-
ficient is a measure of how connected a vertex’s neighbors are
to one another. We define the edges among neighbors of vi
as {ejk : vj, vk ∈NG(vi), ejk ∈E}. For directed graphs, the
clustering coefficient is defined as:
Ci = |{ejk : vj, vk ∈NG(vi), ejk ∈E}|
|NG(vi)||NG(vi) −1|
(4)
For undirected graphs, the clustering coefficient is defined as:
Ci = 2|{ejk : vj, vk ∈NG(vi), ejk ∈E}|
|NG(vi)||NG(vi) −1|
(5)
LLMs can calculate the clustering coefficient as a measure of
the degree to which nodes in a graph tend to cluster together.
14) Graph partition: This task is an online social network
reasoning task, which is to infer the community structure of
an online social network by partitioning users into different
clusters based on their interaction information. Each cluster
represents a social community formed by users who interact
with each other frequently. LLMs partition the users of the
social network based on user social interaction patterns and
generate the resulting cluster assignments.
15) Graph searching: This task is a knowledge graph
reasoning task, which involves inferring relationships between
entities based on their information or inferring connected
entities based on the information of entities and relationships.
Specifically, LLM takes entities or relationships as input
and searches for relevant entities or relationships to generate
output.
16) Pattern matching: This task is to identify star, wedge,
triangle, or clique patterns that contain a target node. The
target node can be defined as the center of the pattern.
Alternatively, the task can involve identifying whether these
patterns exist in a given graph and determining the number
of occurrences. Given a description of the LLM graph, the
goal is for LLM to identify different patterns and provide the
corresponding answers, as shown in Figure 2 (j).
17) Cycle validation: This task is to determine whether a
graph contains a cycle. Given G = {V, E}, a cycle is a non-
empty trail with a vertex sequence (v1, v2, ..., vn, v1). Given
the graph information, LLM is asked to determine whether
this graph has a cycle.
18) Topological sorting: Topological sorting of a directed
graph G = {V, E} refers to a linear ordering of its nodes,
where each node comes before all the nodes it points to,
for example, there exists a directed edge eij from vi to vj,
vi comes before vj in the ordering. The resulting array of
node ordering is called topological ordering. LLM is required
to generate a valid topological sorting for the given directed
graph, and there may be multiple valid solutions, as shown in
Figure 2 (k).
19) Maximum flow: Given a capacity constraint, the max-
imum flow problem involves finding the maximum flow that
can be sent through pipes, channels, or other pathways in a
network. Define a flow as fij from vi to vj and the capacity
on edge eij as cij. Given the capability constraints, fij ≤cij
Manual prompt
Self-prompting
API call prompts
……
……
……
Frozen LLM
Manual prompt: Given <graph>, what is the number of nodes 
and edges in this graph? Please answer with the number of nodes: 
X, number of edges: X. 
……
……
……
Frozen LLM
Instructor: You are a brilliant graph master that can handle anything 
related to graphs like retrieval, detection and classification.
Graph description language: GML, GraphML, etc. 
Query: What is the clustering coefficient of node X?
New contexts: Text description of 
input graph generated by LLM itself.
Final output: The clustering 
coefficient of node X is …
……
……
……
Trainable LLM
Regular prompt: What is the diameter of the binomial tree?
API call prompt: The diameter of the binomial tree is Cerulean 
[GR(GL(“gpr”, “binomial_tree”), “toolx:diameter”) →r]
Fig. 6: Promoting methods in graph structure understanding tasks. There are three categories: manual prompts, self-prompting,
and API call prompts.
for all eij. Meanwhile, P
fij>0
fij = P
fji>0
fji for ∀vi except for
the source and the target {s, t} Given a network graph, LLM
generates a path that maximizes the flow from the source to
the sink, as shown in Figure 2 (l).
20) Bipartite graph matching: A bipartite graph is a type
of graph where the nodes can be divided into two disjoint sets,
U and V, such that there are no adjacent nodes within each set.
A matching in a bipartite graph is a set of edges where no two
edges share an endpoint. In a maximum matching, if any edge
is added, it is no longer a matching. For a given bipartite graph,
there can be multiple maximum matchings. LLM can generate
a solution that finds the maximum matching, as shown in
Figure 2 (m).
21) Hamilton Path: In an undirected graph, a Hamiltonian
path is a path in the graph that visits each vertex exactly once.
Given an undirected graph, the task is for LLM to find a valid
Hamiltonian path, as shown in Figure 2 (n).
B. Graph Structure Understanding Methods
The rise of LLMs has sparked researchers’ interest in
exploring their powerful text processing and generalization
capabilities for graph reasoning. Therefore, existing efforts
have introduced various benchmarks to test LLMs’ graph
reasoning potential, aiming to explore their capacity to address
graph-related problems. Prompting methods have emerged as
the primary approach to assess LLMs’ understanding of graph
structures, with some studies also focusing on fine-tuning
LLMs to enhance their graph reasoning abilities. Thus, the
following two main methods are introduced: prompting method
and fine-tuning LLMs.
1) Prompting method: The prompting method [55] can
be categorized into three main types: manual prompt, self-
prompting, and API call prompt, as shown in Figure 6.
Most studies utilize manual prompts, where carefully crafted
prompts guide LLMs to comprehend graph structures better
and understand the objectives of graph tasks, thereby leading
to improved performance on graph-related tasks.
Manual prompts. NLGraph [27] introduces a benchmark
aiming to assess the understanding capabilities of LLMs in
processing textual descriptions of graphs and translating them
into conceptual spaces. This benchmark covers various graph
reasoning tasks like connectivity, shortest path, maximum flow,
and graph neural network construction, with three difficulty
levels (easy, medium, hard) based on graph size and density.
Meanwhile, the number of nodes n = |V| and the probability
p control edge generation, allowing manipulation of graph size
and density for a more reliable evaluation of LLM potential
in graph comprehension.
Next, to guide LLMs in solving these graph tasks, two
prompt methods are proposed by NLGraph [27]: build-a-graph
prompting and algorithmic prompting.
Prompt III-1: Build-a-Graph Prompting. Build-a-Graph
prompting method is to guide LLMs to conceptual grounding
by adding one sentence shown as red words below:
Prompt III-1: Build-a-Graph Prompting
Given <graph description>. Let’s construct a graph
with the nodes and edges first. Q: What is the degree
of node 4?
Prompt III-2: Algorithmic Prompting. The algorithmic
prompting method is designed to guide LLMs to engage in
algorithmic reflection and thinking by adding the details of
the algorithm shown as red words below:
Prompt III-2: Algorithmic Prompting
We can use a Depth-First Search (DFS) algorithm to
find the shortest path between two given nodes in an
undirected graph.
The basic idea is to start at one of the nodes and use
DFS to explore all of its adjacent nodes. At each
node, you can keep track of the distance it takes to
reach that node from the starting node.
Once you have explored all the adjacent nodes, you
can backtrack and pick the node which has the
shortest distance to reach the destination node.
Given <graph description>. Q: Give the shortest path
from node 0 to node 4.
Compared with other advanced prompts and in-context
learning techniques, the two proposed prompts perform better
on graph tasks. Based on the experiments, LLMs indeed pos-
sess preliminary graph reasoning abilities. Also, the benefits
of advanced prompting and in-context learning diminish in
complex graph problems and may even have a negative impact.
LLMs are also susceptible to false correlations, performing
poorly on graph structures such as chains and cliques.
To explore whether LLMs can truly comprehend graph
structures and reason on graphs, meanwhile, enhance the
performance of LLM-GQP tasks, [26] and [24] test LLMs
also using manual prompts, where [26] explores the conditions
under which LLMs can benefit from the inherent structural
information in the data and examines two potential factors in-
fluencing LLM’s performance: data leakage and homogeneity.
In summary, the conclusions are as follows:
• No evidence suggests that LLM’s performance is signif-
icantly attributed to data leakage.
• The performance of LLMs on target nodes is positively
correlated with the local homogeneity of the nodes.
[24] investigates the graph reasoning capabilities of LLMs
and introduces new evaluation metrics—comprehension, cor-
rectness, fidelity, and rectification—to assess LLMs’ pro-
ficiency in understanding graph structures and performing
reasoning tasks. The findings reveal that LLMs can effectively
understand graph structures and perform reasoning tasks.
However, LLMs still face challenges in structural reasoning,
particularly in multi-answer tasks where GPT models demon-
strate errors and overconfidence. In contrast, GPT-4 displays
improved self-correction abilities.
Beyond static graphs, LLMs’ ability to understand dynamic
graph structures is also assessed. Dynamic graphs change
over time, capturing temporal network evolution patterns.
LLM4DyG [25] introduces the LLM4DyG benchmark, which
uses prompting methods to evaluate LLMs’ spatio-temporal
understanding capabilities on dynamic graphs.
Prompt III-3: DST2. The newly proposed Disentangled
Spatial-Temporal Thoughts (DST2) prompting technique en-
hances LLMs’ spatial and temporal understanding of dynamic
graphs. DST2 is shown below:
Prompt III-3: DST2
DyG Instruction: In an undirected dynamic graph, (u,
v, t) means that node u and node v are linked with an
undirected edge at time t.
Task Instruction: Your task is to answer when two
nodes are first connected in the dynamic graph. Two
nodes are connected if there exists a path between them.
Answer Instruction: Give the answer as an integer
number at the last of your response after ’Answer:’
Exemplar: Here is an example: Question: Given an
undirected dynamic graph with the edges [(0, 1, 0), (1,
2, 1), (0, 2, 2)]. When are node 0 and node 2 first
connected? Answer:1
Question: Question: Given an undirected dynamic
graph with the edges [(0, 9, 0), (1, 9, 0), (2, 5, 0), (1, 2,
1), (2, 6, 1), (3, 7, 1), (4, 5, 2), (4, 7, 2), (7, 8, 2), (0, 1,
3), (1, 6, 3), (5, 6, 3), (0, 4, 4), (3, 4, 4), (3, 6, 4), (4, 6,
4), (4, 9, 4), (6, 7, 4)]. When are node 2 and node 1
first connected?
Results show that LLMs have preliminary spatio-temporal
understanding capabilities on dynamic graphs. Dynamic graph
tasks become increasingly challenging with larger graph sizes
and densities while insensitive to periods and data generation
mechanisms.
We provide manual prompt examples for various graph
structure understanding tasks in Table I and Table II. Addi-
tionally, we test LLMs with GPT 3.5 for path, max flow, and
bipartite graph matching using manual prompts, as shown in
Figure 3, Figure 4 and Figure 5 respectively.
For self-prompting. Self-prompting refers to the process
where an LLM continuously updates the initial prompt to make
it easier for LLMs to understand and more beneficial for solv-
ing tasks. In other words, the LLM designs prompts based on
the original prompt. GPT4Graph [23] utilizes self-prompting
by continuously updating the prompt with descriptions related
to the graph. Specifically, first, the graph data is converted into
graph description languages, as shown in Section II-D. Then,
together with queries, it is inputted into the prompt handler to
create a prompt, which is then inputted into the LLM. Based
on the output of the LLM, the prompt is updated and re-
input into the LLM, repeating multiple rounds of updates to
obtain an optimized graph description context, such as context
summarization and format explanation. This process can be
seen as the LLM’s self-updating prompt procedure. Finally,
the optimized graph description context is input along with
the original input into the LLM to obtain the final result.
Prompt III-4: Self-prompting. The input original prompt is
shown below:
Prompt III-4: Self-prompting
Instructor: You are a brilliant graph master that can
handle anything related to graphs like retrieval, detection
and classification.
Graph description language: GML, GraphML as
shown in Section II-D.
Context: Node P357 has 4 neighbors, where each of
which are about anomaly detection with statistical
models...
Query: What is the clustering coefficient of node P357?
This paper conducts experiments on the obgn-arxiv [56] and
Aminer [57] datasets and finds that:
• The design of prompts significantly impacts the results.
The choice of graph description language, the orga-
nization of input data, and the position of in-context
knowledge, such as questions, statements, and examples,
all affect the model’s ability to understand the graph
structure.
• Role prompting techniques can improve the effectiveness
of LLMs by guiding the model to view the graph as
roles and relationships between roles in a specific context.
Providing LLMs with more semantic information leads to
more accurate results.
• Examples in prompts have mixed impacts on graph
structure understanding. Adding examples in prompts to
guide LLMs in understanding graph structures may not
necessarily improve the results; in some graph structure
learning tasks, examples may introduce noise.
API call prompts LLMs exhibit limited ability to perform
precise mathematical calculations, multi-step logical reason-
ing, spatial topological structuring, and temporal information
processing. To bridge these gaps, taking inspiration from
recent models such as ChatGPT and Toolformer [58], Graph-
ToolFormer [59] is proposed to equip LLMs with graph
reasoning capabilities by training them over a prompt dataset
that contains graph reasoning API annotated by ChatGPT.
These graph reasoning APIs are used to call external reasoning
tools. Then, the trained LLMs can solve graph tasks, from
loading graph data and inferring graph attributes to graph
partition tasks.
The framework consists of three parts. First, it generates a
prompt dataset by providing ChatGPT with a regular prompt,
guiding ChatGPT to add an API call to the original prompt,
and then creating a prompt with an API call.
Prompt III-5: API call prompts
Prompt III-5: API call prompts
Example 1
Input:(Regular prompt)
The structure of the benzene ring molecular graph of
benzene ring contains a hexagon.
Output:(API call prompt)
The structure of the [GL(”benzenering”)] molecular
graph of benzene ring contains a hexagon.
Example 2
Input:(Regular prompt)
What is the diameter of the binomial tree?
Output:(API call prompt)
The diameter of the binomial tree is [GR(GL(”gpr”,
”binomial tree”), ”toolx:diameter”)→r].
Second, fine-tune existing LLMs such as GPT-J [60] [61],
LLaMA [5] [62], etc., using technologies like LoRA [63]
on the generated prompt dataset. Thirdly, utilize the fine-
tuned LLM for inference to add graph reasoning API calls
……
……
……
Instructions: How many C-C-O triangles are 
in the molecule?
Graph-enhanced prefix:
Structural and textual features
LLM
Response: There is 1 C-C-O triangle in the molecule.
Fig. 7: Supervised fine-tuning (SFT) method in graph structure
understanding tasks. Prefix tuning is shown above: combine
graph structural and textual information as prefixes in prefix
tuning and input it into LLM with instructions, like GraphLLM
[64]. Instruction tuning can also be used.
into statements. After generating API call statements, how
can external graph tools be invoked? Graph reasoning query
processing comes in. Graph reasoning query processing entails
utilizing external graph reasoning tools based on API call
statements to obtain the final answer.
2) Supervised fine-tuning (SFT) method: Beyond lever-
aging prompts for graph-structured tasks with LLMs, cer-
tain studies have also implemented supervised fine-tuning of
LLMs, illustrated in Figure 7. GraphLLM [64] is committed
to addressing the obstacles in graph reasoning by LLMs and
introduces a hybrid model that inherits the capabilities of both
graph learning models and LLMs, enabling LLMs to interpret
and reason about graph data proficiently, utilizing the superior
expressive power of graph learning models.
C. Comparisons and Discussions
In the following part, we compare the prompting and SFT
methods mentioned above.
The prompting method can be divided into three cate-
gories: manual prompts, self-prompting, and API call prompts.
Most current methods primarily rely on manual prompts,
incorporating techniques like Chain of Thought (CoT) [65],
self-consistency [66], and in-context learning [67]. To obtain
better prompt representations, self-prompting methods are also
widely used. However, the exclusive use of manual prompts
and self-prompting offers limited enhancement to model per-
formance, as they merely tap into the pre-existing capabilities
of LLMs. Additionally, due to the limited input window of
LLM, the graph size that can be input to LLM at once is also
restricted, while graph sizes in the real world are typically
large.
For the prompting method, we also propose two feasible
directions to better leverage existing LLMs for handling struc-
ture understanding tasks. The first direction is breaking down
complex tasks into several sub-problems. While LLMs can
tackle simple graph tasks, they struggle with more challenging
ones. Breaking down complex graph understanding tasks into
simpler components enables LLMs to engage in multi-step
reasoning processes, leading to the resolution of complex
US
KGQA
Given <knowledge graph>, the director who 
directs Inception also direct what?
Inception
Nolan
Oppenheimer
Leonardo
is starred by
GQL Generation
Given <graph>, the director who directs 
Inception also direct what? Use Cypher to 
answer.
1
4
3
2
0
Node Classification
Given <graph>, which arxiv CS subcategory does 
paper ”paper title” with abstract ”paper abstract” 
belongs to? use the abbreviation to answer.
Abstract: Text in curve orientation, 
despite being one of the common…
Title: Total Text A Comprehensive 
Dataset For Scene Text Detection 
And Recognition.
C
C
C
C
C
O
H
C
Given <graph>, is this molecule active with 
H3C4?
Graph Classification
1
4
3
2
0
Node Feature Explanation
Given <graph>, which arXiv CS sub-category does this 
paper belong to? Give 5 likely arXiv CS subcategories as 
a comma-separated list ordered from most to least 
likely, in the form ”cs.XX”, and provide your reasoning. 
Abstract: Text in curve orientation, 
despite being one of the common…
Title: Total Text A Comprehensive 
Dataset For Scene Text Detection 
And Recognition.
1
4
3
2
0
Edge Classification
Learnable prompt
US
Inception
Nolan
Oppenheimer
Leonardo
is starred by
(a) 
(b) 
(c) 
(d) 
(e) 
(f) 
Fig. 8: Graph Learning tasks.
issues, such as GoT [59], which can help address more
intricate graph tasks like generating GNN frameworks, k-truss
tasks, kd-core tasks, etc. The second direction is API call
prompts. Inspired by ToolFormer [58], LLMs can be trained
as agents to utilize tools for graph tasks that are hard to
solve. However, current API call prompt methods [59] utilize
LLMs not as agents but solely to convert user queries into
API command strings for processing by subsequent programs,
exemplified in Prompt III-5.
However, compared to prompting methods, fine-tuning
LLMs with graph data seems a better way to enhance their
understanding of graph structures. There are two mainstream
methods for fine-tuning LLMs: Supervised Fine-Tuning (SFT)
and Reinforcement Learning with Human Feedback (RLHF)
[6]. SFT helps LLMs understand prompts and generate mean-
ingful responses. However, SFT only offers a single human-
written response for each prompt, whereas RLHF provides
detailed human feedback through pairwise comparison label-
ing. Furthermore, to address the instability issue in PPO [68]
training, the Reward Ranked Fine-Tuning (RAFT) [69] can
also be attempted which requires online interaction. For offline
algorithms, methods like DPO [3] and Preference Ranking
Optimization (PRO) [70] can also be utilized for training
LLMs.
IV. GRAPH LEARNING TASKS
A. Tasks Introduction
Recently, LLMs have been shown to possess extensive
common sense and powerful semantic understanding capa-
bilities, fundamentally transforming the existing workflow
for processing text. However, whether LLMs can effectively
handle graph learning tasks, transferring their generalization
ability from text tasks to graph learning tasks, such as node
and graph classification, is still a research subject that needs
exploring. These tasks require the model to learn and solve
graph learning tasks, as shown in Figure 8. In this section, we
present seven graph learning tasks along with their definitions.
Next, we introduce graph learning methods, categorized into
three types based on the role of LLMs: LLMs act as enhancers,
LLMs act as predictors, and graph prompts.
1) Node classification: The node classification task requires
LLM to learn based on the neighbors of a node or the attributes
of a node. It involves classifying unseen nodes in a given
graph, such as categorizing papers in an academic network
into different research directions, as shown in Figure 8 (a).
2) Graph classification: The graph classification task re-
quires LLM to classify the entire graph. LLM is given several
labeled graphs and is expected to classify unseen graphs. For
example, a molecule can be viewed as a graph, and LLM
can predict the properties or functions of the molecule by
classifying the graph, as shown in Figure 8 (b).
3) Edge classification: The edge classification task involves
classifying the edges in a graph. Existing methods improve
edge classification by training a learnable graph prompt and
combining it with a GNN or LLM, as shown in Figure 8 (c).
4) Node generation: The node generation task refers to pro-
viding requirements for an LLM to generate nodes, allowing it
to generate node attributes, which are then added to the TAG
to enhance it.
5) Knowledge graph question qnswering (KGQA): Knowl-
edge graph organizes data into a structured format, represent-
ing entities, properties, and relationships. Knowledge graph
question answering (KGQA) aims to capture the most appro-
priate answers by querying the knowledge graph (KG) using
natural language questions. This task evaluates the ability of
LLM to reason and understand the underlying graph structure
to provide accurate answers, as shown in Figure 8 (d).
6) Graph query language (GQL) generation: The graph
query language generation task involves generating graph
Task
Prompts
KGQA
Given [knowledge graph], the director who directs Inception also direct what?
GQL Generation
Given [graph], the director who directs Inception also direct what? Use Cypher to answer.
Node Classification
Which arxiv CS subcategory does paper ”paper title” with abstract ”paper abstract” belongs to? use the abbreviation
to answer.
Graph Classification
Given [graph]. Is this molecule active with H3C4?
Node Feature Explanation
Abstract: Text in curve orientation, despite being one of the common text orientations in real world environment... Title:
Total Text A Comprehensive Dataset For Scene Text Detection And Recognition. Question: Which arXiv CS sub-category
does this paper belong to? Give 5 likely arXiv CS sub-categories as a comma-separated list ordered from most to least
likely, in the form ”cs.XX”, and provide your reasoning.
Edge classification
learnable prompt
TABLE III: Prompts for Graph Learning Tasks, where [·] is the input of the data.
query languages, including GQL and Cypher, to perform op-
erations on graph databases. Evaluating LLM’s ability to gen-
erate GQL helps users extract information from the database,
as shown in Figure 8 (e).
7) Node feature explanation: Node feature explanation task
involves extracting the attributes of nodes in a text attribute
graph. For example, in an academic paper network, the node
attributes may include abstracts, titles, etc. LLM is expected to
provide reasoning for the classification process of nodes based
on their text attributes and explain the features of the nodes,
as shown in Figure 8 (f).
B. Graph Learning Methods
LLM-GIL studies focusing on graph learning tasks can be
categorized into three main groups: LLMs act as enhancers,
LLMs act as predictors, and graph prompts. When LLMs act
as enhancers, they leverage their advanced semantic under-
standing of the text, strong reasoning capabilities, and vast
knowledge repository to enhance the text attributes associated
with nodes in the graph to enhance GNNs. When LLMs act
as predictors, LLMs are queried or fine-tuned to predict task
results. Inspired by NLP ideas, the Graph prompt aims to
create a unified framework capable of solving multiple graph
learning tasks. Although LLMs are not used, the concept aligns
with LLM-based pipelines.
In summary, integrating LLMs in graph learning tasks
presents a promising avenue for advancing the field. By
leveraging the strengths of LLMs as enhancers and predictors,
along with the strategic use of graph prompts, researchers can
explore new directions for enhanced performance and more
profound insights in LLM-GIL tasks.
1) LLMs act as enhancers: LLMs act as enhancers per-
tains to the LLMs-GNNs pipelines, where LLMs assume an
enhancer role. Within this framework, LLMs are tasked with
processing text attributes, while GNNs are responsible for
handling graph structures, capitalizing on the complementary
strengths of both components to address graph learning tasks
effectively. LLMs bolster GNNs through three distinct mecha-
nisms: encoding the graph into embeddings (as shown in Fig-
…
Trainable LLM
…
…
…
Frozen LLM
…
…
…
Trainable LM
…
…
Text Embeddings
Node Text Attribute
Graph Structure
GNN
+
Fig. 9: Encoding graph into embeddings, when LLMs act as
enhancers. Input the node text attribute into LM/LLM to obtain
text embeddings, then combine the text embeddings with the
graph structure for training and learning in GNNs.
ure 9), generating graph pseudo labels (as shown in Figure 10),
and providing external knowledge or explanations (as shown
in Figure 11). Subsequently, we will provide a comprehensive
elaboration on these three enhancement strategies.
Encoding graph into embeddings. LLMs possess signif-
icant semantic comprehension capabilities to encode better
node embeddings, as shown in Figure 9. TAPE [30] integrates
LM with LLM to generate node embeddings. The process
involves fine-tuning two LM models using original node text
attributes and LLM explanations for node prediction. The
embeddings.
Generating graph pseudo labels.
Providing external knowledge/explanations.
Unlabeled nodes
Nodes with pseudo labels
Training GNN
Annotation with LLM
Fig. 10: Generating graph pseudo labels, when LLMs act as
enhancers. Input unlabeled nodes into LLM for labeling, then
use the labeled nodes with pseudo-labels as input for training
the GNNs for graph learning.
do labels.
Providing external knowledge/explanations.
Node Text Attribute
LLMs
Enhanced text attributes
Node Text Attribute
Designed queries
LLMs
Explanation for
reasoning process
1.
2.
Fig. 11: Providing external knowledge/explanations, when
LLMs act as enhancers. Two pipelines are shown above. In
the first pipeline, input node text attributes into LLM for
elaboration, enhancing the detail of the text attributes. In the
second pipeline, input node text attributes and designed queries
into LLM. LLM leverages the text attributes to answer queries
and explains the reasoning process.
resulting embeddings are then used as input to train a GNN
model for node classification tasks. To unify graph data and
graph learning tasks, OFA [32] introduces a comprehensive
framework that unifies diverse graph data by describing nodes
and edges using natural language and encoding varied and
potentially cross-domain text attributes into feature vectors
within the same embedding space. The obtained feature vec-
tors are then fed into a GNN to tackle various downstream
tasks effectively. Moreover, SIMTEG [71] and GLEM [31]
involve training an LM with Lora and subsequently generating
embeddings as text representations, then a GNN is trained on
top of these text embeddings. On this basis, G-prompt [33]
introduces a graph adapter to extract node features, thereby
obtaining improved node representations.
Generating graph pseudo labels. Many existing pipelines
utilize LLMs to process text attributes as node features, then
feed the embeddings produced by LLM into a GNN model for
learning, as shown in Figure 10. However, the simultaneous
training of LLM and GNN poses a significant computational
challenge. To bridge this gap, GLEM [31] suggests training
the GNN and LM separately in a variational Expectation-
Maximization (EM) framework. In the E-step, the LM predicts
both gold labels and pseudo-labels from the GNN, while in the
M-step, the GNN predicts gold labels and LM-inferred pseudo
labels using the embeddings and pseudo-labels provided by the
LM.
Moreover, due to the high cost of annotation and the
necessity for GNN to learn from a substantial amount of
high-quality labeled data to ensure its performance on graph
tasks, leveraging the zero-shot learning capability of LLM
becomes advantageous. Therefore, employing LLM for graph
annotation can enhance GNN training even with limited la-
beled data. LLM-GNN [72] proposes to select a candidate
node set to be annotated. Subsequently, LLMs annotate the
candidate node set, and post-filtering is conducted to eliminate
low-quality annotations. Finally, the GNN is trained using
the high-quality annotation set and utilized for prediction.
LLM-GNN [72] proposes to select a candidate node set for
annotation by LLMs, followed by post-filtering to remove low-
quality annotations. Then, GNN is trained using high-quality
annotations for prediction.
Providing external knowledge/explanations. LLMs pos-
sess a vast knowledge base, enabling them to provide external
knowledge or explanations related to node features when en-
coding them, as shown in Figure 11. The additional knowledge
assists the model in better extracting and capturing node
features. Graph-LLM [73] utilizes LLMs, such as ChatGPT, to
explain text attributes, enhancing them and generating pseudo
labels. These enhanced attributes are then fed into a trainable
LLM, like Llama, to produce node feature embeddings. The
combined pseudo labels and embeddings are input into a GNN,
which delivers the final prediction outcomes.
Similarly, TAPE [30] leverages LLMs to provide external
explanations. In a citation network where each node contains
text attributes like title and abstract, the text attribute of each
node serves as input to an LLM. The LLM categorizes the
nodes and generates multiple predictions ranked in a list with
accompanying reasoning explanations. This approach aims
to extract the LLM’s reasoning capabilities while integrating
external knowledge to aid in understanding node text attributes
and extracting node features.
2) LLMs act as predictors.: When LLMs are predictors,
they are usually directly employed as standalone predictors.
The critical aspect of integrating LLMs as predictors lies
in crafting a well-designed prompt that encompasses text
attributes and graph structures, enabling LLMs to compre-
hend the graph structure effectively and enhance prediction
accuracy. Additionally, there are other methodologies to fine-
tune LLMs, such as utilizing techniques like LoRA [63] and
instruction tuning, aiming to deepen the LLM’s understanding
of the graph structure. Based on whether LLMs undergo
parameter training, they are categorized into prompting LLMs
and SFT LLMs, as shown in Figure 12.
Prompting LLMs
Supervised fine-tuning (SFT) LLMs.
……
……
……
Trainable LLM
Instructions: How many C-C-O triangles are in the molecule?
Response: There is 1 C-C-O triangle in the molecule.
Response: There is no C-C-O triangle in the molecule.
Response: There is 4 C-C-O triangle in the molecule.
Instruction tuning
……
……
……
Frozen LLM
Manual prompt: The title of one paper is <Title> and its abstract is 
<Abstract>. This paper is cited by the following papers: <Titlelist1>. 
Each of these papers belongs to one category in: <Categories>. You 
need to analyze the paper’s topic based on the given title and abstract.
Fig. 12: LLMs act as predictors. For prompting LLMs, input
designed manual prompts into LLM, enabling it to predict
nodes/links/graphs. For SFT LLMs, input instructions into the
LLM to generate multiple answers. Tuning the LLM is then
based on these multiple responses.
Prompting LLMs. The prompting method can be divided
into two categories. One type is the manual prompts, which
are manually written prompts.
Prompt IV-1: Manual Prompt Template with Slots. For
instance, Beyond Text [74], ENG [75], and Graph Agent
[76] provide a manual prompt template with slots. By filling
these slots with different examples, various prompts can be
constructed. For example:
Prompt IV-1: Manual Prompt Template with Slots
The title of one paper is <Title> and its abstract is
<Abstract>. This paper is cited by the following
papers: <Titlelist1>. Each of these papers belongs to
one category in: <Categories>. You need to 1.Analyse
the papers’ topic based on the given title and abstract;
2.Analyse the pattern of citation information based on
their titles, and retrieve the citation information you
think is important to help you determine the category of
the first given paper. Now you need to combine the
information from 1 and 2 to predict the category of the
first given paper. You should only output one category.
Compared
to
manual
prompts,
LPNL
[77]
generates
Fig. 13: Examples for Node Classification Task with GPT4 -
Graph Learning Tasks.
prompts through sampling. Specifically, it conducts a two-
stage sampling process on the source node and each candidate
neighbor from the original candidate set to acquire anchor
nodes. Prompt generation is then based on these anchor nodes.
We provide manual prompt examples for various graph
learning tasks in Table III. Additionally, we test LLMs with
GPT 3.5 for node classification and KGQA using manual
prompts, as shown in Figure 13 and Figure 14.
Supervised fine-tuning (SFT) LLMs. IntructGLM [78] and
GraphGPT [79] both employ SFT to train LLM for the node
classification task. IntructGLM [78] utilizes a single LLM by
prompting methods. The prompt includes the description of
node attributes and structure through text descriptions and
corresponding queries. LLMs are then tasked with answer-
ing questions and determining node categories, leading to
fine-tuning through supervised learning. On the other hand,
GraphGPT [79] feeds graph structural information and text
Fig. 14: Examples for KGQA with GPT3.5 - Graph Learning
Tasks.
Prefix tasks
Downstream tasks
Unified tasks
GNN
Training on unified tasks
Tunable prompt
Pre-trained GNN
Training on
Prefix tasks
Downstream tasks
Unifying
+
Downstream
tasks
Tuning prompt for
Fig. 15: Graph prompt for graph learning.Graph prompt meth-
ods first unify prefix and downstream tasks, then pre-train
GNN on the unified tasks. The pre-trained GNN, when faced
with different downstream tasks, combines with a tunable
prompt through tuning prompts to handle the downstream
tasks better.
into LLM via embedding. Subsequently, two rounds of in-
struction tuning are conducted to refine LLM and effectively
address the node classification task. IntructGLM [78] em-
ploys prompts to input subgraph structures into LLM, while
GraphGPT [79] inputs them into LLM through embedding.
3) Graph prompt: In graph learning tasks, a wide array of
tasks at the node, edge, and graph levels creates a challenge in
achieving compatibility between pre-training and downstream
tasks, potentially leading to negative transfer effects that can
harm the performance of downstream tasks and compromise
the reliability of transfer learning in graph data. Current
methods aim to harmonize pre-training and downstream tasks
to facilitate more effective transfer learning of graph infor-
mation. Despite these efforts, it remains essential to identify
task-specific differences for optimal performance. Inspired by
NLP, researchers have started incorporating prompts in graph
contexts to enable the reuse of pre-trained models across
various downstream tasks without the need for repeated fine-
tuning, as shown in Figure 15. The integration of prompts
is crucial in assisting downstream tasks in achieving task-
specific optimal outcomes, bridging the gap between pre-
trained models and the diverse array of graph tasks to enhance
performance and transferability.
GPPT [80] and GraphPrompt [81] aim to unify pre-training
and downstream tasks in graph learning. GPPT transforms
node classification tasks into edge prediction tasks and em-
ploys masked edge prediction for GNN pre-training. Mean-
while, GraphPrompt combines node and graph classification
tasks into a subgraph similarity prediction task and utilizes
graph prompt functions, introducing unified instances and task
templates to enhance performance. Subsequent research, like
All in One [82], further consolidates edge, node, and graph
classification tasks into a single framework using multi-task
prompting approaches, standardizing graph prompts similar to
language prompts and enhancing initialization through meta-
learning techniques for improved reliability and generality
across different tasks in graph data analysis.
C. Comparisons and Discussions
For addressing graph learning tasks, existing methods [30]
[79] [82] categorize based on the role of LLM into three types:
LLMs act as enhancers (LLM-GNN pipelines), LLMs act as
predictors (LLM pipelines), and graph prompts. In the part
of graph prompts, we introduce the prompting engineering
in GNNs without utilizing LLMs. Graph prompts aim to
unify downstream tasks and construct a universal framework.
Therefore, it is compared with LLM-GNN pipelines and LLM
pipelines to provide a comprehensive overview.
When LLMs act as enhancers, the most popular pipeline is
the LLM-GNN pipeline. There are three categories of LLM-
GNN pipelines, depending on how LLM enhances GNN:
encoding the graph into embeddings, generating graph pseudo
labels, and providing external knowledge/explanations. How-
ever, the LLM-GNN pipelines that are currently available are
not end-to-end pipelines, meaning that LLM and GNN cannot
be trained together. LLM and GNN can be trained separately
using frameworks like EM framework [31] or by freezing
LLM and using it as an external knowledge base. Co-training
LLM and GNN can lead to issues like gradient vanishing,
which is a significant obstacle in current LLM-GNN pipelines
due to the large number of parameters in LLM compared
to GNN. To solve this problem, methods like knowledge
distillation can reduce the number of LLM parameters while
retaining the beneficial capabilities for downstream tasks.
When LLMs act as predictors, two main methods are
used: prompting LLMs and SFT LLMs. All approaches for
fine-tuning LLMs can be reviewed in the ”comparisons and
discussions” section of Section III. Currently, SFT and DPO
are popular methods for fine-tuning LLMs.
For graph prompt, the workflow involves unifying pre-
training and downstream tasks, followed by prompt tuning
for different downstream tasks through prompt engineering,
as shown in Figure 15. Graph prompts require fewer tunable
Graph Reasoning
Sorting
Sort the following list of numbers in ascending 
order. Output only the sorted list of numbers, 
no additional text. Input: [5, 1, 0, 1, 2, 0, 4, 8, 1, 
9, 5, 1, 3, 3, 9, 7] Output: [0, 0, 1, 1, 1, 1, 2, 3, 3, 
4, 5, 5, 7, 8, 9, 9]
Find the intersection of two sets of numbers. 
Output only the set of numbers that are present 
in both sets, no additional text. Input Set 1: [13, 
16, 30, 6, 21, 7, 31, 15, 11, 1, 24, 10, 9, 3, 20, 8] 
Input Set 2: [25, 24, 10, 4, 27, 0, 14, 12, 8, 2, 29, 
20, 17, 19, 26, 23]
Set Operations
Count the frequency of how many times each country is explicitly 
named in the input text. You can generate any intermediate lists 
and states, but the final output should only contain the frequency 
of each country that appears at least once in the following json 
format, prefixed with ”Output: ” (make sure to keep the same 
spelling for each country in the output as in the input text): {{ 
”country1”: frequency1, ”country2”: frequency2, ... }}
Keyword Counting
Keyword: frequency
Merge the following 4 NDA documents - into a single 
NDA, maximizing retained information and minimizing 
redundancy. Output only the created NDA between 
the tags and , without any additional text. Here are 
NDAs: [four documents] 
Document Merging
Math word problems
Janet’s ducks lay 16 eggs per day. She eats three 
for breakfast every morning and bakes muffins 
for her friends every day with four. She sells the 
remainder at the farmers’ market daily for $2 
per fresh duck egg. How much in dollars does 
she make every day at the farmers’ market?
Math problems
Multi-hop Question Answering
Question triplets: (’Hypocrite’, directed by, $1), ($1, 
death date, $2) Question: When did the director of film 
Hypocrite (Film) die? To answer this question, we 
answer the following subquestions: (1) Who directed 
Hypocrite (Film)? (2) When did Miguel Morayta die? 
Logic reasoning
• Premises: 
1.
It is not true that some giant language models do not have good performance. 
2.
All language models with good performance are used by some researchers. 
3.
If a language model is used by some researchers, it is popular. 
4.
If BERT is a giant language model, then GPT-3 is also a giant language model. 
5.
BERT is a giant language model. 
• Hypothesis: GPT-3 is popular. 
Give hypothesis label, true or false.
5 1 0 1 2 0 4 8 1 9 5 1 3 3 9 7
0 0 1 1 1 1 2 3 3 4 5 5 7 8 9 9
(a) 
(b) 
(c) 
(d) 
(e) 
(a) 
(f) 
(g) 
Fig. 16: Graph-formed Reasoning Tasks.
Fig. 17: Illustration of human logical derivation. [35]
parameters compared to LLM-GNN and LLM pipelines; how-
ever, they have a shallower semantic understanding of graph
attributes. In LLM pipelines, LLMs need to undergo alignment
tuning before they can be used for various downstream tasks.
In LLM-GNN pipelines, there is a general trend of training
GNNs. Combining LLM-GNN and graph prompts is possible
because graph prompts are designed for GNNs through prompt
engineering and can be applied to LLM-GNN pipelines. By
leveraging LLM’s robust semantic representation capabilities
and the lightweight fine-tuning of graph prompts, similar
results can be achieved.
Classical graph tasks, such as node classification on at-
tributed static networks, have recently obtained the most
attention. However, there is potential for more complex tasks
in the future, such as predicting graph evolution on dynamic
graphs. Leveraging LLM models that are suitable for handling
sequential data and can process time series data, along with
GNNs that are adept at capturing changes in graph structures,
can help address a broader range of problems effectively. By
combining the strengths of LLM and GNN, we can tackle
more challenging tasks in the field of graph analysis.
V. GRAPH-FORMED REASONING
A. Tasks Introduction
Graph-formed reasoning refers to combining the graph form
with LLMs to obtain more accurate and reliable answers.
LLMs have strong reasoning capabilities, and many prompting
methods are proposed to enhance LLMs’ reasoning abilities,
addressing algorithmic problems, mathematical issues, etc.,
such as chain of thought, self-consistency, in-context learning,
and more. However, these methods diverge from the patterns
of human thought. The human thought process is typically
non-linear rather than a simple chain of continuous thoughts,
like in Figure 17. Graphs can represent the thinking patterns
of individuals during the thought process. Suppose LLMs
can also use graph-formed reasoning for inference. In that
case, they may be able to solve more complex problems,
such as algorithmic problems, logical reasoning problems,
and mathematical word problems, as shown in Figure 16. In
this section, we present seven graph-formed reasoning tasks
along with their definitions. Next, we introduce graph-formed
reasoning methods involving two types of reasoning: think on
the graph and verify on the graph.
1) Sorting: The problem of sorting involves arranging
certain elements in a specific order. For example, sorting a
list of duplicate numbers from 0 to 9 can be done using a
merge-based sorting algorithm. First, the input sequence of
numbers is divided into subarrays. Then, these subarrays are
sorted individually and merged to form the final solution, as
shown in Figure 16 (a).
2) Set operations: Set operation task mainly focuses on
set intersection. Specifically, the second input set is split into
subsets and the intersection of those subsets with the first input
set is determined with the help of the LLM, as shown in Figure
16 (b).
3) Keyword counting: The keyword counting task aims to
determine the frequency of specific keywords within a given
category in the input text. The input text is divided into
Think on graph
Verify on graph
LLM's intermediate answer
LLM’s final answer
Input
Deleted intermediate answer
Thinking process
Input
LLM's intermediate conclusion
Verification
Verify whether two conclusions 
from two paths are the same.
Verifying process
Fig. 18: Graph-formed reasoning. Two directions: think on graphs and verify on graphs. Think on the graph refers to using
the graph structure to derive the final conclusion during the LLMs’ reasoning process. Verify on the graph refers to using the
graph to verify the correctness of the LLMs’ intermediate and final output.
multiple paragraphs, and the keywords are counted in each
paragraph, with the sub-results aggregated, as shown in Figure
16 (e).
4) Document merging: Document merging is the process
of generating a new document based on multiple input docu-
ments that have overlapping content sections. The goal is to
minimize duplication as much as possible while preserving the
maximum amount of information, as shown in Figure 16 (c).
5) Math word problems: Math word problems include
single- and multi-step word problems with addition, multipli-
cation, subtraction, division and other math topics. LLM re-
quires an understanding of text and mathematical relationships
and involves a multi-step reasoning process where calculations
are performed step by step to arrive at an answer ultimately,
as shown in Figure 16 (d).
6) Multi-hop question qnswering: Multi-hop question an-
swering requires LLM to retrieve and integrate information
from multiple text passages or multi-hop graphs to answer
questions. For a complex reasoning question, LLM uses a
sophisticated thinking process to perform reasoning and ul-
timately arrive at the correct answer, as shown in Figure 16
(f).
7) Logic reasoning: Logical reasoning is a process aimed at
concluding rigorously. It occurs in inference or argumentation,
starting from a set of premises and reasoning towards a
conclusion supported by those premises. Propositional logic
is the most fundamental logical system, consisting of p, q, r,
and various operations, as shown in Figure 16 (g).
B. Graph-formed Reasoning Methods
The graph form, with its inherent structural features, not
only mimics human reasoning patterns but also validates
answers from LLM through the relationships between nodes
and local structure. Existing work can roughly be divided
into two categories: think on the graph and verify on the
graph, as shown in Figure 18. Think on the graph refers
to LLM thinking in the form of a graph, where each node
on the graph represents a step in the thinking process or
an intermediate conclusion during thinking, and the edges
on the graph indicate the direction of LLM inference or the
relationships between intermediate thinking steps. In this way,
the LLM thinking process can be visually represented in graph
form. Verify on the graph means verifying the consistency and
correctness of answers by utilizing the graph’s structure. For
example, if the end node of different paths is the same, the
results derived from different paths should be the same. If
contradictory conclusions arise, then the obtained conclusion
is incorrect.
1) Think on the graph: The GoT* reasoning method [36]
is proposed with a two-stage framework to enable LLM to
reason on a graph for answering multiple-choice questions.
Initially, the input query is converted into a graph form, and
with the incorporation of graph and multimodal features, LLM
generates rationale. This rationale updates the graph to a graph
with rationales, which is then combined with the original input
and fed into the decoder to obtain the final answer.
However, GoT* allows LLM to enhance the graph us-
ing multimodal information but does not reason step-by-
step deduction in graph form. The Graph of Thought (GoT)
[34] represents LLM’s intermediate thinking as an arbitrary
graph, facilitating powerful prompting for solving algorithmic
problems like sorting and keyword counts. LLM thoughts are
depicted as vertices in this approach, with edges representing
dependencies between them. By continuously adding LLM
responses to the graph, arbitrary thoughts can be aggregated,
forming a directed acyclic graph.
Multiple LLMs can also be collaboratively harnessed to
tackle complex mathematical challenges, extending beyond
the capabilities of a single LLM. Cumulative Reasoning (CR)
[35] is proposed as a more human-like reasoning process. CR
utilizes three LLMs in different roles: the proposer, verifier,
and reporter. The proposer suggests the next step, the verifier
checks the accuracy of the steps, and the reporter decides
when the reasoning process should end. Three roles of LLMs
collaborate to achieve more accurate reasoning processes.
Task
Prompts
Sorting
<Instruction>Sort the following list of numbers in ascending order. Output only the sorted list of numbers, no
additional text. </Instruction><Examples>like Input: [5, 1, 0, 1, 2, 0, 4, 8, 1, 9, 5, 1, 3, 3, 9, 7] Output: [0, 0,
1, 1, 1, 1, 2, 3, 3, 4, 5, 5, 7, 8, 9, 9]</Examples>Input: [input list]
Set Operations
<Instruction>Find the intersection of two sets of numbers. Output only the set of numbers that are present in
both sets, no additional text.</Instruction><Examples>like Input Set 1: [13, 16, 30, 6, 21, 7, 31, 15, 11, 1, 24,
10, 9, 3, 20, 8] Input Set 2: [25, 24, 10, 4, 27, 0, 14, 12, 8, 2, 29, 20, 17, 19, 26, 23] Output: [24, 10, 20, 8]
</Examples>Input Set 1: set1 Input Set 2: set2
Keyword Counting
<Instruction>Count the frequency of how many times each country is explicitly named in the input text. You can
generate any intermediate lists and states, but the final output should only contain the frequency of each country that
appears at least once in the following json format, prefixed with ”Output: ” (make sure to keep the same spelling
for each country in the output as in the input text): {{ ”country1”: frequency1, ”country2”: frequency2, ... }}
</Instruction><Approach>To count the frequency for each country follow these steps: 1. Split the input passage
into four paragraphs of similar length. 2. Count the frequency of each country in each paragraph. 3. Combine the
frequencies of each country from each paragraph by adding them together. </Approach><Examples>(Omitted)
</Examples>Input: input text
Document Merging
Merge the following 4 NDA documents <Doc1>- <Doc4>into a single NDA, maximizing retained information
and minimizing redundancy. Output only the created NDA between the tags <Merged>and </Merged>, without
any additional text. Here are NDAs: [four documents]
Math word problems
Q: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends
every day with four. She sells the remainder at the farmers’ market daily for $2 per fresh duck egg. How much
in dollars does she make every day at the farmers’ market?
Multi-hop Question Answering
Question triplets: (’Hypocrite’, directed by, $1), ($1, death date, $2) Question: When did the director of film
Hypocrite (Film) die? To answer this question, we answer the following subquestions: (1) Who directed Hypocrite
(Film)? The film Hypocrite was directed by Miguel Morayta. (2) When did Miguel Morayta die? Miguel Morayta
died on 19 June 2013. So the answer is 19 June 2013.
Logic reasoning
• Premises: 1. It is not true that some giant language models do not have good performance. 2. All language
models with good performance are used by some researchers. 3. If a language model is used by some researchers,
it is popular. 4. If BERT is a giant language model, then GPT-3 is also a giant language model. 5. BERT is a
giant language model. • Hypothesis: GPT-3 is popular. • Label: [True]
TABLE IV: Prompts for Graph-formed Reasoning.
2) Verify on the graph: Verify on the graph is to validate
the intermediate reasoning results of LLM to enhance its
performance. The Reasoning Graph Verifier (RGV) [83] in this
study assumes a logical connection between the intermediate
steps of different inference paths created by LLM. This allows
the multiple solutions generated by LLM for a reasoning task
to be structured into a reasoning graph, aiming to improve
the accuracy and reliability of the outcomes. By constructing
reasoning graphs from the various solutions provided by LLM,
a verifier is trained to determine the correctness of the resulting
reasoning graph. During the prediction phase, RGV assesses
the solutions and selects the highest-scoring one as the final
answer.
However, this work trains an extra model to determine
whether the graph formed by the solutions generated by LLM
is correct rather than utilizing the knowledge within the graph
and the relationships between the knowledge for validation.
The Graph-guided CoT [84] approach aims to improve the
relevance of rationales generated by CoT during multi-step
reasoning. It starts by extracting triplets from questions using
LLM to build a question graph and generates intermediate sub-
questions from this graph. To ensure the rationale from LLM is
logical, Retrieval Augmented Generation (RAG) is used. In an
open-book scenario, knowledge retrieval is based on the sub-
questions, providing retrieved documents and sub-questions
as input to LLMs. LLMs generate rationales for the sub-
questions, creating a rationale graph. Based on the rationale
graph, the study assesses whether the generated rationales
aid in solving the original question. By iteratively generating
intermediate rationales, the solution to the original question
can be determined.
Finally, we provide manual prompt examples for various
graph learning tasks in Table IV. Additionally, we test LLMs
with GPT-4 for sorting and logic reasoning using manual
prompts, as shown in Figure 19.
C. Comparisons and Discussions
Graph-formed reasoning is categorized into think on the
graph and verify on the graph. Think on the graph refers to
using the graph structure to derive the final conclusion during
the reasoning process with LLM. On the other hand, verify
on the graph involves treating the intermediate or final results
generated by LLM as nodes on the graph and using the graph
to determine if there are contradictions between the nodes,
thus verifying the correctness of the LLM output.
For “think on the graph”, a common issue with existing
approaches is their lack of convenience. Compared to CoT
and SC, the reasoning processes in current works are complex,
Fig. 19: Examples for Logic Reasoning Task with GPT4 -
Graph Reasoning Tasks.
requiring multiple stages of reasoning and validation. Graph
of thought methods are not plug and play, which contradicts
the original intent of prompts. Even though using more LLMs
can simplify the reasoning and validation process, it raises the
cost and barrier to entry for reasoning. Therefore, the current
challenge is to find a plug-and-play, low-barrier LLM graph
reasoning method that improves LLM reasoning capabilities.
For “verify on the graph”, the current approaches have yet to
utilize the nature of the graph structure for validation. Existing
methods either retrain a model to determine correctness or use
a KG for assessment without using the relationships between
nodes to infer whether the conclusions within each node in
the graph are correct.
Therefore, for the “think on the graph,” the future direction
could focus on developing a plug-and-play, low-barrier LLM
graph reasoning method that enhances LLM reasoning abili-
ties, a pressing issue that needs to be addressed. On the other
hand, concerning the “verify on the graph” method, future
research could explore how to utilize the relationships between
nodes in the graph structure to verify the outputs of LLM or
the reasoning process itself.
VI. GRAPH REPRESENTATION
A. Tasks Introduction
LLMs’ powerful text representation abilities empower text
embeddings to capture deeper semantic nuances, which also
can enhance graph representations, particularly for Text At-
tributed Graphs (TAGs). When dealing with structured text
data, the key challenge is integrating graph structures into text
embeddings produced by LLMs to enhance their informative-
ness or enable LLMs to process text embeddings with graph
structures within the text space. Moreover, effectively incor-
porating the graph description within the prompt is essential
for LLMs, especially in closed-source models like ChatGPT,
where the embedding is invisible. How the graph is encoded
within the prompt influences the model’s comprehension of
the graph. Thus, we summarize three types of graph repre-
sentation: graph embedding, graph-enhanced text embedding,
and graph-encoded prompts, as shown in Figure 20. Next, we
introduce graph-formed reasoning methods corresponding to
the above three types.
1) Graph embedding: Graph embedding focuses on trans-
forming a graph into a specific ordered sequence, which is then
fed into an LLM to learn the sequence’s embedding using their
excellent semantic capturing ability and then derive the graph
embedding.
2) Graph-enhanced text embedding: Graph-enhanced text
embedding emphasizes incorporating structural embedding
into text embedding. There are two types of embeddings:
structural embedding, which captures the local structure, and
text embedding, which captures the semantic meaning. How to
combine these two types of embeddings is the core of graph-
enhanced text embedding.
3) Graph-encoded prompts: Graph-encoded prompts con-
centrate on how to describe a graph so that LLMs can
understand it more efficiently and then input it into LLMs.
For instance, in a regular graph, the graph can be placed in a
story context by assuming that the relationships between the
nodes are friends or colleagues.
With the emergence of LLM, much work has been done on
graph representation. Three goals of the graph representation
direction can be identified from the above three categories:
to obtain better graph embeddings as an input into GNNs, to
obtain better text embeddings as an input into LLMs/LMs,
and to get better prompts for graph description as an input
into LLMs.
B. Graph Representation Methods
For the three categories of tasks mentioned above, each
type of task has specific focuses, technical characteristics, and
objectives.
1) Graph embedding: Text data is sequential, while graph
data is structural, posing a challenge for LLMs, which excel at
handling text but struggle with graphs. How do we transform
graphs into sequences? Graph embedding methods use specific
order sequences to represent the graph, where specific order
represents graph structure. WalkLM [38] aims to enhance
Graph representation
Graph embedding
Graph-enhanced text embedding
Graph-encoded prompts
Graph
1
4
3
2
0
1
4
3
1
4
0
1
3
2
0
4
……
Specific order sequences
LLM/PLM
Graph Embeddings
Text embeddings
Text tokens
LLM/PLM
Text embeddings with
graph structure
Graph structural embeddings
+
……
Prompt: (placing graph G in multiple contexts)
G describes a friendship graph among James, David, John…
G describes a co-authorship graph among James, David, John…
G describes a social network graph among James, David, John…
……
LLM
Response: ……
Fig. 20: Graph representation. Three types of graph representation are shown: graph embedding, graph-enhanced text embedding,
and graph-encoded prompts. Graph embedding methods use specific order sequences to represent the graph. Graph-enhanced
text embedding emphasizes incorporating structural embedding into text embedding. Graph-encoded prompts concentrate on
how to describe a graph in prompts.
graph representations in TAGs by utilizing a language model.
Initially, text sequences are generated on the TAG through
the random walk algorithm, capturing structural features and
node proximity. By incorporating text information from nodes
and edges into these sequences based on the graph structure,
the texturing process preserves component attributes. Subse-
quently, these sequences are input into a masked language
model for training, where each token represents a node or
edge, leading to improved graph representations and enhanced
downstream task efficiency. Notably, various masked language
model options, including LLMs, are available.
While WalkLM [38] focuses on superior graph embeddings
for tasks like node classification, GraphText [37] transforms
graphs into the natural language to enable LLMs to process
graph data in the text domain, leveraging LLMs’ generalization
capabilities for graph tasks. GraphText [37] reformulates graph
reasoning as text-to-text problems, establishing text input and
output spaces. GraphText first constructs grammar trees for
graphs, then traverses them to generate graph text sequences,
and finally maps the graph to the text space. The text input is
then fed into an LLM, with the LLM results mapped to the
label space, effectively enabling LLMs to handle graph tasks.
2) Graph-enhanced text embedding: Current work focuses
on simply passing graph structure information to the LLM
through prompts without deeply learning the graph structure,
which can lead to an LLM’s insufficient understanding of
complex structural relationships.
DGTL [39] integrates graph information into text with
LLMs for node classification tasks. It begins by inputting text
into a frozen LLM to create text embeddings from the last
layer. Then, a disentangled graph learning method is employed
to extract various structural details and generate structure
embeddings. These structure embeddings are combined with
the text embeddings and fed back into the frozen LLM for
node classification. The entire process is fine-tuned to optimize
the disentangled graph learning for better results.
While DGTL [39] concentrates on utilizing LLMs to in-
tegrate text and graph structure for graph tasks, G2P2 [85]
emphasizes merging graph structure with text to address text
classification tasks. Textual data commonly exhibit network
structures, such as hyperlinks in citation networks or purchase
networks, which encapsulate meaningful semantic relation-
ships that can enhance text classification performance.
G2P2 [85] is proposed to tackle low-resource text clas-
sification through a dual approach. Three graph interaction-
based contrastive strategies are introduced during pre-training
to jointly pre-train the graph-text model. In the downstream
classification process, efforts are made to facilitate the joint
pre-trained model in achieving low-resource classification.
3) Graph-encoded prompts:
The prompting method is
crucial for LLMs to solve tasks. For closed-source LLMs,
the prompt serves as instructions to guide the LLM in under-
standing and solving problems. Therefore, effectively encoding
graphs in the prompt is vital for LLMs to comprehend graph
structure and solve graph tasks. Graph encoding refers to how
graphs are represented in the prompt.
Talk Like A Graph [86] introduces diverse graph encoding
techniques by placing the same graph in multiple contexts.
This strategy highlights how a node, which may lack intrinsic
meaning, can be interpreted differently based on the context;
for instance, a node could represent a person named David,
with edges indicating various relationships like co-authorships
or friendships. When asking LLM the degree of one node, in
the given contexts, that equals how many friendships David
has.
In contrast, Talk Like A Graph [86] primarily emphasizes
text modality graph encoding, while Which Modality Should I
Use [87] employs three encoding modalities - text, image, and
motif - to encode graphs. The latter method utilizes different
prompt techniques to evaluate the overall connectivity of a
graph, enabling LLMs to handle intricate graph structures
more effectively. Specifically, the text modality encoding pro-
vides insights into subgraphs and their connections at a local
level, while the motif modality encoding captures essential
graph patterns like stars, triangles, and cliques, offering a bal-
anced perspective on local and global information. Moreover,
the image modality encoding delivers a broader view of nodes
with limited labels, effectively utilizing the input context.
Query: What other works 
does the director who 
directed Inception have?
KGs
LLMs
1. ""The Dark Knight Trilogy"" 
2. ""Interstellar""
3. ""Dunkirk""
4. ""Memento""
5. ""The Prestige""
6. ""Insomnia""
Incomplete answer
Query: What other works 
does the director who 
directed Inception have?
LLMs
1. ""The Dark Knight Trilogy"" 
2. ""Interstellar""
3. ""Dunkirk""
4. ""Memento""
5. ""The Prestige""
6. ""Insomnia""
7. “Oppenheimer""
Complete answer
+
KGs enhanced LLMs
Fig. 21: KG-based augmented retrieval. Knowledge graphs can
enhance LLMs to provide more comprehensive answers.
In comparing these two methods, Talk Like A Graph [86]
focuses on diverse graph encoding within text modality by
constructing contexts, whereas Which Modality Should I Use
[87] utilizes multiple modalities to encode graphs compre-
hensively, enhancing the LLMs’ ability to understand graph
structures.
C. Comparisons and Discussions
Graph embedding focuses on transforming a graph into a
specific ordered sequence, which is then fed into an LLM
to learn the sequence’s embedding and derive the graph em-
bedding. On the other hand, graph-enhanced text embedding
emphasizes incorporating structural embedding into text em-
bedding. Lastly, graph-encoded prompts concentrate on how
to describe a graph and input it into an LLM.
However, due to LLMs’ powerful text representation capa-
bilities, the first two methods exhibit a deep semantic under-
standing of graph attributes. However, they still need suitable
structural information capturing, which remains rudimentary
and inadequate. Additionally, aligning the graph structure
features with text features to better represent the graph’s
features is a current issue that needs to be addressed.
For graph-encoded prompts, most methods build a narrative
context for the graph or describe it multimodally before feed-
ing it into an LLM. Both methods enable the LLM to interpret
the graph from various perspectives to improve performance.
The critical challenge currently lies in designing diverse and
easily understandable graph descriptions for LLMs, convey-
ing essential graph descriptions while enhancing the LLM’s
comprehension of the input description.
VII. KNOWLEDGE GRAPH BASED AUGMENTED
RETRIEVAL
LLMs have shown remarkable reasoning capabilities in
challenging tasks, sparking debates on the potential replace-
ment of Knowledge Graphs (KGs) in triplet form (subject,
predicate, object) by LLMs. Recent LLMs are seen as viable
alternatives to structured knowledge repositories such as KGs,
indicating a shift towards utilizing LLMs for processing real-
world factual knowledge [88] [89].
A. LLMs limitations and comparison with KGs
LLMs, while powerful, face several significant challenges:
• Hallucination is a common issue for LLMs due to a
lack of domain-specific knowledge and knowledge ob-
solescence, leading to incorrect reasoning and reduced
credibility in critical scenarios like medical diagnosis and
legal judgments [88] [90] [43]. Although some LLMs can
explain predictions through causal chains, they struggle
to address hallucination effectively. Integrating external
KGs can help mitigate these problems [41].
• Insufficient domain knowledge hampers LLM perfor-
mance in specific areas, including private datasets, ne-
cessitating the integration of domain-specific knowledge
graphs to enhance their ability to answer domain-specific
questions [40].
• LLMs struggle with recalling facts when generating
knowledge-based content, despite excelling in learning
language patterns and conversing with humans [89].
• LLMs have limitations in accurately capturing and re-
trieving foundational knowledge, hindering their ability
to access factual information effectively [42].
In contrast, KGs like Wikipedia and DBpedia are structured
repositories of rich factual knowledge, providing a more
explicit and reliable source of information compared to the
black-box nature of LLMs, as shown in Figure 21. How do
we measure the shortcomings of LLM relative to KG? KGLens
is proposed as an effective method to evaluate the factual
accuracy and identify knowledge gaps in LLMs by assessing
the alignment between a KG and LLM [91].
B. Solutions to LLMs limitations
To address the limitations of LLMs, such as hallucination,
insufficient domain knowledge, etc., integrating LLMs with
KGs is a potential way to allow LLMs to learn knowledge
from KGs and enhance their capabilities. The REASONING
ON GRAPHS (RoG) framework [43] synergizes LLMs with
KGs for faithful and interpretable reasoning. Specifically,
RoG utilizes a planning retrieval-reasoning framework where
relation paths grounded by KGs are generated as faithful plans.
These plans are then used to retrieve valid reasoning paths
from KGs to facilitate LLMs’ faithful reasoning. Existing work
has taken on the challenges posed by the four main limitations
of LLMs through distinct perspectives, each offering unique
solutions.
Addressing the first limitation concerning hallucination is-
sues in LLMs, the Head to Tail benchmark [88] is introduced
to assess LLMs’ reliability in answering factual questions
and to evaluate the probability of hallucination in generating
KG triples. Additionally, it explores whether factors like
model size or instruction tuning can enhance LLM knowledge.
Think-on-Graph (ToG) [41] partially addresses hallucination
by involving the LLM agent in iteratively searching KGs,
identifying promising reasoning paths, and providing likely
reasoning outcomes. The second limitation is LLM needs
domain-specific knowledge. To tackle this, GLaM [40] is
developed to convert knowledge graphs into text paired with
labeled questions and answers, allowing LLMs to acquire and
respond to domain-specific knowledge. Regarding the third
limitation related to LLMs forgetting facts, integrating KGs
with PLMs (KGPLMs) [89] is introduced to enhance the
model’s ability to recall facts compared to standalone LLMs.
This approach emphasizes the competitive and complementary
relationship between LLMs and KGs, where LLMs improve
knowledge extraction accuracy, and KGs guide LLM training
to enhance memory and knowledge application capabilities.
Finally, the fourth limitation pertains to LLMs’ challenge
in accurately retrieving and returning knowledge from KGs.
KGs can enhance LLM performance by incorporating them
during pre-training and inference stages or to deepen LLM’s
understanding of acquired knowledge. Graph Neural Prompt-
ing (GNP) [42] is proposed to augment pre-trained LLMs
using foundational knowledge, such as retrieval-augmented
generation, to facilitate effective learning from KGs. GNP [42]
retrieves and encodes relevant, grounded knowledge to gener-
ate Graph Neural Prompts, embedding vectors that provide
guidance and instructions for LLMs.
C. Other KG + LLMs works
1) KG tasks with LLMs: Moreover, LLMs can enhance
KGs to tackle a broader array of challenges. By leverag-
ing LLMs, KGs can be fortified to perform various KG-
related tasks such as embedding, completion, construction,
text generation from graphs, and question answering [90].
An illustrative example is how LLMs can support KG tasks
such as knowledge graph alignment. In entity alignment tasks
between different knowledge graphs, the objective is to iden-
tify pairs of entities representing the same entity. To address
this, AutoAlign [92] facilitates alignment without the need for
expensive manual seed creation. Specifically, AutoAlign [92]
automatically identifies similarities between predicates across
different KGs with the assistance of LLMs.
2) Applications of KGs + LLMs: The combination of KGs
and LLMs has other applications as well. For instance, it can
address tasks like multi-document question answering. Knowl-
edge Graph Prompting (KGP) [93] is introduced to design
appropriate context by building and exploring a knowledge
graph. Subsequently, this context guides LLMs for answering
multi-document questions.
D. Summary
In conjunction with LLMs, the future directions for KGs fo-
cus on overcoming challenges and seizing opportunities in this
evolving field. Firstly, leveraging KGs for Hallucination Detec-
tion in LLMs aims to address the issue of generating inaccurate
content. Secondly, utilizing KGs for Editing Knowledge in
LLMs will enable the swift adaptation of internal knowledge
to real-world changes. Moreover, the challenge of injecting
knowledge into Black-box LLMs due to restricted access to
internal structures necessitates innovative approaches. Lastly,
Occupations
LLM tuning
Behavior Graph
Fig. 22: Graph-LLM-based applications - Recommendation
systems. This shows LLM for graph data understanding in
online job recommendations [46].
integrating Multi-Modal LLMs with KGs can enrich handling
diverse data types within knowledge graphs [90].
VIII. GRAPH-LLM-BASED APPLICATIONS
Graph-LLM-based applications refer to frameworks that
integrate graphs with LLMs. Apart from their applications
in graph-related tasks, they are also utilized in various other
domains (as shown in Figure ??), such as conversational
understanding and recommendation systems, as shown in
Figure 22. Common frameworks involve combining GNNs
with LLMs, merging graph data with LLMs, and exploring
additional innovative approaches that leverage the advantages
between graph structures and language models for diverse
applications.
1) Conversational understanding:
By combining LLM
with graph traversal, collaborative query rewriting [94] is
proposed to improve the coverage of unseen interactions,
addressing the flawed queries users pose in dialogue systems.
Flawed queries often arise due to ambiguities or inaccura-
cies in automatic speech recognition and natural language
understanding. When integrated with graph traversal, LLM
can effectively navigate through the graph structure to retrieve
relevant information and provide more accurate responses.
2) Response forecasting: LLM can effectively handle social
networks and extract latent personas from users’ profiles and
historical posts. SOCIALSENSE [95] is proposed to utilize
LLMs to extract information to predict the reactions of news
media. By analyzing individuals’ characteristics and behavior
patterns within social networks, LLM can effectively predict
the impact of news releases and prevent unintended adverse
outcomes.
3) Multi-domain dialogue state tracking: LLM can learn
from multi-domain dialogue history, query, and graph prompts,
enabling it to track dialogue states and generate dialogue
content, like SHEGO [96]. By incorporating information from
various sources, such as previous dialogue exchanges, user
queries, and relevant graph prompts, LLM can understand the
conversation’s context and dynamics, allowing LLM to track
the current dialogue state effectively and generate appropriate
responses or dialogue content based on the inputs.
4) Recommendation systems: LLMs can also help address
issues in recommendation systems [46], as many tasks in
recommendation systems require learning graph structures,
such as user-item interaction networks. LLMRec [44] aims
to enhance recommendation systems by tackling data sparsity
by adopting three simple yet effective LLM-based graph-
enhancement strategies.
5) Graph neural architecture search: LLMs can help ad-
dress Graph Neural Architecture Search (GNAS). GNAS re-
quires intensive human effort and rich domain knowledge
to design search spaces and strategies. Leveraging powerful
knowledge and reasoning capabilities, LLMs can identify
suitable GNN frameworks within the search space of graph
neural network frameworks. GPT4GNAS [45] integrates GPT-
4 into GNAS, introducing a new set of prompts for GPT-4 to
guide it towards generating graph neural structures.
IX. BENCHMARK DATASETS AND EVALUATIONS
In this section, we summarize benchmark datasets and
evaluation metrics for LLMs.
A. Datasets
This paper summarizes the popular and new datasets, the
LLM employed, the performed tasks, and the links to the open-
source code in the LLM-GGA area, as illustrated in Table V.
Below, we introduce commonly used benchmarks and the new
benchmarks proposed for the LLM-GGA field.
1) Popular datasets: Popular benchmark refers to a graph
benchmark that is widely and frequently used. We have sys-
tematically categorized these popular benchmarks according to
six directions, detailing which benchmarks are used for each
direction. Below are listed popular benchmarks commonly
used in the six directions.
• Graph
structure
understanding:
ogbn-arxiv
[56],
ogbn-products
[56],
Cora
[100],
CiteSeer
[101],
Aminer(DBLP) [57], MetaQA [102], Wikidata5M [103],
PROTEINS [104], MUTAG [105], NCI1 [106], PTC
[107], Foursqure [108].
• Graph learning: ogbn-arxiv [56], ogbn-products [56],
ogb-papers110M [56], ogb-citation2 [56], Cora [100],
CiteSeer [101], Amazon-items [109], PubMed [110],
Reddit [111], CoraFull [112], Amazon [113], PROTEINS
[104], COX2 [114], BZR [114], OAG [115]
• Graph-formed
reasoning:
GSM8K
[116],
SVAMP
[117], FOLIO [118]
• Graph
representation: Cora [100], CiteSeer [101],
Goodreads-books [119], PubMed [110], Amazon [113],
MIMIIC-III [120], Freebase [121], FB15K-237 [122]
• KG-based augmented retrieval: CWQ [123], WebQSP
[124], Wikidata [103]
• Graph-LLM-based applications: depending on specific
applications.
2) New datasets: More than existing datasets are needed to
explore LLMs’ ability to understand graph structures and their
potential to solve graph problems better. As a result, many
works have proposed new benchmarks to advance research in
this field, as shown in Table VI.
• GPR [59] contains 37 particular connected graph in-
stances generated by the Networkx toolkit, which include
the “bull graph,” “wheel graph,” “lollipop graph,” etc.
These generated graph instances are relatively small, with
about 15 nodes and 28 links on average.
• GraphTMI [87] is a graph benchmark featuring a hi-
erarchy of graphs, associated prompts, and encoding
modalities. Different graph task difficulty depends on the
dual criteria of 1) count of motifs and 2) homophily in
the graph, which yields a dataset of EASY, MEDIUM,
and HARD graph problems.
• LLM4DyG [25] benchmark is to evaluate whether LLMs
are capable of understanding spatial-temporal information
on the dynamic graph. Nine dynamic graph tasks are
designed to assess LLMs’ abilities considering spatial and
temporal dimensions.
• GraphQA [86] comprises a set of diverse fundamental
graph problems with more varied and realistic graph
structures compared to previous studies in LLM research.
GraphQA is designed to measure the performance of
LLMs in graph data reasoning.
• NLGraph [27] benchmark is to examine whether lan-
guage models can reason with graphs and structures.
NLGraph contains eight graph structure understanding
tasks with varying algorithmic difficulties. Depending
on different network sizes, graph sparsity, and more,
NLGraph results in easy, medium, and hard subsets in
each graph reasoning task to enable difficulty scaling and
fine-grained analysis.
• GraphextQA [98] benchmark is a dataset for open do-
main question answering. It includes paired subgraphs
used to develop and evaluate graph language models.
The subgraphs are retrieved from Wikidata and contain
reasoning paths from entities mentioned in the questions
to the entities that the questions are asking about.
• CS-TAG [99] benchmark is a comprehensive and wide-
ranging compilation of benchmark datasets for TAGs.
This dataset encompasses a variety of challenging scenar-
ios, ranging from citation networks to purchase graphs.
The collection consists of eight distinct TAGs sourced
from diverse domains.
We also list which directions these new benchmarks are typ-
ically used for. For graph structure understanding, GPR [59],
GraphTMI [87], LLM4DyG [25], NLGraph [27], and CS-TAG
[99]can be used. For graph learning, CS-TAG [99] can be used.
For graph-formed reasoning, GraphextQA [98] can be used.
For graph representation, GraphTMI [87], GraphQA [86], and
CS-TAG [99] can be used. For KG-based augmented retrieval,
GraphextQA [98] can be used.
B. Evaluations
Evaluating the results of different tasks related to LLM-
GGA is also a critical issue. Thus, selecting evaluation metrics
to assess the results is essential to determining how well LLMs
perform their understanding of graphs and how effectively
models combining graphs and LLMs perform on various tasks
is vital. This section summarizes the metrics of different tasks,
shown as Table VII. Note that all test results related to LLMs
TABLE V: A summary of LLM-GGA methods with datasets and source links.
Method
Dataset
LLM
Task
Link
InstrucGLM [78]
ogbn-arxiv, Cora, PubMed
Flan-T5 (instruction-finetune), Llama-
v1-7b (LoRA)
Link, Node
code link
GPT4Graph [23]
ogbn-arxiv,Aminer,Wiki,MetaQA
InstructGPT-3(frozen)
Reasoning, Node, Graph
code link
LLMtoGraph [24]
generated by GPTs
GPT-3.5-turbo, GPT-4, Wizard-Vicuna-
13B, 30B-Lazarus-Uncensored-HF
Multi-hop Reasoning
code link
Graph-LLM [73]
ogbn-arxiv, Cora, PubMed, ogbn-products
LLaMA,
text-ada-embedding-002,
Palm-Cortex-001
Node
code link
TAPE [30]
ogbn-arxiv, Cora, PubMed, ogbn-products
GPT-3.5
Node
code link
LLM4DyG [25]
LLM4DyG
GPT-3.5-turbo, Vicuna-7B, Vicuna-13B,
Llama-2-13B, CodeLlama-2-13B
Graph
-
GraphGPT [79]
ogbn-arxiv, Cora, PubMed
vicuna-7B-v1.1, vicuna-7B-v1.5
Node
code link
GPPT [80]
Cora,
Reddit,
CoraFull,
Amazon-CoBuy,
ogbn-arxiv etc.
-
Link, Node
code link
GraphPrompt [81]
Flickr, PROTEINS, COX2, ENZYMES, BZR
-
Link, Node, Graph
code link
All in one [82]
Cora, CiteSeer, Reddit, Amazon, Pubmed
-
Link, Edge, Node, Graph
code link
Graph-ToolFormer [59]
GPR, Cora, Pubmed, Citeseer, PROTEINS,
MUTAG, NCI1, PTC, Twitter, Foursquare
GPT-J-6B
Q&A, Reasoning
code link
RGV [83]
GSM8K, SVAMP, ASDiv-a
GPT-3.5-turbo
math problems
-
LLM-GNN [72]
CORA, CITESEER, PUBMED, WIKICS,
OGBN-ARXIV, OGBN-PRODUCTS
GPT-3.5-turbo
Node
code link
Which Modality should I use [87]
Cora, Citeseer, Pubmed,GraphTMI
GPT-4, GPT-4V
Representation, Node
-
WalkLM [38]
PubMed, MIMIC-III, Freebase, FB15K-237
PLMs
Representation, Node, Link
code link
GraphText [37]
Cora, Citeseer, Texas, Wisconsin, Cornell
Llama-2-7B
Node
code link
TALK LIKE A GRAPH [86]
GraphQA
PaLM 2-XXS, PaLM 62B
Node, Link
-
Graph-guided CoT [84]
2WikiMultihopQA, MusiQue, Bamboogle
Llama-2-13B,Llama-2-70B
multi-hop question answering
-
NLGraph [27]
NLGraph
TEXT-DAVINCI-003,
GPT-3.5-
TURBO, CODE-DAVINCI-002, GPT-4
Link,Node,Graph,Path,Pattern
code link
Collaborative Query Rewriting [94]
opportunity test sets, guardrail test set
Dolly V2
Conversational Understanding
-
WHEN AND WHY [26]
OGBN-ARXIV, CORA, PUBMED, OGBN-
PRODUCT, ARXIV-2023
ChatGPT
Node
code link
CR [35]
FOLIO, LogiQA, ProofWriter, LogicalDe-
duction
GPT-3.5-turbo,
GPT-4,
LLaMA-13B,
LLaMA-65B
Logic reasoning
code link
SOCIALSENSE [95]
RFPN, Twitter
PLMs
Response Forecasting
code link
DGTL [39]
Cora, PubMed, Books-History
Llama-2-13B
Node
-
SHEGO [96]
SGD, MultiWOZ 2.1
T5-small
multi-domain DST
-
Graph of Thought(GoT) [34]
individual data
GPT3.5(frozen)
Graph-formed reasoning
code link
GLEM [31]
ogbnarxiv, ogbn-products, ogbn-papers100M
PLMs
Node
code link
LPNL [77]
OAG
T5-base
Link
-
SIMTEG [71]
OGBN-Arxiv,
OGBN-Products,
OGBL-
Citation2
PLMs
Node, link
code link
Llmrec [44]
Netflix, MovieLens
gpt-3.5-turbo-16k
Recommendation
code link
ENG [75]
OGB
gpt-3.5-turbo
Node generation
-
OFA [32]
OGBN-ARXIV, CORA
PLMs
Node, link, graph
code link
G-prompt [33]
OGBN-ARXIV, Instagram, Reddit
PLMs
Representation
-
Beyond Text [74]
OGBN-ARXIV, CORA, PubMed
GPT-3.5, GPT-4
Node, link
-
GPT4GNAS [45]
OGBN-ARXIV, CORA, PubMed, Citeseer
GPT-4
Graph neural architecture search
-
Graphllm [64]
NLGraph
Llama2-7B, Llama2-13B
Link, node, graph, path, pattern
code link
G2P2 [85]
Cora, Amazon
PLMs
Representation
code link
ChatGraph [97]
Gradio
GPT-4V, Next-GPT
Link, node, graph, application
-
Graph Agent [76]
Cora, PubMed
GPT-4
Link, node, graph
-
GoT* [36]
AQUA-RAT, ScienceQA
T5-base
Graph-formed reasoning
code link
KGP [93]
HotpotQA, IIRC, 2WikiMQA, MuSiQue,
PDFTriage, Rank
Llama
KG+LLM
code link
Head-to-Tail [88]
DBpredia, Movie, Book, Academics
GPT-4
KG+LLM
-
GLaM [40]
DBLP, UMLS
Llama-7B
KG+LLM
-
ToG [41]
CWQ, WebQSP, GrailQA, QALD10-en, etc.
GPT-3.5, GPT-4, Llama-2
KG+LLM
code link
Autoalign [92]
DBpedia, Wikidata
ChatGPT, Claude
KG+LLM
code link
GNP [42]
ConceptNet, UMLS, OpenBookQA, etc.
FLAN-T5 xlarge (3B), xxlarge (11B)
KG+LLM
code link
RoG [43]
WebQSP, CWQ, Freebase
LLaMA2-7B
KG+LLM
code link
KGLens [91]
Wikidata
GPT-3.5-turbo,
GPT-4,
Babbage-002,
Davinci-002, Vicuna-33b-v1.3, Xwin-
LM-13B-V0.2, Yi-34B-Chat
KG+LLM
-
TABLE VI: A summary of new datasets.
New Benchmark
Link
GPR [59]
https://github.com/jwzhanggy/Graph Toolformer/tree/main/data
GraphTMI [87]
To be released
LLM4DyG [25]
To be released
GraphQA [86]
To be released
NLGraph [27]
https://github.com/Arthur-Heng/NLGraph/tree/main/NLGraph
GraphextQA [98]
https://huggingface.co/datasets/drt/graphext-qa
CS-TAG [99]
https://github.com/sktsherlock/TAG-Benchmark
TABLE VII: Evaluations.
Tasks
Metrics
Graph structure understanding task
Accuracy, ROUGE, BLEU, Time cost, Comprehension, Correctness, Fidelity, Rectification
Comprehension
Graph learning task
Accuracy, Macro-F1, Training Time, Tuned Parameters, GPU Occupy, Mismatch Rate,
Denial Rate, Token Limit Fraction
Graph resoning task
Accuracy, F1-score, Precision, Recall, The Latency-Volume Trade-off, Number of errors
and cost
Graph representation
depending on downstream tasks
KG-based augmented retrieval
Accuracy, F1-score, Precision, Recall,
Graph-LLM-based applications
depending on different tasks
in this paper are conducted using GPT-3.5 turbo or GPT-4
turbo.
1) Graph structure understanding task.: Several metrics are
usually used in graph structure understanding tasks: accuracy,
ROUGE [125], BLEU [126], Time cost, comprehension, cor-
rectness, fidelity, and rectification comprehension. Accuracy,
ROUGE, BLEU, and time cost are viral metrics. Meanwhile,
comprehension, correctness, fidelity, and rectification compre-
hension are new metrics [24] used to evaluate the ability
of LLMs to understand graphs through natural language,
the accuracy of solving graph problems, and the level of
confidence in the answers provided.
2) Graph learning task.: For graph learning tasks, when
evaluating a model, various metrics are considered to deter-
mine its effectiveness, efficiency, and computational demands.
When assessing the effectiveness of a model, metrics such
as accuracy, macro-F1, mismatch rate, and denial rate [87]
are considered. In terms of efficiency, metrics like training
time and tuned parameters are assessed. For computational
costs, metrics such as GPU occupancy and token limit fraction
are examined. Notably, the token limit fraction indicates the
proportion of tokens used compared to the maximum allowed
by the model’s constraints and can be formed as follows:
T =
Number of usage tokens
Token limit constraint for the model
(6)
3) Graph reasoning task.: When it comes to graph reason-
ing tasks, two main factors that are taken into consideration
are effectiveness and efficiency. Several metrics are used to
assess effectiveness, including accuracy, number of errors and
cost, F1-score, precision, and recall [127]. On the other hand,
efficiency is evaluated through metrics such as the Latency-
Volume Trade-off.
4) Graph representation.: The effectiveness of graph rep-
resentation is typically judged based on the performance of
downstream tasks that use this graph representation.
5) Knowledge graph-based augmented retrieval.: Tasks in
the KG-based augmented retrieval direction typically involve
question-answering tasks. Evaluation metrics commonly used
include accuracy, precision, recall, F1-score, Hits@k [128],
EM [129], MSE, and for some generative tasks, human eval-
uation may also be utilized.
X. FUTURE DIRECTIONS
The above survey of the state-of-the-art LLM-GGA research
reveals a promising and young research field. The following
section discusses exciting directions for future work.
A. More Complex Graph Problems
More complex graph tasks. Can LLMs solve graph al-
gorithm problems? Existing works on traditional graph tasks
are based on fundamental graph problems such as shortest
path, clustering coefficient computing, maximum flow, etc.
However, can LLMs address NP problems such as community
search, interactive graph problems, or even NP-hard problems,
and if so, how can they tackle them? For graph learning tasks,
current research primarily focuses on simple node, edge, and
graph classification. Future work can focus on more complex
graph learning problems, such as the diverse classification
outcomes arising from isomorphic and heterogeneous graphs.
More complex graph patterns. Graphs contain various
graph patterns, each with its explicit definition and unique
characteristics, such as stars, triangles, cliques, butterflies, and
more. Therefore, recognizing graph patterns and utilizing their
characteristics to solve downstream tasks can be highly advan-
tageous. Currently, only limited works leverage the properties
of stars, triangles, and cliques to solve problems.
Furthermore, understanding graph data still remains a sig-
nificant challenge for existing LLMs, limiting their ability to
tackle more complex graph problems. Therefore, incorporating
LLMs into the process is a promising direction for solving
more complex graph problems.
B. LLM Exploration on Diverse Graphs
Most existing work mainly focuses on static graphs, while
there exists a wide range of different graphs, including
undirected, directed, cyclic, acyclic, isomorphic, heteroge-
neous, dynamic, etc. Different types of graphs have significant
structural differences, such as static graphs, dynamic graphs,
temporal graphs, uncertain graphs, heterogeneous graphs, etc.
Specifically, unlike static graphs, dynamic graphs can be
represented as ordered lists or asynchronous streams of timed
events, capturing patterns of temporal network evolution, such
as the addition or removal of nodes and edges. Evaluating the
ability of LLMs to understand the spatio-temporal information
of dynamic graphs is crucial for web applications. Evaluating
whether LLMs can determine when nodes are connected,
identify which nodes are connected to a given node at a
specific time, and find a chronological path by combining
temporal and spatial information is essential to assessing
LLMs’ understanding of dynamic graphs. Future work can
further explore other types of graphs, such as dynamic graphs
and temporal graphs, address problems like maximum flow,
and predict the evolution of graphs.
Moreover, existing studies have conflicting views on the
LLM graph reasoning ability, with some presenting contradic-
tory findings. This ambiguity could be due to various factors,
including dataset selection, diverse prompt engineering tech-
niques, the range of graph reasoning tasks, and the utilization
of different LLM models.
C. Better LLM-GNN Pipelines
GNNs are designed to handle structural information by
continuously learning information from surrounding subgraphs
through aggregation functions. On the other hand, LLMs excel
in processing textual information, text reasoning, semantic
understanding, and more. The challenge lies in leveraging both
advantages to enable a pipeline that can effectively handle both
attributed and pure graphs. If GNNs and LLMs are simply
stacked, the parameter size of GNNs is notably smaller than
that of LLMs. This discrepancy may result in the issue of
vanishing gradients during training, as mentioned in [130],
which can impede the iterative updating process of GNNs.
Additionally, GNNs need to utilize the extensive knowledge
contained within LLMs fully, and they cannot effectively
extract specific knowledge tailored for specific downstream
tasks in different graphs.
D. Graph Foundation Model
LLM is undoubtedly the foundational model in NLP. Can
we draw inspiration from LLMs to train a graph foundation
model? For example, can training strategies like instruction
tuning and DPO be applied to tasks involving graphs? The
current research has primarily introduced graph foundation
models in the form of LLM-GNN pipelines and graph-aware
tuning LLMs. Future endeavors can focus on exploring graph
foundation models better suited for tasks involving graphs.
E. Better Graph Prompts
Most graph prompts are currently designed based on GNNs,
with only a few works focusing on LLMs. Graph prompts for
LLMs have yet to be sufficiently explored.
Graph Prompt for GNNs. The typical approach uses
simple concatenation, addition, or dot product operations with
trainable parameters. Some existing works have considered
more complex fusion methods, such as [82], which assumes
the structural features of graph prompts. However, compared
to the combination of prompts and pretexts, the variety of
graph prompts and pre-graphs is still in the exploratory stage.
Graph-enhanced Prompts for LLMs. Relying solely on
manual prompts and self-prompting has limited capabilities
in improving model performance, as they only explore the
existing abilities of LLM. As shown in Section III-C, LLMs
can be trained as agents to utilize tools for graph tasks that are
hard to solve, like API call prompt [59]. GoT [130] is also
a graph reasoning paradigm that enables LLMs to provide
correct answers. Future work based on the graph reasoning
paradigm can consider cost-effective approaches for GoT,
such as pruning and tricks to reduce algorithm complexity.
In the future, it would be beneficial to explore simpler GoT
paradigms that can improve the effectiveness of LLMs.
F. Modal Alignment
Modal alignment refers to the alignment between two
modalities: text and graph. The input for LLMs is typically
sequential data, often text. Graph and text are two different
modalities, and studying the alignment between these two
modalities for LLMs involves finding a shared mapping feature
space for graphs and text. The shared mapping space allows
LLMs to understand graph data similarly to how they know
textual information if they comprehend text.
G. Explainabilily
GNNs are currently widely used for solving complex graph
problems. However, they need more interpretability, which
hinders their practical application. On the other hand, LLMs
possess reasoning capabilities and have succeeded in various
natural language processing tasks. The combination of LLMs
and GNNs has the potential to offer a more transparent ap-
proach to solving graph problems by leveraging the reasoning
abilities of LLMs. If the combination of LLMs and GNNs
is interpretable, it can be utilized for various tasks., including
recommendation systems, drug discovery, and fraud detection.
This combination can lead to the development of more reliable
and efficient decision-making systems across various domains.
H. Efficiency on Large-scale Graphs
Due to the limited input length of LLM, the graph sizes
inputted through prompts typically consist of dozens of nodes.
However, for large graphs with tens of thousands of nodes and
edges, how can LLMs with limited input length solve such
large graphs? A larger input window is required in the case of
attributed graphs, where both node and edge attributes need to
be considered along with the graph structure. How does LLM
address this case? There are currently few effective methods
to enable LLM to handle them.
XI. CONCLUSIONS
LLM-GGA has emerged as a promising field that has
garnered significant attention from researchers. This paper
introduces a comprehensive structural taxonomy based on
recent research, which classifies LLM-GGA research into three
main directions: LLM-GQP, LLM-GIL, and graph-LLM-based
applications. LLM-GQP encompasses graph understanding
and KG-based augmented retrieval, while LLM-GIL involves
graph learning, graph-formed reasoning, and graph represen-
tation. The motivation, challenges, and mainstream methods
of each direction are thoroughly examined.
For the six mentioned directions, a comparison of various
methods was conducted to explore their potential in each
area. It is observed that LLM shows preliminary capabilities
in structural understanding, addressing issues like maximum
flow and bipartite graph matching over small graphs. However,
it is susceptible to factors such as node degree and graph
density, leading to potential misjudgments in graph connec-
tivity. Additionally, LLM proves beneficial for graph learning
tasks due to its strong semantic understanding and reasoning
abilities, coupled with learning from extensive corpora, which
can provide external knowledge to GNNs and aid in semantic
information comprehension, learning, and reasoning. Thanks
to LLM’s semantic understanding capabilities, graph represen-
tation can achieve deeper semantic embeddings. The discus-
sion also delves into KG-based augmented retrieval to enhance
LLMs retrieval and factual knowledge-answering abilities. The
paper summarizes over 40 datasets, evaluation metrics for six
directions, and source code for over 30 mainstream methods in
these directions. It highlights the existing challenges in current
methods and proposes future directions to guide and motivate
further research in the LLM-GGA field.
REFERENCES
[1] J. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester,
N. Du, A. M. Dai, and Q. V. Le, “Finetuned Language Models
Are Zero-Shot Learners,” Feb. 2022, arXiv:2109.01652 [cs]. [Online].
Available: http://arxiv.org/abs/2109.01652
[2] B. Peng, C. Li, P. He, M. Galley, and J. Gao, “Instruction Tuning
with GPT-4,” Apr. 2023, arXiv:2304.03277 [cs]. [Online]. Available:
http://arxiv.org/abs/2304.03277
[3] R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon,
and
C.
Finn,
“Direct
preference
optimization:
Your
language
model
is
secretly
a
reward
model,”
in
Advances
in
Neural
Information Processing Systems 36: Annual Conference on Neural
Information Processing Systems 2023, NeurIPS 2023, New Orleans,
LA,
USA,
December
10
-
16,
2023,
A.
Oh,
T.
Naumann,
A. Globerson, K. Saenko, M. Hardt, and S. Levine, Eds., 2023.
[Online]. Available: http://papers.nips.cc/paper files/paper/2023/hash/
a85b405ed65c6477a4fe8302b5e06ce7-Abstract-Conference.html
[4] L. Sun, Y. Huang, H. Wang, S. Wu, Q. Zhang, Y. Li, C. Gao, Y. Huang,
W. Lyu, Y. Zhang, X. Li, Z. Liu, Y. Liu, Y. Wang, Z. Zhang, B. Vidgen,
B. Kailkhura, C. Xiong, C. Xiao, C. Li, E. Xing, F. Huang, H. Liu,
H. Ji, H. Wang, H. Zhang, H. Yao, M. Kellis, M. Zitnik, M. Jiang,
M. Bansal, J. Zou, J. Pei, J. Liu, J. Gao, J. Han, J. Zhao, J. Tang,
J. Wang, J. Vanschoren, J. Mitchell, K. Shu, K. Xu, K.-W. Chang,
L. He, L. Huang, M. Backes, N. Z. Gong, P. S. Yu, P.-Y. Chen, Q. Gu,
R. Xu, R. Ying, S. Ji, S. Jana, T. Chen, T. Liu, T. Zhou, W. Wang,
X. Li, X. Zhang, X. Wang, X. Xie, X. Chen, X. Wang, Y. Liu,
Y. Ye, Y. Cao, Y. Chen, and Y. Zhao, “TrustLLM: Trustworthiness in
Large Language Models,” Mar. 2024, arXiv:2401.05561 [cs]. [Online].
Available: http://arxiv.org/abs/2401.05561
[5] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei,
N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher,
C. Canton-Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes,
J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn,
S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa,
I. Kloumann, A. Korenev, P. S. Koura, M. Lachaux, T. Lavril, J. Lee,
D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra,
I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi,
A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan,
B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov,
Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic,
S. Edunov, and T. Scialom, “Llama 2: Open foundation and fine-tuned
chat models,” CoRR, vol. abs/2307.09288, 2023. [Online]. Available:
https://doi.org/10.48550/arXiv.2307.09288
[6] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright,
P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman,
J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder,
P.
F.
Christiano,
J.
Leike,
and
R.
Lowe,
“Training
language
models to follow instructions with human feedback,” in Advances in
Neural Information Processing Systems 35: Annual Conference on
Neural Information Processing Systems 2022, NeurIPS 2022, New
Orleans, LA, USA, November 28 - December 9, 2022, S. Koyejo,
S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, Eds.,
2022. [Online]. Available: http://papers.nips.cc/paper files/paper/2022/
hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html
[7] Y. Zhuang, Y. Yu, K. Wang, H. Sun, and C. Zhang, “Toolqa:
A dataset for LLM question answering with external tools,” in
Advances in Neural Information Processing Systems 36: Annual
Conference on Neural Information Processing Systems 2023, NeurIPS
2023, New Orleans, LA, USA, December 10 - 16, 2023, A. Oh,
T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine,
Eds., 2023. [Online]. Available: http://papers.nips.cc/paper files/paper/
2023/hash/9cb2a7495900f8b602cb10159246a016-Abstract-Datasets
and Benchmarks.html
[8] Z. Li, S. Fan, Y. Gu, X. Li, Z. Duan, B. Dong, N. Liu, and J. Wang,
“Flexkbqa: A flexible llm-powered framework for few-shot knowledge
base question answering,” in Proceedings of the AAAI Conference on
Artificial Intelligence, vol. 38, no. 17, 2024, pp. 18 608–18 616.
[9] B. Zhang, B. Haddow, and A. Birch, “Prompting large language model
for machine translation: A case study,” in International Conference on
Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii,
USA, ser. Proceedings of Machine Learning Research, A. Krause,
E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett,
Eds., vol. 202.
PMLR, 2023, pp. 41 092–41 110. [Online]. Available:
https://proceedings.mlr.press/v202/zhang23m.html
[10] J.
Liu,
C.
S.
Xia,
Y.
Wang,
and
L.
Zhang,
“Is
your
code
generated by chatgpt really correct? rigorous evaluation of large
language models for code generation,” in Advances in Neural
Information Processing Systems 36: Annual Conference on Neural
Information Processing Systems 2023, NeurIPS 2023, New Orleans,
LA,
USA,
December
10
-
16,
2023,
A.
Oh,
T.
Naumann,
A. Globerson, K. Saenko, M. Hardt, and S. Levine, Eds., 2023.
[Online]. Available: http://papers.nips.cc/paper files/paper/2023/hash/
43e9d647ccd3e4b7b5baab53f0368686-Abstract-Conference.html
[11] A. Ni, S. Iyer, D. Radev, V. Stoyanov, W. Yih, S. I. Wang,
and
X.
V.
Lin,
“LEVER:
learning
to
verify
language-to-code
generation with execution,” in International Conference on Machine
Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA,
ser.
Proceedings
of
Machine
Learning
Research,
A.
Krause,
E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett,
Eds., vol. 202.
PMLR, 2023, pp. 26 106–26 128. [Online]. Available:
https://proceedings.mlr.press/v202/ni23b.html
[12] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Gallagher, and
T. Eliassi-Rad, “Collective Classification in Network Data,” AI
Magazine, vol. 29, no. 3, pp. 93–106, Sep. 2008. [Online]. Available:
https://onlinelibrary.wiley.com/doi/10.1609/aimag.v29i3.2157
[13] W. L. Hamilton, Z. Ying, and J. Leskovec, “Inductive representation
learning on large graphs,” in Advances in Neural Information
Processing Systems 30: Annual Conference on Neural Information
Processing Systems 2017, December 4-9, 2017, Long Beach, CA,
USA, I. Guyon, U. von Luxburg, S. Bengio, H. M. Wallach,
R. Fergus, S. V. N. Vishwanathan, and R. Garnett, Eds., 2017, pp.
1024–1034. [Online]. Available: https://proceedings.neurips.cc/paper/
2017/hash/5dd9db5e033da9c6fb5ba83c7a7ebea9-Abstract.html
[14] Z. Wu, B. Ramsundar, E. N. Feinberg, J. Gomes, C. Geniesse, A. S.
Pappu, K. Leswing, and V. Pande, “Moleculenet: a benchmark for
molecular machine learning,” Chemical science, vol. 9, no. 2, pp. 513–
530, 2018.
[15] A. Broder, R. Kumar, F. Maghoul, P. Raghavan, S. Rajagopalan,
R. Stata, A. Tomkins, and J. Wiener, “Graph structure in the
Web,” Computer Networks, vol. 33, no. 1-6, pp. 309–320, Jun.
2000. [Online]. Available: https://linkinghub.elsevier.com/retrieve/pii/
S1389128600000839
[16] T. N. Kipf and M. Welling, “Semi-supervised classification with graph
convolutional networks,” in 5th International Conference on Learning
Representations, ICLR 2017, Toulon, France, April 24-26, 2017,
Conference Track Proceedings.
OpenReview.net, 2017. [Online].
Available: https://openreview.net/forum?id=SJU4ayYgl
[17] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Li`
o,
and Y. Bengio, “Graph attention networks,” in 6th International
Conference on Learning Representations, ICLR 2018, Vancouver, BC,
Canada, April 30 - May 3, 2018, Conference Track Proceedings.
OpenReview.net, 2018. [Online]. Available: https://openreview.net/
forum?id=rJXMpikCZ
[18] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl,
“Neural message passing for quantum chemistry,” in Proceedings
of the 34th International Conference on Machine Learning, ICML
2017, Sydney, NSW, Australia, 6-11 August 2017, ser. Proceedings
of
Machine
Learning
Research,
D.
Precup
and
Y.
W.
Teh,
Eds., vol. 70.
PMLR, 2017, pp. 1263–1272. [Online]. Available:
http://proceedings.mlr.press/v70/gilmer17a.html
[19] Y. Hong, J. W. Lam, and B. Z. Tang, “Aggregation-induced emission:
phenomenon, mechanism and applications,” Chemical communications,
no. 29, pp. 4332–4353, 2009.
[20] W. Cong, M. Ramezani, and M. Mahdavi, “On provable benefits
of depth in training graph convolutional networks,” in Advances
in Neural Information Processing Systems 34: Annual Conference
on Neural Information Processing Systems 2021, NeurIPS 2021,
December 6-14, 2021, virtual, M. Ranzato, A. Beygelzimer, Y. N.
Dauphin, P. Liang, and J. W. Vaughan, Eds., 2021, pp. 9936–
9949. [Online]. Available: https://proceedings.neurips.cc/paper/2021/
hash/524265e8b942930fbbe8a5d979d29205-Abstract.html
[21] S. Fan, X. Wang, C. Shi, P. Cui, and B. Wang, “Generalizing Graph
Neural Networks on Out-of-Distribution Graphs,” IEEE Transactions
on Pattern Analysis and Machine Intelligence, vol. 46, no. 1, pp.
322–337, Jan. 2024. [Online]. Available: https://ieeexplore.ieee.org/
document/10268633/
[22] J. Liu, Z. Shen, Y. He, X. Zhang, R. Xu, H. Yu, and P. Cui,
“Towards Out-Of-Distribution Generalization: A Survey,” Jul. 2023,
arXiv:2108.13624 [cs]. [Online]. Available: http://arxiv.org/abs/2108.
13624
[23] J. Guo, L. Du, H. Liu, M. Zhou, X. He, and S. Han, “GPT4Graph:
Can Large Language Models Understand Graph Structured Data ? An
Empirical Evaluation and Benchmarking,” Jul. 2023, arXiv:2305.15066
[cs]. [Online]. Available: http://arxiv.org/abs/2305.15066
[24] C.
Liu
and
B.
Wu,
“Evaluating
Large
Language
Models
on
Graphs: Performance Insights and Comparative Analysis,” Sep. 2023,
arXiv:2308.11224 [cs]. [Online]. Available: http://arxiv.org/abs/2308.
11224
[25] Z. Zhang, X. Wang, Z. Zhang, H. Li, Y. Qin, and W. Zhu,
“LLM4DyG: Can Large Language Models Solve Spatial-Temporal
Problems on Dynamic Graphs?” Mar. 2024, arXiv:2310.17110 [cs].
[Online]. Available: http://arxiv.org/abs/2310.17110
[26] J. Huang, X. Zhang, Q. Mei, and J. Ma, “Can llms effectively
leverage graph structural information: When and why,” CoRR, vol.
abs/2309.16595, 2023. [Online]. Available: https://doi.org/10.48550/
arXiv.2309.16595
[27] H. Wang, S. Feng, T. He, Z. Tan, X. Han, and Y. Tsvetkov, “Can
Language Models Solve Graph Problems in Natural Language?” Jan.
2024, arXiv:2305.10037 [cs]. [Online]. Available: http://arxiv.org/abs/
2305.10037
[28] Q. Dong, L. Dong, K. Xu, G. Zhou, Y. Hao, Z. Sui, and
F. Wei, “Large Language Model for Science: A Study on P
vs. NP,” Sep. 2023, arXiv:2309.05689 [cs]. [Online]. Available:
http://arxiv.org/abs/2309.05689
[29] L. Fan, W. Hua, L. Li, H. Ling, and Y. Zhang, “NPHardEval:
Dynamic Benchmark on Reasoning Ability of Large Language Models
via Complexity Classes,” Feb. 2024, arXiv:2312.14890 [cs]. [Online].
Available: http://arxiv.org/abs/2312.14890
[30] X.
He,
X.
Bresson,
T.
Laurent,
A.
Perold,
Y.
LeCun,
and
B. Hooi, “Harnessing Explanations: LLM-to-LM Interpreter for
Enhanced Text-Attributed Graph Representation Learning,” Mar. 2024,
arXiv:2305.19523 [cs]. [Online]. Available: http://arxiv.org/abs/2305.
19523
[31] J. Zhao, M. Qu, C. Li, H. Yan, Q. Liu, R. Li, X. Xie, and J. Tang,
“Learning on Large-scale Text-attributed Graphs via Variational
Inference,” Mar. 2023, arXiv:2210.14709 [cs]. [Online]. Available:
http://arxiv.org/abs/2210.14709
[32] H. Liu, J. Feng, L. Kong, N. Liang, D. Tao, Y. Chen, and
M. Zhang, “One for All: Towards Training One Graph Model for
All Classification Tasks,” Dec. 2023, arXiv:2310.00149 [cs]. [Online].
Available: http://arxiv.org/abs/2310.00149
[33] X. Huang, K. Han, D. Bao, Q. Tao, Z. Zhang, Y. Yang, and Q. Zhu,
“Prompt-based Node Feature Extractor for Few-shot Learning on
Text-Attributed Graphs,” Sep. 2023, arXiv:2309.02848 [cs]. [Online].
Available: http://arxiv.org/abs/2309.02848
[34] M. Besta, N. Blach, A. Kubicek, R. Gerstenberger, M. Podstawski,
L. Gianinazzi, J. Gajda, T. Lehmann, H. Niewiadomski, P. Nyczyk,
and T. Hoefler, “Graph of Thoughts: Solving Elaborate Problems with
Large Language Models,” Proceedings of the AAAI Conference on
Artificial Intelligence, vol. 38, no. 16, pp. 17 682–17 690, Mar. 2024,
arXiv:2308.09687 [cs]. [Online]. Available: http://arxiv.org/abs/2308.
09687
[35] Y. Zhang, J. Yang, Y. Yuan, and A. C.-C. Yao, “Cumulative Reasoning
with Large Language Models,” Apr. 2024, arXiv:2308.04371 [cs].
[Online]. Available: http://arxiv.org/abs/2308.04371
[36] Y. Yao, Z. Li, and H. Zhao, “Beyond Chain-of-Thought, Effective
Graph-of-Thought Reasoning in Language Models,” Mar. 2024,
arXiv:2305.16582 [cs]. [Online]. Available: http://arxiv.org/abs/2305.
16582
[37] J.
Zhao,
L.
Zhuo,
Y.
Shen,
M.
Qu,
K.
Liu,
M.
Bronstein,
Z.
Zhu,
and
J.
Tang,
“GraphText:
Graph
Reasoning
in
Text
Space,”
Oct.
2023,
arXiv:2310.01089
[cs].
[Online].
Available:
http://arxiv.org/abs/2310.01089
[38] Y.
Tan,
Z.
Zhou,
H.
Lv,
W.
Liu,
and
C.
Yang,
“Walklm: A uniform language model fine-tuning framework for
attributed graph embedding,” in Advances in Neural Information
Processing Systems 36: Annual Conference on Neural Information
Processing
Systems
2023,
NeurIPS
2023,
New
Orleans,
LA,
USA,
December
10
-
16,
2023,
A.
Oh,
T.
Naumann,
A.
Globerson,
K.
Saenko,
M.
Hardt,
and
S.
Levine,
Eds.,
2023. [Online]. Available: http://papers.nips.cc/paper files/paper/2023/
hash/2ac879d1865475a7abc8dfc7a9c15c27-Abstract-Conference.html
[39] Y. Qin, X. Wang, Z. Zhang, and W. Zhu, “Disentangled Representation
Learning with Large Language Models for Text-Attributed Graphs,”
Mar. 2024, arXiv:2310.18152 [cs]. [Online]. Available: http://arxiv.
org/abs/2310.18152
[40] S. Dernbach, K. Agarwal, A. Zuniga, M. Henry, and S. Choudhury,
“GLaM: Fine-Tuning Large Language Models for Domain Knowledge
Graph Alignment via Neighborhood Partitioning and Generative
Subgraph Encoding,” Apr. 2024, arXiv:2402.06764 [cs]. [Online].
Available: http://arxiv.org/abs/2402.06764
[41] J. Sun, C. Xu, L. Tang, S. Wang, C. Lin, Y. Gong, L. M. Ni,
H.-Y. Shum, and J. Guo, “Think-on-Graph: Deep and Responsible
Reasoning of Large Language Model on Knowledge Graph,” Mar.
2024, arXiv:2307.07697 [cs]. [Online]. Available: http://arxiv.org/abs/
2307.07697
[42] Y. Tian, H. Song, Z. Wang, H. Wang, Z. Hu, F. Wang, N. V.
Chawla, and P. Xu, “Graph Neural Prompting with Large Language
Models,” Dec. 2023, arXiv:2309.15427 [cs]. [Online]. Available:
http://arxiv.org/abs/2309.15427
[43] L. Luo, Y.-F. Li, G. Haffari, and S. Pan, “Reasoning on Graphs:
Faithful and Interpretable Large Language Model Reasoning,” Feb.
2024, arXiv:2310.01061 [cs]. [Online]. Available: http://arxiv.org/abs/
2310.01061
[44] W. Wei, X. Ren, J. Tang, Q. Wang, L. Su, S. Cheng, J. Wang,
D. Yin, and C. Huang, “LLMRec: Large Language Models with Graph
Augmentation for Recommendation,” Jan. 2024, arXiv:2311.00423
[cs]. [Online]. Available: http://arxiv.org/abs/2311.00423
[45] H.
Wang,
Y.
Gao,
X.
Zheng,
P.
Zhang,
H.
Chen,
J.
Bu,
and P. S. Yu, “Graph Neural Architecture Search with GPT-
4,” Mar. 2024, arXiv:2310.01436 [cs]. [Online]. Available: http:
//arxiv.org/abs/2310.01436
[46] L. Wu, Z. Qiu, Z. Zheng, H. Zhu, and E. Chen, “Exploring
large language model for graph data understanding in online job
recommendations,” in Thirty-Eighth AAAI Conference on Artificial
Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative
Applications
of
Artificial
Intelligence,
IAAI
2024,
Fourteenth
Symposium on Educational Advances in Artificial Intelligence, EAAI
2014, February 20-27, 2024, Vancouver, Canada, M. J. Wooldridge,
J. G. Dy, and S. Natarajan, Eds.
AAAI Press, 2024, pp. 9178–9186.
[Online]. Available: https://doi.org/10.1609/aaai.v38i8.28769
[47] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min,
B. Zhang, J. Zhang, Z. Dong, Y. Du, C. Yang, Y. Chen, Z. Chen,
J. Jiang, R. Ren, Y. Li, X. Tang, Z. Liu, P. Liu, J.-Y. Nie, and J.-R. Wen,
“A Survey of Large Language Models,” Nov. 2023, arXiv:2303.18223
[cs]. [Online]. Available: http://arxiv.org/abs/2303.18223
[48] J. Yang, H. Jin, R. Tang, X. Han, Q. Feng, H. Jiang, B. Yin, and
X. Hu, “Harnessing the power of llms in practice: A survey on chatgpt
and beyond,” CoRR, vol. abs/2304.13712, 2023. [Online]. Available:
https://doi.org/10.48550/arXiv.2304.13712
[49] M. Himsolt, “Gml: A portable graph file format,” Technical report,
Universitat Passau, Tech. Rep., 1997.
[50] U. Brandes, M. Eiglsperger, J. Lerner, and C. Pich, “Graph markup lan-
guage (graphml),” in Handbook on Graph Drawing and Visualization,
R. Tamassia, Ed.
Chapman and Hall/CRC, 2013, pp. 517–541.
[51] N. Francis, A. Green, P. Guagliardo, L. Libkin, T. Lindaaker,
V. Marsault, S. Plantikow, M. Rydberg, P. Selmer, and A. Taylor,
“Cypher: An Evolving Query Language for Property Graphs,” in
Proceedings of the 2018 International Conference on Management of
Data.
Houston TX USA: ACM, May 2018, pp. 1433–1445. [Online].
Available: https://dl.acm.org/doi/10.1145/3183713.3190657
[52] M. A. Rodriguez, “The Gremlin graph traversal machine and language
(invited talk),” in Proceedings of the 15th Symposium on Database
Programming Languages.
Pittsburgh PA USA: ACM, Oct. 2015,
pp. 1–10. [Online]. Available: https://dl.acm.org/doi/10.1145/2815072.
2815073
[53] J. P´
erez, M. Arenas, and C. Gutierrez, “Semantics and complexity of
SPARQL,” ACM Transactions on Database Systems, vol. 34, no. 3,
pp. 1–45, Aug. 2009. [Online]. Available: https://dl.acm.org/doi/10.
1145/1567274.1567278
[54] B. Wang, R. Shin, X. Liu, O. Polozov, and M. Richardson, “RAT-
SQL: Relation-Aware Schema Encoding and Linking for Text-to-SQL
Parsers,” Aug. 2021, arXiv:1911.04942 [cs]. [Online]. Available:
http://arxiv.org/abs/1911.04942
[55] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig,
“Pre-train, Prompt, and Predict: A Systematic Survey of Prompting
Methods in Natural Language Processing,” ACM Computing Surveys,
vol. 55, no. 9, pp. 1–35, Sep. 2023. [Online]. Available: https:
//dl.acm.org/doi/10.1145/3560815
[56] W. Hu, M. Fey, M. Zitnik, Y. Dong, H. Ren, B. Liu, M. Catasta,
and J. Leskovec, “Open graph benchmark: Datasets for machine
learning on graphs,” in Advances in Neural Information Processing
Systems 33: Annual Conference on Neural Information Processing
Systems
2020,
NeurIPS
2020,
December
6-12,
2020,
virtual,
H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds.,
2020. [Online]. Available: https://proceedings.neurips.cc/paper/2020/
hash/fb60d411a5c5b72b2e7d3527cfc84fd0-Abstract.html
[57] J.
Tang,
J.
Zhang,
L.
Yao,
J.
Li,
L.
Zhang,
and
Z.
Su,
“ArnetMiner: extraction and mining of academic social networks,”
in Proceedings of the 14th ACM SIGKDD international conference
on Knowledge discovery and data mining.
Las Vegas Nevada
USA: ACM, Aug. 2008, pp. 990–998. [Online]. Available: https:
//dl.acm.org/doi/10.1145/1401890.1402008
[58] T. Schick, J. Dwivedi-Yu, R. Dess`
ı, R. Raileanu, M. Lomeli,
E.
Hambro,
L.
Zettlemoyer,
N.
Cancedda,
and
T.
Scialom,
“Toolformer: Language models can teach themselves to use tools,”
in Advances in Neural Information Processing Systems 36: Annual
Conference on Neural Information Processing Systems 2023, NeurIPS
2023, New Orleans, LA, USA, December 10 - 16, 2023, A. Oh,
T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, Eds.,
2023. [Online]. Available: http://papers.nips.cc/paper files/paper/2023/
hash/d842425e4bf79ba039352da0f658a906-Abstract-Conference.html
[59] J. Zhang, “Graph-ToolFormer: To Empower LLMs with Graph
Reasoning Ability via Prompt Augmented by ChatGPT,” May 2023,
arXiv:2304.11116 [cs]. [Online]. Available: http://arxiv.org/abs/2304.
11116
[60] H. Face, “hivemind/gpt-j-6b-8bit.”
[61] B. Wang and A. Komatsuzaki, “Gpt-j-6b: A 6 billion parameter
autoregressive language model,” 2021.
[62] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,
T. Lacroix, B. Rozi`
ere, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez,
A. Joulin, E. Grave, and G. Lample, “LLaMA: Open and Efficient
Foundation Language Models,” 2023, publisher: [object Object]
Version Number: 1. [Online]. Available: https://arxiv.org/abs/2302.
13971
[63] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang,
and W. Chen, “LoRA: Low-Rank Adaptation of Large Language
Models,” Oct. 2021, arXiv:2106.09685 [cs]. [Online]. Available:
http://arxiv.org/abs/2106.09685
[64] Z. Chai, T. Zhang, L. Wu, K. Han, X. Hu, X. Huang, and Y. Yang,
“GraphLLM: Boosting Graph Reasoning Ability of Large Language
Model,” Oct. 2023, arXiv:2310.05845 [cs]. [Online]. Available:
http://arxiv.org/abs/2310.05845
[65] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia,
E. H. Chi, Q. V. Le, and D. Zhou, “Chain-of-thought prompting
elicits reasoning in large language models,” in Advances in Neural
Information Processing Systems 35: Annual Conference on Neural
Information Processing Systems 2022, NeurIPS 2022, New Orleans,
LA, USA, November 28 - December 9, 2022, S. Koyejo, S. Mohamed,
A.
Agarwal,
D.
Belgrave,
K.
Cho,
and
A.
Oh,
Eds.,
2022.
[Online]. Available: http://papers.nips.cc/paper files/paper/2022/hash/
9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html
[66] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowd-
hery, and D. Zhou, “Self-Consistency Improves Chain of Thought
Reasoning in Language Models,” Mar. 2023, arXiv:2203.11171 [cs].
[Online]. Available: http://arxiv.org/abs/2203.11171
[67] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun, J. Xu,
L. Li, and Z. Sui, “A Survey on In-context Learning,” Jun. 2023,
arXiv:2301.00234 [cs]. [Online]. Available: http://arxiv.org/abs/2301.
00234
[68] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Prox-
imal Policy Optimization Algorithms,” Aug. 2017, arXiv:1707.06347
[cs]. [Online]. Available: http://arxiv.org/abs/1707.06347
[69] H. Dong, W. Xiong, D. Goyal, Y. Zhang, W. Chow, R. Pan,
S. Diao, J. Zhang, K. Shum, and T. Zhang, “RAFT: Reward
rAnked FineTuning for Generative Foundation Model Alignment,”
Dec. 2023, arXiv:2304.06767 [cs, stat]. [Online]. Available: http:
//arxiv.org/abs/2304.06767
[70] F. Song, B. Yu, M. Li, H. Yu, F. Huang, Y. Li, and H. Wang, “Preference
Ranking Optimization for Human Alignment,” Proceedings of the
AAAI Conference on Artificial Intelligence, vol. 38, no. 17, pp.
18 990–18 998, Mar. 2024. [Online]. Available: https://ojs.aaai.org/
index.php/AAAI/article/view/29865
[71] K. Duan, Q. Liu, T.-S. Chua, S. Yan, W. T. Ooi, Q. Xie, and J. He,
“SimTeG: A Frustratingly Simple Approach Improves Textual Graph
Learning,” Aug. 2023, arXiv:2308.02565 [cs]. [Online]. Available:
http://arxiv.org/abs/2308.02565
[72] Z. Chen, H. Mao, H. Wen, H. Han, W. Jin, H. Zhang, H. Liu, and
J. Tang, “Label-free node classification on graphs with large language
models
(LLMS),”
CoRR,
vol.
abs/2310.04668,
2023.
[Online].
Available: https://doi.org/10.48550/arXiv.2310.04668
[73] Z. Chen, H. Mao, H. Li, W. Jin, H. Wen, X. Wei, S. Wang,
D. Yin, W. Fan, H. Liu, and J. Tang, “Exploring the Potential of
Large Language Models (LLMs) in Learning on Graphs,” Jan. 2024,
arXiv:2307.03393 [cs]. [Online]. Available: http://arxiv.org/abs/2307.
03393
[74] Y. Hu, Z. Zhang, and L. Zhao, “Beyond Text: A Deep Dive
into Large Language Models’ Ability on Understanding Graph
Data,”
Oct.
2023,
arXiv:2310.04944
[cs].
[Online].
Available:
http://arxiv.org/abs/2310.04944
[75] J. Yu, Y. Ren, C. Gong, J. Tan, X. Li, and X. Zhang, “Empower
Text-Attributed
Graphs
Learning
with
Large
Language
Models
(LLMs),” Oct. 2023, arXiv:2310.09872 [cs]. [Online]. Available:
http://arxiv.org/abs/2310.09872
[76] Q. Wang, Z. Gao, and R. Xu, “Graph agent: Explicit reasoning agent
for graphs,” CoRR, vol. abs/2310.16421, 2023. [Online]. Available:
https://doi.org/10.48550/arXiv.2310.16421
[77] B. Bi, S. Liu, Y. Wang, L. Mei, and X. Cheng, “LPNL: Scalable Link
Prediction with Large Language Models,” Feb. 2024, arXiv:2401.13227
[cs]. [Online]. Available: http://arxiv.org/abs/2401.13227
[78] R. Ye, C. Zhang, R. Wang, S. Xu, and Y. Zhang, “Language is All a
Graph Needs,” Feb. 2024, arXiv:2308.07134 [cs]. [Online]. Available:
http://arxiv.org/abs/2308.07134
[79] J. Tang, Y. Yang, W. Wei, L. Shi, L. Su, S. Cheng, D. Yin, and
C. Huang, “GraphGPT: Graph Instruction Tuning for Large Language
Models,” Dec. 2023, arXiv:2310.13023 [cs]. [Online]. Available:
http://arxiv.org/abs/2310.13023
[80] M. Sun, K. Zhou, X. He, Y. Wang, and X. Wang, “GPPT:
Graph Pre-training and Prompt Tuning to Generalize Graph Neural
Networks,” in Proceedings of the 28th ACM SIGKDD Conference
on
Knowledge
Discovery
and
Data
Mining.
Washington
DC
USA:
ACM,
Aug.
2022,
pp.
1717–1727.
[Online].
Available:
https://dl.acm.org/doi/10.1145/3534678.3539249
[81] Z. Liu, X. Yu, Y. Fang, and X. Zhang, “GraphPrompt: Unifying
Pre-Training and Downstream Tasks for Graph Neural Networks,”
in
Proceedings
of
the
ACM
Web
Conference
2023.
Austin
TX USA: ACM, Apr. 2023, pp. 417–428. [Online]. Available:
https://dl.acm.org/doi/10.1145/3543507.3583386
[82] X. Sun, H. Cheng, J. Li, B. Liu, and J. Guan, “All in One: Multi-Task
Prompting for Graph Neural Networks,” in Proceedings of the 29th
ACM SIGKDD Conference on Knowledge Discovery and Data Mining.
Long Beach CA USA: ACM, Aug. 2023, pp. 2120–2131. [Online].
Available: https://dl.acm.org/doi/10.1145/3580305.3599256
[83] L. Cao, “GraphReason: Enhancing Reasoning Capabilities of Large
Language Models through A Graph-Based Verification Approach,”
Apr. 2024, arXiv:2308.09267 [cs]. [Online]. Available: http://arxiv.
org/abs/2308.09267
[84] J. Park, A. Patel, O. Z. Khan, H. J. Kim, and J.-K. Kim, “Graph-
Guided Reasoning for Multi-Hop Question Answering in Large
Language Models,” Nov. 2023, arXiv:2311.09762 [cs]. [Online].
Available: http://arxiv.org/abs/2311.09762
[85] Z. Wen and Y. Fang, “Augmenting Low-Resource Text Classification
with Graph-Grounded Pre-training and Prompting,” in Proceedings
of the 46th International ACM SIGIR Conference on Research
and Development in Information Retrieval, Jul. 2023, pp. 506–516,
arXiv:2305.03324 [cs]. [Online]. Available: http://arxiv.org/abs/2305.
03324
[86] B. Fatemi, J. Halcrow, and B. Perozzi, “Talk like a Graph: Encoding
Graphs for Large Language Models,” Oct. 2023, arXiv:2310.04560
[cs]. [Online]. Available: http://arxiv.org/abs/2310.04560
[87] D. Das, I. Gupta, J. Srivastava, and D. Kang, “Which Modality
should I use – Text, Motif, or Image? : Understanding Graphs with
Large Language Models,” Mar. 2024, arXiv:2311.09862 [cs]. [Online].
Available: http://arxiv.org/abs/2311.09862
[88] K. Sun, Y. E. Xu, H. Zha, Y. Liu, and X. L. Dong, “Head-to-Tail: How
Knowledgeable are Large Language Models (LLMs)? A.K.A. Will
LLMs Replace Knowledge Graphs?” Apr. 2024, arXiv:2308.10168
[cs]. [Online]. Available: http://arxiv.org/abs/2308.10168
[89] L. Yang, H. Chen, Z. Li, X. Ding, and X. Wu, “Give us the facts:
Enhancing large language models with knowledge graphs for fact-
aware language modeling,” IEEE Transactions on Knowledge and Data
Engineering, 2024.
[90] S. Pan, L. Luo, Y. Wang, C. Chen, J. Wang, and X. Wu, “Unifying
large language models and knowledge graphs: A roadmap,” CoRR,
vol. abs/2306.08302, 2023. [Online]. Available: https://doi.org/10.
48550/arXiv.2306.08302
[91] S. Zheng, H. Bai, Y. Zhang, Y. Su, X. Niu, and N. Jaitly, “KGLens:
A Parameterized Knowledge Graph Solution to Assess What an LLM
Does and Doesn’t Know,” Feb. 2024, arXiv:2312.11539 [cs]. [Online].
Available: http://arxiv.org/abs/2312.11539
[92] R. Zhang, Y. Su, B. D. Trisedya, X. Zhao, M. Yang, H. Cheng,
and J. Qi, “AutoAlign: Fully Automatic and Effective Knowledge
Graph Alignment enabled by Large Language Models,” Nov. 2023,
arXiv:2307.11772 [cs]. [Online]. Available: http://arxiv.org/abs/2307.
11772
[93] Y.
Wang,
N.
Lipka,
R.
A.
Rossi,
A.
Siu,
R.
Zhang,
and
T. Derr, “Knowledge Graph Prompting for Multi-Document Question
Answering,” Dec. 2023, arXiv:2308.11730 [cs]. [Online]. Available:
http://arxiv.org/abs/2308.11730
[94] Z. Chen, Z. Jiang, F. Yang, E. Cho, X. Fan, X. Huang, Y. Lu,
and A. Galstyan, “Graph Meets LLM: A Novel Approach to
Collaborative Filtering for Robust Conversational Understanding,”
Jun. 2023, arXiv:2305.14449 [cs]. [Online]. Available: http://arxiv.org/
abs/2305.14449
[95] C. Sun, J. Li, Y. R. Fung, H. P. Chan, T. Abdelzaher, C. Zhai, and
H. Ji, “Decoding the Silent Majority: Inducing Belief Augmented
Social Graph with Large Language Model for Response Forecasting,”
Oct. 2023, arXiv:2310.13297 [cs]. [Online]. Available: http://arxiv.org/
abs/2310.13297
[96] R. Su, T.-W. Wu, and B.-H. Juang, “Schema Graph-Guided Prompt for
Multi-Domain Dialogue State Tracking,” Nov. 2023, arXiv:2311.06345
[cs]. [Online]. Available: http://arxiv.org/abs/2311.06345
[97] Y. Peng, S. Lin, Q. Chen, L. Xu, X. Ren, Y. Li, and J. Xu,
“ChatGraph: Chat with Your Graphs,” Jan. 2024, arXiv:2401.12672
[cs]. [Online]. Available: http://arxiv.org/abs/2401.12672
[98] Y. Shen, R. Liao, Z. Han, Y. Ma, and V. Tresp, “GraphextQA:
A
Benchmark
for
Evaluating
Graph-Enhanced
Large
Language
Models,” Oct. 2023, arXiv:2310.08487 [cs]. [Online]. Available:
http://arxiv.org/abs/2310.08487
[99] H. Yan, C. Li, R. Long, C. Yan, J. Zhao, W. Zhuang, J. Yin,
P.
Zhang,
W.
Han,
H.
Sun,
W.
Deng,
Q.
Zhang,
L.
Sun,
X. Xie, and S. Wang, “A comprehensive study on text-attributed
graphs: Benchmarking and rethinking,” in Advances in Neural
Information Processing Systems 36: Annual Conference on Neural
Information Processing Systems 2023, NeurIPS 2023, New Orleans,
LA,
USA,
December
10
-
16,
2023,
A.
Oh,
T.
Naumann,
A.
Globerson,
K.
Saenko,
M.
Hardt,
and
S.
Levine,
Eds.,
2023. [Online]. Available: http://papers.nips.cc/paper files/paper/2023/
hash/37d00f567a18b478065f1a91b95622a0-Abstract-Datasets and
Benchmarks.html
[100] A. McCallum, K. Nigam, J. Rennie, and K. Seymore, “Automating
the construction of internet portals with machine learning,” Inf.
Retr.,
vol.
3,
no.
2,
pp.
127–163,
2000.
[Online].
Available:
https://doi.org/10.1023/A:1009953814988
[101] C. L. Giles, K. D. Bollacker, and S. Lawrence, “CiteSeer: an
automatic citation indexing system,” in Proceedings of the third ACM
conference on Digital libraries - DL ’98.
Pittsburgh, Pennsylvania,
United States: ACM Press, 1998, pp. 89–98. [Online]. Available:
http://portal.acm.org/citation.cfm?doid=276675.276685
[102] Y.
Zhang,
H.
Dai,
Z.
Kozareva,
A.
Smola,
and
L.
Song,
“Variational Reasoning for Question Answering With Knowledge
Graph,” Proceedings of the AAAI Conference on Artificial Intelligence,
vol. 32, no. 1, Apr. 2018. [Online]. Available: https://ojs.aaai.org/
index.php/AAAI/article/view/12057
[103] X.
Wang,
T.
Gao,
Z.
Zhu,
Z.
Zhang,
Z.
Liu,
J.
Li,
and
J. Tang, “KEPLER: A unified model for knowledge embedding
and pre-trained language representation,” Trans. Assoc. Comput.
Linguistics, vol. 9, pp. 176–194, 2021. [Online]. Available: https:
//doi.org/10.1162/tacl a 00360
[104] K. M. Borgwardt, C. S. Ong, S. Schonauer, S. V. N. Vishwanathan,
A. J. Smola, and H.-P. Kriegel, “Protein function prediction via
graph kernels,” Bioinformatics, vol. 21, no. Suppl 1, pp. i47–i56, Jun.
2005. [Online]. Available: https://academic.oup.com/bioinformatics/
article-lookup/doi/10.1093/bioinformatics/bti1007
[105] A. K. Debnath, R. L. Lopez de Compadre, G. Debnath, A. J. Shuster-
man, and C. Hansch, “Structure-activity relationship of mutagenic aro-
matic and heteroaromatic nitro compounds. correlation with molecular
orbital energies and hydrophobicity,” Journal of medicinal chemistry,
vol. 34, no. 2, pp. 786–797, 1991.
[106] N.
Wale,
I.
A.
Watson,
and
G.
Karypis,
“Comparison
of
descriptor
spaces
for
chemical
compound
retrieval
and
classification,”
Knowledge
and
Information
Systems,
vol.
14,
no.
3,
pp.
347–375,
Mar.
2008.
[Online].
Available:
http://link.springer.com/10.1007/s10115-007-0103-5
[107] H.
Toivonen,
A.
Srinivasan,
R.
D.
King,
S.
Kramer,
and
C.
Helma,
“Statistical
evaluation
of
the
Predictive
ToxicologyChallenge
2000–2001,”
Bioinformatics,
vol.
19,
no.
10,
pp.
1183–1193,
Jul.
2003.
[Online].
Available:
https://academic.oup.com/bioinformatics/article/19/10/1183/184239
[108] X.
Kong,
J.
Zhang,
and
P.
S.
Yu,
“Inferring
anchor
links
across multiple heterogeneous social networks,” in Proceedings
of
the
22nd
ACM
international
conference
on
Conference
on
information & knowledge management - CIKM ’13.
San Francisco,
California, USA: ACM Press, 2013, pp. 179–188. [Online]. Available:
http://dl.acm.org/citation.cfm?doid=2505515.2505531
[109] M. Wan and J. McAuley, “Item recommendation on monotonic
behavior chains,” in Proceedings of the 12th ACM Conference on
Recommender Systems.
Vancouver British Columbia Canada: ACM,
Sep. 2018, pp. 86–94. [Online]. Available: https://dl.acm.org/doi/10.
1145/3240323.3240369
[110] J.
Ni,
J.
Li,
and
J.
McAuley,
“Justifying
Recommendations
using Distantly-Labeled Reviews and Fine-Grained Aspects,” in
Proceedings of the 2019 Conference on Empirical Methods in Natural
Language Processing and the 9th International Joint Conference
on Natural Language Processing (EMNLP-IJCNLP).
Hong Kong,
China: Association for Computational Linguistics, 2019, pp. 188–197.
[Online]. Available: https://www.aclweb.org/anthology/D19-1018
[111] W. L. Hamilton, Z. Ying, and J. Leskovec, “Inductive representation
learning on large graphs,” in Advances in Neural Information
Processing Systems 30: Annual Conference on Neural Information
Processing Systems 2017, December 4-9, 2017, Long Beach, CA,
USA, I. Guyon, U. von Luxburg, S. Bengio, H. M. Wallach,
R. Fergus, S. V. N. Vishwanathan, and R. Garnett, Eds., 2017, pp.
1024–1034. [Online]. Available: https://proceedings.neurips.cc/paper/
2017/hash/5dd9db5e033da9c6fb5ba83c7a7ebea9-Abstract.html
[112] A. Bojchevski and S. G¨
unnemann, “Deep Gaussian Embedding of
Graphs: Unsupervised Inductive Learning via Ranking,” Feb. 2018,
arXiv:1707.03815 [cs, stat]. [Online]. Available: http://arxiv.org/abs/
1707.03815
[113] O. Shchur, M. Mumme, A. Bojchevski, and S. G¨
unnemann, “Pitfalls
of Graph Neural Network Evaluation,” Jun. 2019, arXiv:1811.05868
[cs, stat]. [Online]. Available: http://arxiv.org/abs/1811.05868
[114] R. Rossi and N. Ahmed, “The Network Data Repository with
Interactive Graph Analytics and Visualization,” Proceedings of the
AAAI Conference on Artificial Intelligence, vol. 29, no. 1, Mar.
2015. [Online]. Available: https://ojs.aaai.org/index.php/AAAI/article/
view/9277
[115] H. Huang, H. Wang, and X. Wang, “An analysis framework
of
research
frontiers
based
on
the
large-scale
open
academic
graph,” Proceedings of the Association for Information Science and
Technology, vol. 57, no. 1, p. e307, Oct. 2020. [Online]. Available:
https://asistdl.onlinelibrary.wiley.com/doi/10.1002/pra2.307
[116] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser,
M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and
J. Schulman, “Training Verifiers to Solve Math Word Problems,” Nov.
2021, arXiv:2110.14168 [cs]. [Online]. Available: http://arxiv.org/abs/
2110.14168
[117] A. Patel, S. Bhattamishra, and N. Goyal, “Are NLP Models really able
to Solve Simple Math Word Problems?” Apr. 2021, arXiv:2103.07191
[cs]. [Online]. Available: http://arxiv.org/abs/2103.07191
[118] S. Han, H. Schoelkopf, Y. Zhao, Z. Qi, M. Riddell, L. Benson,
L. Sun, E. Zubova, Y. Qiao, M. Burtell, D. Peng, J. Fan, Y. Liu,
B. Wong, M. Sailor, A. Ni, L. Nan, J. Kasai, T. Yu, R. Zhang,
S. Joty, A. R. Fabbri, W. Kryscinski, X. V. Lin, C. Xiong, and
D. Radev, “FOLIO: Natural Language Reasoning with First-Order
Logic,”
Sep.
2022,
arXiv:2209.00840
[cs].
[Online].
Available:
http://arxiv.org/abs/2209.00840
[119] Y. Zhang, B. Jin, Q. Zhu, Y. Meng, and J. Han, “The effect of metadata
on scientific literature tagging: A cross-field cross-model study,” in
Proceedings of the ACM Web Conference 2023, 2023, pp. 1626–1637.
[120] A. Johnson, T. Pollard, and R. Mark, “MIMIC-III Clinical Database,”
2015. [Online]. Available: https://physionet.org/content/mimiciii/1.4/
[121] K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and J. Taylor,
“Freebase: a collaboratively created graph database for structuring
human knowledge,” in Proceedings of the 2008 ACM SIGMOD
international
conference
on
Management
of
data.
Vancouver
Canada: ACM, Jun. 2008, pp. 1247–1250. [Online]. Available:
https://dl.acm.org/doi/10.1145/1376616.1376746
[122] K.
Toutanova
and
D.
Chen,
“Observed
versus
latent
features
for knowledge base and text inference,” in Proceedings of the
3rd
Workshop
on
Continuous
Vector
Space
Models
and
their
Compositionality.
Beijing, China: Association for Computational
Linguistics, 2015, pp. 57–66. [Online]. Available: http://aclweb.org/
anthology/W15-4007
[123] W. Yih, M. Richardson, C. Meek, M. Chang, and J. Suh, “The
value of semantic parse labeling for knowledge base question
answering,” in Proceedings of the 54th Annual Meeting of the
Association
for
Computational
Linguistics,
ACL
2016,
August
7-12,
2016,
Berlin,
Germany,
Volume
2:
Short
Papers.
The
Association for Computer Linguistics, 2016. [Online]. Available:
https://doi.org/10.18653/v1/p16-2033
[124] A. Talmor and J. Berant, “The Web as a Knowledge-base for
Answering Complex Questions,” Mar. 2018, arXiv:1803.06643 [cs].
[Online]. Available: http://arxiv.org/abs/1803.06643
[125] C.-Y. Lin, “Rouge: A package for automatic evaluation of summaries,”
in Text summarization branches out, 2004, pp. 74–81.
[126] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “BLEU: a method
for automatic evaluation of machine translation,” in Proceedings
of the 40th Annual Meeting on Association for Computational
Linguistics - ACL ’02.
Philadelphia, Pennsylvania: Association
for Computational Linguistics, 2001, p. 311. [Online]. Available:
http://portal.acm.org/citation.cfm?doid=1073083.1073135
[127] C. Goutte and ´
E. Gaussier, “A probabilistic interpretation of precision,
recall and F-score, with implication for evaluation,” in Advances in
Information Retrieval, 27th European Conference on IR Research,
ECIR 2005, Santiago de Compostela, Spain, March 21-23, 2005,
Proceedings, ser. Lecture Notes in Computer Science, D. E. Losada and
J. M. Fern´
andez-Luna, Eds., vol. 3408.
Springer, 2005, pp. 345–359.
[Online]. Available: https://doi.org/10.1007/978-3-540-31865-1 25
[128] J. M. Kleinberg, R. Kumar, P. Raghavan, S. Rajagopalan, and
A.
Tomkins,
“The
web
as
a
graph:
Measurements,
models,
and
methods,”
in
Computing
and
Combinatorics,
5th
Annual
International Conference, COCOON ’99, Tokyo, Japan, July 26-
28, 1999, Proceedings, ser. Lecture Notes in Computer Science,
T. Asano, H. Imai, D. T. Lee, S. Nakano, and T. Tokuyama,
Eds., vol. 1627.
Springer, 1999, pp. 1–17. [Online]. Available:
https://doi.org/10.1007/3-540-48686-0 1
[129] S. M. Iacus, G. King, and G. Porro, “Causal inference without balance
checking: Coarsened exact matching,” Political analysis, vol. 20, no. 1,
pp. 1–24, 2012.
[130] H. Zhao, S. Liu, C. Ma, H. Xu, J. Fu, Z. Deng, L. Kong, and
Q. Liu, “GIMLET: A unified graph-text model for instruction-
based
molecule
zero-shot
learning,”
in
Advances
in
Neural
Information Processing Systems 36: Annual Conference on Neural
Information Processing Systems 2023, NeurIPS 2023, New Orleans,
LA,
USA,
December
10
-
16,
2023,
A.
Oh,
T.
Naumann,
A. Globerson, K. Saenko, M. Hardt, and S. Levine, Eds., 2023.
[Online]. Available: http://papers.nips.cc/paper files/paper/2023/hash/
129033c7c08be683059559e8d6bfd460-Abstract-Conference.html
"
4,5,TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series,"Chenxi Sun, Hongyan Li, Yaliang Li, Shenda Hong","This work summarizes two ways to accomplish Time-Series (TS) tasks in today's
Large Language Model (LLM) context: LLM-for-TS (model-centric) designs and
trains a fundamental large model, or fine-tunes a pre-trained LLM for TS data;
TS-for-LLM (data-centric) converts TS into a model-friendly representation to
enable the pre-trained LLM to handle TS data. Given the lack of data, limited
resources, semantic context requirements, and so on, this work focuses on
TS-for-LLM, where we aim to activate LLM's ability for TS data by designing a
TS embedding method suitable for LLM. The proposed method is named TEST. It
first tokenizes TS, builds an encoder to embed TS via instance-wise,
feature-wise, and text-prototype-aligned contrast, where the TS embedding space
is aligned to LLM embedding layer space, then creates soft prompts to make LLM
more open to that embeddings, and finally implements TS tasks using the frozen
LLM. We also demonstrate the feasibility of TS-for-LLM through theory and
experiments. Experiments are carried out on TS classification, forecasting, and
representation tasks using eight frozen LLMs with various structures and sizes.
The results show that the pre-trained LLM with TEST strategy can achieve better
or comparable performance than today's SOTA TS models and offer benefits for
few-shot and generalization. By treating LLM as the pattern machine, TEST can
endow LLM's ability to process TS data without compromising language ability.
We hope that this study will serve as a foundation for future work to support
TS+LLM progress.","Published as a conference paper at ICLR 2024
TEST: TEXT PROTOTYPE ALIGNED EMBEDDING TO
ACTIVATE LLM’S ABILITY FOR TIME SERIES
Chenxi Sun1,2,3, Hongyan Li1,2,3,4,∗, Yaliang Li5, Shenda Hong6,7,∗
1National Key Laboratory of General Artificial Intelligence, Peking University
2Key Laboratory of Machine Perception (Ministry of Education), Peking University
3School of Intelligence Science and Technology, Peking University
4PKU-WUHAN Institute for Artificial Intelligence
5Alibaba Group
6National Institute of Health Data Science, Peking University
7Institute of Medical Technology, Health Science Center of Peking University
{chenxi sun,leehy}@pku.edu,cn
yaliang.li@alibaba-inc.com, hongshenda@pku.edu.cn
ABSTRACT
This work summarizes two ways to accomplish Time-Series (TS) tasks in today’s
Large Language Model (LLM) context: LLM-for-TS (model-centric) designs and
trains a fundamental large model, or fine-tunes a pre-trained LLM for TS data;
TS-for-LLM (data-centric) converts TS into a model-friendly representation to
enable the pre-trained LLM to handle TS data. Given the lack of data, limited
resources, semantic context requirements, and so on, this work focuses on TS-
for-LLM, where we aim to activate LLM’s ability for TS data by designing a TS
embedding method suitable for LLM. The proposed method is named TEST. It
first tokenizes TS, builds an encoder to embed TS via instance-wise, feature-wise,
and text-prototype-aligned contrast, where the TS embedding space is aligned to
LLM’s embedding layer space, then creates soft prompts to make LLM more open
to that embeddings, and finally implements TS tasks using the frozen LLM. We
also demonstrate the feasibility of TS-for-LLM through theory and experiments.
Experiments are carried out on TS classification, forecasting, and representation
tasks using eight frozen LLMs with various structures and sizes. The results show
that the pre-trained LLM with TEST strategy can achieve better or comparable
performance than today’s SOTA TS models and offer benefits for few-shot and
generalization. By treating LLM as the pattern machine, TEST can endow LLM’s
ability to process TS data without compromising language ability. We hope that
this study will serve as a foundation for future work to support TS+LLM progress.
1
INTRODUCTION
Implementing Time-Series (TS) tasks, such as medical, industrial, and meteorological, is a research-
intensive field Sun et al. (2020). The relevant models evolved from statistical models to RNNs,
CNNs, and Transformers. Nowadays, we see a fast growth and remarkable performances of Large-
scale pre-trained Language Models (LLM) in NLP and CV fields Zhao et al. (2023). Consequently,
it seems natural to inquire whether LLMs can be used for TS tasks. However, according to experi-
ments, most pre-trained LLMs have not made significant progress in relation to abstract TS.
In answer to this requirement, we envision two ways to achieve the paradigm of TS+LLM 1:
• LLM-for-TS (model-centric, modify LLM). For TS data, design and train a fundamental Large
Model from scratch (LM-of-TS), then fine-tune the model accordingly for various downstream
tasks. Or, fine-tune the existing pre-trained LLM and convert it from text tasks to TS tasks;
∗Corresponding authors
1This categorization focuses on the requirement for changing the model. But from technology, LLM+TS
can be achieved by pre-training, fine-tuning, tool-augmented methods, external encoders, and their ensemble.
1
arXiv:2308.08241v2  [cs.CL]  22 Feb 2024
Published as a conference paper at ICLR 2024
• TS-for-LLM (data-centric, modify TS). Based on the existing LLMs, furthest freezing them,
design some mechanisms to customize TS for them by creating LLM-friendly TS representation.
We acknowledge that the first way, particularly developing and training a model from scratch, is the
most essential solution since pre-training is the crucial step of instilling knowledge to the model.
And the second way is actually challenging to break beyond the model’s original capabilities. How-
ever, in this work, we still focus on the second way due to the following three considerations:
Data perspective. LLM-for-TS methods, especially when building a foundation model, necessitate
large dataset, but TS is professional, the largest dataset is less than 10GB, which is much smaller
than that for NLP Zhou et al. (2023); TS-for-LLM methods can use a relatively small dataset as
its objective is solely to assist the existing LLM in inferring TS; Model perspective. LLM-for-TS
methods focus on vertical industries. Because of the major disparities in TS across domains, various
large models targeting medical TS, industrial TS, etc. must be built and trained from the start;
TS-for-LLM methods need little or even no training. By utilizing plug-in modules, it makes the
utilization more general and convenient; Usage perspective. LLM-for-TS methods are appropriate
for instances involving specialists; TS-for-LLM methods maintain LLM’s textual capabilities while
providing rich complementing semantics, being easily accessible and user-friendly.
Without changing the existing model, the most natural approach is treating TS as text data. For
example, a possible dialogue is: [Q] Diagnose if a patient has sepsis through the following mean
arterial pressure sequence in mm Hg: 88, 95, 78, 65, 52, 30. [A] Yes. However, TS is often
multivariate while text is univariate. For example, excepting mean arterial pressure, dozens of vital
signs, and laboratory values, such as heart rate, lactic acid, etc., need to be included when diagnosing
sepsis. One intuitive method is to divide a multivariate TS into multiple univariate sequences and
input them into LLM one by one. However, this will lead to three drawbacks. First, different prompt
sentences, data order, and connection statements will produce different results; Second, a long input
sequence likely to make LLM inefficient and hard to remember the previous univariate TS; Third,
the crucial aspects of multivariate dependency in TS will be ignored.
To address the above issues and achieve TS-for-LLM, we do not directly input TS into LLM, but
instead, we first tokenize TS, then design an encoder to embed them, finally skip the embedding layer
to input them into LLM. In this way, the core is to create embeddings that the LLM can understand.
High-quality TS embedding can be employed as the computational phenotype that the deep learning
model can understand Hong et al. (2023). To make the embedding understandable by language
models. Most multimodal approaches use alignment, for example, aligning text embedding and
image embedding through text descriptions of the image Wang et al. (2023). However, TS lacks
visual cues and has an annotation bottleneck caused by its complex characteristics. Only a few
specific TS, such as ECG, have text descriptions in each segment, where the image-text matching
route could be implemented. But in most cases, it’s not feasible.
Contrastive Learning (CL) can avoid the annotation bottleneck through designing pretext tasks by
utilizing intrinsic information instead of relying on pre-defined prior knowledge. Currently, CL
methods for TS data has also advanced Meng et al. (2023b). These methods evaluate the effec-
tiveness of TS embedding through follow-up classification, prediction, or clustering models, such
as SVM Franceschi et al. (2019b). However, these simple and newly-trained models are consid-
erably different from the complex and pre-trained LLM. The representation vector generated by
unconstrained CL is likely to deviate greatly from the LLM’s cognitive embedding space.
To address the above issues, we propose an embedding method for TimE Series tokens to align
the Text embedding space of LLM (TEST). Based on CL, TEST uses text embedding vectors as
prototypes to constrain TS’ embedding space and highlights feature-wise patterns. We show that
TEST can activate LLM’s ability as pattern machine. The contributions of this work are:
• Summarize two TS+LLM paradigms, LLM-for-TS, TS-for-LLM, with their potential methods;
• Propose TEST for TS-for-LLM. TEST can produce the similarity-based, instance-wise, feature-
wise, and text-prototype-aligned embedding for TS tokens. We prove that prompt tuning is al-
most equivalent to supervised fine-tuning when TS embedding and word embedding are aligned;
• Experiments on TS classification, forecasting, few-shot, and representation tasks demonstrate
that TEST can activate LLM’s capability to archive TS tasks, where the random and unsatisfac-
tory results produced by original LLMs can be elevated to the baseline.
2
Published as a conference paper at ICLR 2024
Category
Means
Pros
Cons
Work
LM-of-TS
Training
Specialized,
Not universal,
Pre-training Ma et al. (2023)
accurate
large datasets
Earth transformer Bi et al. (2023)
LLM-for-TS
Tuning
End-to-end,
More experiments,
GPT4TSZhou et al. (2023)
accurate
lose language ability
LLM4TSChang et al. (2023)
Tool
augmented
Parameter-efficient,
less experiments
Need experts,
need annotation
PromptCast Xue & Salim (2023)
Health Learner Liu et al. (2023)
METS Li et al. (2024)
Text2ECGChung et al. (2023)
TS-for-LLM
External
encoder
Parameter-efficient,
Weak robust
TEST
multiple abilities
Table 1: Existing Work about TS+LLM
As the name of TEST implies, it’s a forward-looking test that we hope to lay the groundwork for
future study. And it does give LLM new capabilities and highlight its qualities as a pattern machine.
2
RELATED WORK
2.1
TIME SERIES AND LARGE LANGUAGE MODEL
There hasn’t been much research done on TS+LLM because this field is still in its infancy. We
summarize the existing work in Table 1. LLM-for-TS with changing the model can be achieved
through tuning or tool augmented means; TS-for-LLM with changing the data can be achieved
through building the external encoder.
LM-of-TS Ma et al. (2023) trains a fundamental and accurate model based on accumulated domain
TS data, but it can be difficult to construct a large well-labeled dataset due to data acquisition and
annotation costs. By comparison, Supervised Fine-Tuning (SFT) in LLM-for-TS Chang et al. (2023)
has a relatively smaller workload than pre-training, but it can make the LLM lose its language
capabilities and its advantages over a sophisticated model designed specifically for TS tasks are
unclear. Regarding TS as the text sequence and using prompts as the augmented tool Liu et al.
(2023) could input numerical TS into LLM directly, but it is inaccurate, requires more experience,
and will fail for multivariate TS. The multimodal methods Li et al. (2024) could align the text and
TS, but apart from ECG, most TS datasets have no segment annotation.
2.2
TIME SEIRES EMBEDDING
TS embedding can provide identities by including typical, associated, and dependant attributes.
CL-based methods can get the data representation Chen et al. (2020), employing the instance dis-
crimination pretext task to bring similar pairs closer while pushing dissimilar pairs apart in the
embedding space. Some efforts have been made to implement instance-level contrast Woo et al.
(2022b); Zheng et al. (2023), temporal-level contrast Meng et al. (2023c); Franceschi et al. (2019b),
and clustering-level contrast Meng et al. (2023a) on TS data, with promising results. However, the
direct contrast cannot bridge TS embedding and the LLM’s comprehensible space. In our setting,
we prefer to freeze the pre-trained LLM and let the embedding compromise. That is, we use the text
token embedding in LLM to limit and guide the TS token embedding.
Inspired by the prototype-level contrast Caron et al. (2020a), which goes beyond the independence
assumption and exploits latent cluster information present within samples. We can select some text
embeddings as basic prototypes to lead the learning. However, in addition to the alignment, we still
need to consider issues of prototype selection, differentiation Meng et al. (2023c), uniformity Wang
& Isola (2020), stability Huang et al. (2023) and etc.
3
METHODS
TEST has two key steps: In Figure 1, build an encoder to embed TS; In Figure 2, create prompts to
make the LLM can accept TS embeddings as input.
3
Published as a conference paper at ICLR 2024
Encoder
Instance contrast
Text prototype
Anchor
Positive
Negative
Augmentation
Projector
Feature contrast
Text alignment
Similarity
Value
1
9
Shape
up
down
Frequency
high
low
Frequency
Value
Shape
Value
Frequency
Shape
Decoder
Autoencoding
Feature matrix
Figure 1: Text-prototype-aligned TS Embedding by Instance-wise and Feature-wise Contrast
3.1
TS TOKEN AUGMENTATION AND ENCODING
Definition 1 (Token Embedding of Time Series) A multivariate time series x = {xd
t }T,D
t=1,d=1 has
D variables and T time points. It can be segmented to a list of K non-overlapping subsequences
s = {sk}K
k=1 by a segmentation function fs : x →s, where the length of sk = xti:tj is arbitrary,
1 ≤ti < tj ≤T. We call s as the token list of time series x. Further, each token can be embeded
to a M-dimensional representation space by an embedding function fe : sk ∈RD×T →ek ∈RM.
Finally, the token embedding list of x is e = {ek}K
k=1 = fe(s) = fe(fs(x)).
We first tokenize TS into some segmentation/subsequences/tokens/instances through the classical
sliding window method in representation learning Yue et al. (2022) s = fs(x). We define a TS
token s as the anchor instance. Its positives s+ are the augmented instances, sweak ∼Tweak (jitter-
and-scale strategy, adding random variations to the signal and scale up its magnitude), sstrong ∼
Tstrong (permutation-and-jitter strategy, splitting the sequence into a random number of segments
and randomly shuffling them) Eldele et al. (2021b). Its negatives s−are from non-overlapping
instances which do not have the same subsequence as s.
After getting anchor-positive-negative, we built a neural network as the encoder to embed in-
stance into vector e = fe(s).
We also trained a decoder fd by using the auto-encoding loss
Lae =
1
N
PN
i=1 sim(s, fd(e)) to ensure the representativeness of the embedding and subsequent
verification. Because our primary goal is to retrieve the encoder, this decoder can likewise be un-
built without harming the future process.
3.2
INSTANCE-WISE AND FEATURE-WISE CONTRAST
The basic instance-wise CL treats each instance independently and design the instance discrimina-
tion pretext task to keep similar instances close and dissimilar instances far away. To prevent embed-
ding space collapse, we treat augmented views of the same instance as the unique positive pair, and
all remaining ones within the B size minibatch as negative pairs He et al. (2020). The instance-wise
contrastive loss is shown in Equation 1. Where given the instance embedding e, e+/−, we construct
a projection head fp, which is a one-layer MLP to obtain fp(e). σ(e, e+/−) is used to calculate
the similarity between two projected vectors through a similarity function sim like cosine similarity
with the instance-level temperature parameter τ.
Lins = −log
exp(σ(e, e+))
exp(σ(e, e+)) + PB
i=1 exp(σ(e, e−
i ))
σ(e, e+/−) = sim(fp(e), fp(e+/−))
τ
(1)
4
Published as a conference paper at ICLR 2024
We also propose a feature-wise contrast method to break the independence between instances. As
shown in Figure 1, after embedding, a feature matrix RB×M is formed by the representation vectors
of instances in a minibatch. Where each row is an embedding of a instance, thus rows could be
regarded as soft labels of instances which are used in Equation 1. In addition to rows, columns of
feature matrix also have semantic information. Li et al. (2021c) proposed that the columns could
be further regarded as cluster representations. However such cluster-wise methods require prior
knowledge to pre-specify the number of clusters, which is non-trivial for the unlabeled TS data
in this work. Thus, we propose to regard the columns as the soft labels of features and perform
discrimination between groups of similar features.
For an anchor feature matrix m, where m is the B-th row copy of the vector e, we obtain a positive
feature matrix m+ and a negative feature matrix m−, where m+/−= [ei]B
i=1 ∈RB×M. We mark
the columns in the matrix as m ∈mT. As expressed by the item before the right arrow in the
Equation 2, the feature-wise contrast mainly align and differentiate the same feature column among
the positive and negative. However, this may cause the representation space to shrink within a small
area. We find that ensuring differences between features can better address this issue. That is, we
suggest the contrast between different feature columns as shown in the item after the right arrow.
Lfea = −
M
X
i=1
(σ(mi, m+
i )
|
{z
}
Alignment
−σ(mi, m−
i )
|
{z
}
Difference
) ⇒−
M
X
i=1
log
exp(σ(mi, m+
i ))
PM
j=1[exp(σ(mi, m+
j )) + exp(σ(mi, m−
j ))]
|
{z
}
Feature category uniformity
(2)
More importantly, the injection of feature column differences can also greatly assist in the sub-
sequent implementation of text-prototype-aligned contrast. Because that contrast will apply the
selected text token embedding to the feature columns, like coordinate axes.
3.3
TEXT-PROTOTYPE-ALIGNED CONTRAST
The pre-trained LLM has its own token embedding, e.g., small, medium, and big GPT-2 embed
text tokens from word dictionaries into representation spaces with 768, 1024, and 1280 dimensions.
Naively, we can align the token embedding of TS and text using the similarity estimation. Although
TS tokens lack text annotation, we can place their embedding near typical text descriptions of TS,
such as value, shape, and frequency. In this fashion, it is intuitively expected that various TS tokens
can represent various descriptive terms such as small, big, up, down, stable, fluctuating, and so
on. Naturally, the example above is based on the closest neighbor principle because the embedding
space of a text token is discrete, akin to a vector table, but that of our TS token is continuous.
However, of course, the actual outcomes will not match what we expect because we are not providing
the supervised label or ground truth. For example, the embedding of a subsequence with an upward
trend may be very close to that of a decline word, or even that does not describe the trend. But it
is irrelevant whether semantics can be understood by us. As usual, the fact is that humans cannot
comprehend the model’s perceptual mode.
Recently, researchers proved that LLMs are pattern machines Mirchandani et al. (2023). Thus, in
this work, we achieve “TS →pattern →text” to activate LLM’s ability for TS tasks. The choice of
text prototype can be relaxed, not necessarily the description related to TS.
In this work, we choose P representative text embedding tp as pivots/prototypes, and map TS em-
bedding to them. In high dimensional space, almost all vectors are pairwise orthogonal Hopcroft &
Kannan (2013), thus the number of prototypes rather than the type does matter, and their differences
can be reflected in a single dimension/feature. Thus, the modeling function of the text prototype tp
is realized by feature-wise contrast. As expressed by Equation 3, the alignment term guarantees that
the two space ranges are roughly the same through the similarity constraint, the contrast term uses tp
as the coordinate axis to map the TS embedding, making the representation values in text coordinate
axes of similar instance similar. The feature matrix is no longer obtained through the projector but
through the prototype mapping e · tp →m.
Ltext = −
P
X
i=1
[sim(tpi, e)
|
{z
}
Text alignment
−Lfea(e · tp, e+ · tp, e−· tp)]
|
{z
}
Text contrast
(3)
5
Published as a conference paper at ICLR 2024
3.4
LEARNABLE PROMPT EMBEDDING
Even TS has been described using an embedded representation that the LLM can understand, LLM
still has to be instructed on how to do subsequent TS tasks.
Token
Encoder
Language model
TS embedding
…
Question
Answer
Train
Decoder
Fine-tune
Fine-tune
Trainable layer
Soft prompt
Classifier
Classification
Regression
[cls]
Figure 2: Framework of LLM for TS Tasks
Prompt engineering like template and
chain-of-thought is intuitive. Their con-
texts are coherent in human semantics, but
a TS embedding list has no human se-
mantics, it is more about a pattern se-
quence. Thus, to create a more consistent
prompt pattern, we train a soft prompt by
p-tuning Lester et al. (2021) make LLM
be easier to understand the input. These
soft prompts are task-specific embedding,
learning through the loss from LLM’s out-
put and task ground truth in Equation 4.
Lpromp = Lreg/cls(concat(pe, e))
(4)
GPT4TS Zhou et al. (2023)has proved the
feasibility that SFT can make LLM apply
to TS. Based on this, we demonstrate the
feasibility of TEST by proving the equiva-
lence between soft prompt and SFT.
Consider a conditional generation task
where the input x is a context and the output y is a sequence of tokens. Assume an autoregres-
sion LLM pϕ(y|x) with parameter ϕ, z = [x; y]. The inference of a pre-trained LLM is computing
hi as a function of zi and the past activations in its left context, Y = LMϕ(zi, hi). The past hi in
the soft prompt turning with prompt peθ is hi =
peθ[i, :],
if i ∈peidx
LMϕ(zi, hi), otherwise . The SFT from LLM
to TS-LLM is Equation 5. Its transformation shows that the soft prompt tuning is approximately
equivalent to SFT.
max
ϕ
pϕ(y′|x) = max
ϕ
X
i∈Yidx
log pϕ(z′
i|h<i) =
X
i∈Yidx
log pϕ+∆(zi + δzi|h<i)
≈
X
i∈Yidx
log pϕ(zi|h<i) ·
X
i∈peidx
log p∆(δzi|h<i)
=
X
i∈Yidx
log pϕ(zi|
fe(s)
| {z }
Text−TS alignment
|
{z
}
Frozen LLM
) ·
X
i∈peidx
log p∆(δzi|h<i)
|
{z
}
Prompt peθ
(5)
Equation 5 also suggests that the projection space of TS tokens should preferably cover the complete
set of text embedding space. Thus, we utilize clustering to find P representative text prototypes. The
process of using LLM to infer TS is shown in Figure 2. In this framework, the text data is input into
the embedding layer of LLM, while the prompts and TS embeddings skip this layer.
4
EXPERIMENTS
The core of TEST is to train an encoder fe and a soft prompt pe as described in Algorithm 1. The
encoder must can extract relevant information from TS, needs to be time- and memory-efficient,
and has to allow variable-length inputs. Thus, we build a causal TCN with 10 layers of convolution
blocks. Each convolution block is a sequence of GELU, DilatedConv, BatchNorm, GELU, Dilated-
Conv, with skip connections across each block. The DilatedConvs have dilation of 2i in each layer
i of convolution block. A final convolution block is used to map the hidden channels to the output
channel whose size is the same as the LLM’s embedding size.
6
Published as a conference paper at ICLR 2024
Algorithm 1 Training TEST
1: for e in epochs do
2:
// UPDATE ENCODER
3:
θfe = θfe −η▽θfe (Lins + Ltext)
4:
// UPDATE DECODER (OPTIMAL)
5:
θfd = θfd −η▽θfd Lae
6:
// UPDATE PROJECTOR
7:
θfp = θfp −η▽θfp Lins
8: end for
9: for e in epochs do
10:
// UPDATE PROMPT
11:
pe = pe −η▽θpeLpromp
12:
// FINE TUNE DECODER (OPTIMAL)
13:
θfd = θfd −η′▽θfd Lreg
14:
// UPDATE CLASSIFIER (OPTIMAL)
15:
θfc = θfc −η▽θfc Lcls
16: end for
Model
Size
Embed. dimension
Bert Devlin et al. (2018)
110M, 335M
748, 1024
GPT2 Radford et al. (2019)
117M, 345M, 774M
768, 1024, 1280
ChatGLM Du et al. (2022)
6B
4096
LLaMa2 Touvron et al. (2023)
7B, 13B
4096
Table 2: The Used Language Model
The used LLMs are as listed
in Table 2.
Each encoder and
soft prompt of LLM are trained
using the Adam optimizer on
20 NVIDIA Tesla V100-SXM2
GPU with CUDA 11.3.
We compare our method to 5 kinds of methods including 12 baselines: 1) LLM-QA methods Xue &
Salim (2023); Liu et al. (2023) with the classification template Classify the given [domain] sequence
as either [class label] or [class label]: [numerical sequence]. [A] and the forecasting template [Q]
Forecast the next value of the given [domain] sequence: [numerical sequence]. [A]; 2) SFT LLM-
for-TS method GPT4TS Zhou et al. (2023); 3) classical TS models DWT, DWTD Bagnall et al.
(2018), 1NNED, and TCN Tan et al. (2021); 4) SOTA TS models Informer Zhou et al. (2021),
DLinear Zeng et al. (2023), and TimesNet Wu et al. (2023); 5) SOTA CL-based TS models Tloss
Franceschi et al. (2019b), TS2Vec Yue et al. (2022), and CoST Woo et al. (2022a).
The overall results are shown in Figure 3 (The appendix has more compared classical SOTA models
and detailed results about long-term, short-term, few-shot, and zero-shot forecasting, multivariate
time series classification, and representation tasks.). Overall, after using TEST, when the size of
LLM reaches about 300M, their accuracy comparable to SOTA model.
4.1
CLASSIFICATION
We present accuracy scores for all 128 kinds of univariate TS datasets in UCR archive Dau et al.
(2019) and all 30 kinds of multivariate TS datasets in UEA archive Bagnall et al. (2018).
Accuracy. In Figure 3 (a-b), TEST makes the classification accuracy of LLM increase significantly.
LLM’s original classification performances are demonstrated through two QA results. It almost
guesses the classification labels at random, especially for multivariate TS. After using TEST, GPT2-
774M, which has the median accuracy among all models, can improve accuracy by at least 18% for
univariate TS and 25% for multivariate TS. TEST makes most LLMs comparable to, if not better
than, the existing models. When the size reaches about 300M, the accuracy can exceed TS baselines;
When the size reaches about 700M, the accuracy can exceed SOTA TS transformers.
Ablation. In Figure 3 (c-d), different text prototypes will lead to different results. We set 3 groups of
text prototypes: embeddings of value, shape, frequency, and embeddings of 3 or 10 cluster centers.
Choosing a prototype group that more accurately represents LLM’s entire text embedding space can
improve the performance. This is also suggested by Equation 5. Different prompt types, initializa-
tion, and length will lead to different results. We compare the soft prompt with the hard prompt of
Classify the given [domain] sequence as either [class label] or [class label]: [TS embedding]. The
accuracy differs by at least 10%. We set random initialization from uniform distribution and task
description initialization from Classify the given sequence. The latter makes the training converge
faster. When the model reaches 1B, a prompt length of 10 can achieve excellent results.
4.2
FORECASTING
We present short-forecasting MSE scores for all 19 kinds of varied time series datasets in TSER
archive Tan et al. (2021), and long-forecasting MSE scores for 8 popular real-world benchmark
datasets including weather, traffic, electricity, ILI, and ETT from Wu et al. (2023).
7
Published as a conference paper at ICLR 2024
Univariate Classification
Multivariate 
Classification
Shot-term 
Forecasting
Long-term Forecasting
Few-shot Forecasting
General 
Forecasting
Representation
TEST (GPT2-774M)
TimesNet
DLinear 
Informer
TCN
TS2Vec
LLM+TEST (ours)
Classical
SOTA models
QA
CL models
LLM+TEST (ours)
Classical
SOTA models
QA
CL models
𝑏
LLM+TEST (ours)
Classical
SOTA models
QA
LLM+TEST (ours)
SOTA models
QA
SFT
𝑒
𝑓
UCR classification accuracy
UCR classification accuracy
𝑐
LLM+TEST (ours)
Classical
SOTA models
LLM+TEST (ours)
SOTA models
QA
SFT
ℎ
Embedding before / after inputting LLM (ours)  + SVM
CL models + SVM
𝑎
𝑖
𝑑
𝑔
LLM+TEST (ours)
Classical
SOTA models
QA
CL models
UCR classification accuracy 
using representation
UCR classification accuracy
Figure 3: Experiment Results. (a-d) shows the classification results; (e-h) shows the forecasting
results; (i) shows the representation results. The red dashed line represents the best result.
Accuracy. In Figure 3 (e-f), TEST makes the forecasting accuracy of LLM increase significantly
and comparable to SOTA models. When the size reaches about 300M, the accuracy can exceed
SOTA TS transformers.
Generalization. We fuse 19 datasets into 1 dataset and test the method on this fused dataset. As
shown in Figure 3 (g), compared with baselines, LLM-based models have better generality.
Few-shot. LLM has demonstrated remarkable performance in few-shot learning. Based on the
settings in Zhou et al. (2023), we present few-shot forecasting for 10% time steps in training datasets.
As shown in Figure 3 (h), TEST achieves the best performance and demonstrates a relative average
MSE reduction of 23.5%.
8
Published as a conference paper at ICLR 2024
4.3
REPRESENTATION
Active silence silent absent important final night voiced
White important change loop happy actively limit finally
Figure 4: Matching TS Embedding to Words
Representation learning. Learning universal rep-
resentations for TS is a fundamental but challeng-
ing problem. Both TEST’s first step (creating TS
embedding) and second step (LLM’s output) can
achieve this task. Based on the classical representa-
tion learning task, we evaluated the effectiveness of
TEST representation using SVM classifier on UCR
dataset. Note that using a simple classifier can bet-
ter reflect the presentation effect. In Figure 3 (i),
the embedding in TEST’s first step is comparable to
SOTA representation methods, and the embedding
in TEST’s second step can outperform them. This
indicates that after using LLM, the representation of
TS becomes more discriminative.
Case. We use nearest neighbor method to find the text that a TS token matches to in the word
embedding space of frozen LLM. In Figure 4, the majority of the identified words are sentiment-
related adjectives and nouns. We speculate that by prompting, the model will treat TS classification
task as an sentiment classification task. Thus, introducing prompt is like introducing a shortcut
for LLM. Besides, the matched words are like a kind of textual Shapelet for TS segmentation,
representing TS through a series of patterns. Instead of regarding TS as a sequence of numbers,
we suggest using words to identify patterns in TS as LLMs without SFT are not good for math
when performing digital tasks, but they are good at extracting knowledge as a pattern machine. The
semantics of the patterns be perplexing to us, but it makes sense to LLM.
5
DISCUSSION AND CONCLUSION
This paper proposes an instance-wise, feature-wise, and text-prototype-aligned TS embedding
method to achieve TS-for-LLM. It can activate LLM’s ability for TS tasks while maintaining its
original language ability. Experiments on classification, forecasting, and representation tasks show
that using TEST, LLM can archive comparable performance to SOTA methods.
TS-for-LLM can enrich LLM’s capabilities. SFT LLM may be more effective than TS-for-LLM,
yet its superiority over customized TS models remains unclear; Training customized models may be
more accurate in TS tasks, yet TS-for-LLM offers all notable benefits of LLM additionally.
TS-for-LLM can explore LLM’s mechanism as a pattern machine. The essence of TS-for-LLM
is: TS ↔TS embeddings ↔patterns ↔text/word embedding ↔text. Although TEST gives the
impression of a forcibly aligning operations between TS and text, it dose convert TS into an under-
standable pattern sequence for LLMs, that clearly demonstrates that the essence of LLM is pattern
recognition. In fact, TS is objective data, whereas images, text, and speech are subjective data that
can be perceived by human senses. TEST aligns objective TS data and subjective text data at the
machine level, but how to align them at the human perception level requires future research.
Meanwhile, in addition to text prototypes and prompts, LLM size and type also affect the results.
The impact of model type is intuitive, it is related to downstream tasks, where the bidirectional
structure is beneficial for classification, and the generated structure is beneficial for forecasting. The
impact of model size, where a larger model produces more accurate results, can be attributed to
various reasons. Aside from the impact of additional parameters, we believe that the datasets used
in the pre-training process are also important, with the size, diversity, and corpus type all having an
impact. We conjecture that more training data will provide the model with more opportunities to
learn temporal patterns. As a result, we intend to conduct more experiments to investigate deeper
correlations between corpora and TS data Chen et al. (2023).
ACKNOWLEDGMENTS
This work is supported by National Natural Science Foundation of China (No.62172018,
No.62102008) and Wuhan East Lake High-Tech Development Zone National Comprehensive Ex-
perimental Base for Governance of Intelligent Society.
9
Published as a conference paper at ICLR 2024
REFERENCES
Anthony J. Bagnall, Hoang Anh Dau, Jason Lines, Michael Flynn, James Large, Aaron Bostrom,
Paul Southam, and Eamonn J. Keogh. The UEA multivariate time series classification archive,
2018. CoRR, abs/1811.00075, 2018.
Kaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, Xiaotao Gu, and Qi Tian. Accurate medium-
range global weather forecasting with 3d neural networks. Nature, pp. 1476–4687, 2023. doi:
10.1038/s41586-023-06545-z.
Aaron Bostrom, Anthony Bagnall, Eamonn Keogh, Hoang Anh Dau, James Large, Jason Lines,
Michael Flynn, and Paul Southam. The uea multivariate time series classification archive, 2018,
2018.
Eoin Brophy, Zhengwei Wang, Qi She, and Tom´
as Ward. Generative adversarial networks in time
series: A systematic literature review. ACM Comput. Surv., 55(10):199:1–199:31, 2023.
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.
Unsupervised learning of visual features by contrasting cluster assignments. In Hugo Larochelle,
Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances
in Neural Information Processing Systems, 2020a.
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.
Unsupervised learning of visual features by contrasting cluster assignments. 2020b.
CDC. Illness. 2021. doi: https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html.
Ching Chang, Wen-Chih Peng, and Tien-Fu Chen. LLM4TS: two-stage fine-tuning for time-series
forecasting with pre-trained llms. CoRR, abs/2308.08469, 2023.
Daoyuan Chen, Yilun Huang, and et al. Data-juicer: A one-stop data processing system for large
language models. CoRR, abs/2309.0203, 2023.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework
for contrastive learning of visual representations. In Proceedings of International Conference on
Machine Learning, volume 119, pp. 1597–1607, 2020.
Hyunseung Chung, Jiho Kim, Joon-Myoung Kwon, Ki-Hyun Jeon, Min Sung Lee, and Edward
Choi. Text-to-ecg: 12-lead electrocardiogram synthesis conditioned on clinical text reports. In
IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 1–5, 2023.
Hoang Anh Dau, Anthony Bagnall, Kaveh Kamgar, Chin-Chia Michael Yeh, Yan Zhu, Shaghayegh
Gharghabi, Chotirat Ann Ratanamahatana, and Eamonn Keogh. The ucr time series archive.
IEEE/CAA Journal of Automatica Sinica, 6:1293–1305, 2019. doi: 10.1109/JAS.2019.1911747.
Angus Dempster, Daniel F. Schmidt, and Geoffrey I. Webb. Minirocket: A very fast (almost) de-
terministic transform for time series classification. In ACM SIGKDD Conference on Knowledge
Discovery and Data Mining, pp. 248–257, 2021. doi: 10.1145/3447548.3467231.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep
bidirectional transformers for language understanding. CoRR, abs/1810.04805, 2018.
Jiaxiang Dong, Haixu Wu, Haoran Zhang, Li Zhang, Jianmin Wang, and Mingsheng Long. Simmtm:
A simple pre-training framework for masked time-series modeling. CoRR, abs/2302.00861, 2023.
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm:
General language model pretraining with autoregressive blank infilling. In Proceedings of Annual
Meeting of the Association for Computational Linguistics, volume 1, pp. 320–335, 2022.
Emadeldeen Eldele, Mohamed Ragab, Zhenghua Chen, Min Wu, Chee Keong Kwoh, Xiaoli Li, and
Cuntai Guan. Time-series representation learning via temporal and contextual contrasting. In
Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, pp. 2352–
2359, 2021a.
10
Published as a conference paper at ICLR 2024
Emadeldeen Eldele, Mohamed Ragab, Zhenghua Chen, Min Wu, Chee Keong Kwoh, Xiaoli Li, and
Cuntai Guan. Time-series representation learning via temporal and contextual contrasting. In
International Joint Conference on Artificial Intelligence, pp. 2352–2359, 2021b.
Jean-Yves Franceschi, Aymeric Dieuleveut, and Martin Jaggi. Unsupervised scalable representation
learning for multivariate time series. In Advances in Neural Information Processing Systems, pp.
4652–4663, 2019a.
Jean-Yves Franceschi, Aymeric Dieuleveut, and Martin Jaggi. Unsupervised scalable representation
learning for multivariate time series. In Advances in Neural Information Processing Systems, pp.
4652–4663, 2019b.
Ge Gao, Qitong Gao, Xi Yang, Miroslav Pajic, and Min Chi. A reinforcement learning-informed pat-
tern mining framework for multivariate time series classification. In Proceedings of International
Joint Conference on Artificial Intelligence, pp. 2994–3000, 2022. doi: 10.24963/IJCAI.2022/415.
Jean-Bastien Grill, Florian Strub, Florent Altch´
e, Corentin Tallec, Pierre H. Richemond, Elena
Buchatskaya, Carl Doersch, Bernardo ´
Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar,
Bilal Piot, Koray Kavukcuoglu, R´
emi Munos, and Michal Valko. Bootstrap your own latent - A
new approach to self-supervised learning. In Advances in Neural Information Processing Systems,
2020.
Nate Gruver, Marc Finzi, Shikai Qiu, and Andrew Gordon Wilson. Large language models are zero-
shot time series forecasters. CoRR, abs/2310.07820, 2023. doi: 10.48550/ARXIV.2310.07820.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. Momentum contrast for
unsupervised visual representation learning. In Computer Vision and Pattern Recognition, pp.
9726–9735, 2020.
Shenda Hong, Hongyan Li, Chenxi Sun, and Junyuan Shang. Research and applications of extracting
computational phenotype from vital sign time series. China Seience and Technology Achivements,
10, 2023. doi: 10.3772/j.issn.1009-5659.223.10.002.
John Hopcroft and Ravindran Kannan. Computer science theory for the information age. Cambridge
University press, 2013.
Zhizhong Huang, Jie Chen, Junping Zhang, and Hongming Shan. Learning representation for clus-
tering via prototype scattering and positive sampling. IEEE Trans. Pattern Anal. Mach. Intell., 45
(6):7509–7524, 2023. doi: 10.1109/TPAMI.2022.3216454.
Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y. Zhang, Xiaoming Shi, Pin-Yu Chen,
Yuxuan Liang, Yuan-Fang Li, Shirui Pan, and Qingsong Wen. Time-llm: Time series forecasting
by reprogramming large language models. CoRR, abs/2310.01728, 2023. doi: 10.48550/ARXIV.
2310.01728.
Fazle Karim, Somshubra Majumdar, Houshang Darabi, and Samuel Harford. Multivariate lstm-fcns
for time series classification. Neural Networks, 116:237–245, 2019. doi: 10.1016/J.NEUNET.
2019.04.014.
Salar Hosseini Khorasgani, Yuxuan Chen, and Florian Shkurti. SLIC: self-supervised learning with
iterative clustering for human action videos. In IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 16070–16080, 2022. doi: 10.1109/CVPR52688.2022.01562.
Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.
Reformer: The efficient transformer.
In
International Conference on Learning Representations, 2020.
Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt
tuning. In Proceedings of Conference on Empirical Methods in Natural Language Processing,
pp. 3045–3059, 2021. doi: 10.18653/v1/2021.emnlp-main.243.
Guozhong Li, Byron Choi, Jianliang Xu, Sourav S. Bhowmick, Kwok-Pan Chun, and Grace Lai-
Hung Wong.
Shapenet:
A shapelet-neural network approach for multivariate time series
classification.
In AAAI Conference on Artificial Intelligence, pp. 8375–8383, 2021a.
doi:
10.1609/AAAI.V35I9.17018.
11
Published as a conference paper at ICLR 2024
Jun Li, Che Liu, Sibo Cheng, Rossella Arcucci, and Shenda Hong. Frozen language model helps
ecg zero-shot learning. In Medical Imaging with Deep Learning, pp. 402–415, 2024.
Junnan Li, Pan Zhou, Caiming Xiong, and Steven C. H. Hoi. Prototypical contrastive learning of
unsupervised representations. In International Conference on Learning Representations, 2021b.
Yunfan Li, Peng Hu, Jerry Zitao Liu, Dezhong Peng, Joey Tianyi Zhou, and Xi Peng. Contrastive
clustering. In AAAI Conference on Artificial Intelligence,, pp. 8547–8555, 2021c.
Xin Liu, Daniel McDuff, Geza Kovacs, Isaac R. Galatzer-Levy, Jacob E. Sunshine, Jiening Zhan,
Ming-Zher Poh, Shun Liao, Paolo Di Achille, and Shwetak N. Patel. Large language models are
few-shot health learners. CoRR, abs/2305.15525, 2023. doi: 10.48550/arXiv.2305.15525.
Yong Liu, Haixu Wu, Jianmin Wang, and Mingsheng Long. Non-stationary transformers: Exploring
the stationarity in time series forecasting. In Advances in Neural Information Processing Systems,
2022.
Qianli Ma, Zhen Liu, Zhenjing Zheng, Ziyang Huang, Siying Zhu, Zhongzhong Yu, and James T.
Kwok. A survey on time-series pre-trained models. CoRR, abs/2305.10716, 2023. doi: 10.48550/
arXiv.2305.10716.
Qianwen Meng, Hangwei Qian, Yong Liu, Yonghui Xu, Zhiqi Shen, and Lizhen Cui. MHCCL:
masked hierarchical cluster-wise contrastive learning for multivariate time series.
CoRR,
abs/2212.01141, 2022.
Qianwen Meng, Hangwei Qian, Yong Liu, Lizhen Cui, Yonghui Xu, and Zhiqi Shen. MHCCL:
masked hierarchical cluster-wise contrastive learning for multivariate time series. In AAAI Con-
ference on Artificial Intelligence, pp. 9153–9161, 2023a. doi: 10.1609/aaai.v37i8.26098.
Qianwen Meng, Hangwei Qian, Yong Liu, Yonghui Xu, Zhiqi Shen, and Lizhen Cui. Unsupervised
representation learning for time series: A review. CoRR, abs/2308.01578, 2023b. doi: 10.48550/
arXiv.2308.01578.
Qianwen Meng, Hangwei Qian, Yong Liu, Yonghui Xu, Zhiqi Shen, and Lizhen Cui. Unsupervised
representation learning for time series: A review. CoRR, abs/2308.01578, 2023c.
Suvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter, Danny Driess, Montserrat Gonzalez Are-
nas, Kanishka Rao, Dorsa Sadigh, and Andy Zeng. Large language models as general pattern
machines. In Conference on Robot Learning, volume 229 of Proceedings of Machine Learning
Research, pp. 2498–2518, 2023.
Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth
64 words: Long-term forecasting with transformers. In International Conference on Learning
Representations, 2023.
Boris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. N-BEATS: neural basis
expansion analysis for interpretable time series forecasting. In 8th International Conference on
Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020.
PeMS. Traffic. 2021. doi: http://pems.dot.ca.gov/.
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners. OpenAI, 2019.
Patrick Sch¨
afer and Ulf Leser. Multivariate time series classification with WEASEL+MUSE. CoRR,
abs/1711.11343, 2017.
Vivek Sharma, Makarand Tapaswi, M. Saquib Sarfraz, and Rainer Stiefelhagen. Clustering based
contrastive learning for improving face representations. In IEEE International Conference on Au-
tomatic Face and Gesture Recognition, pp. 109–116, 2020. doi: 10.1109/FG47880.2020.00011.
Taylor SJ and Letham B. Forecasting at scale. In PeerJ Preprints, pp. 5:e3190v2, 2017. doi:
10.7287/peerj.preprints.3190v2.
12
Published as a conference paper at ICLR 2024
Chenxi Sun, Shenda Hong, and et al. A review of deep learning methods for irregularly sampled
medical time series data. CoRR, abs/2010.12493, 2020. doi: 10.48550/arXiv.2010.12493.
Chang Wei Tan, Christoph Bergmeir, Francois Petitjean, and Geoffrey I Webb. Time series extrinsic
regression. Data Mining and Knowledge Discovery, pp. 1–29, 2021. doi: https://doi.org/10.1007/
s10618-021-00745-9.
Sana Tonekaboni, Danny Eytan, and Anna Goldenberg. Unsupervised representation learning for
time series with temporal neighborhood coding. In International Conference on Learning Repre-
sentations, 2021.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, and et al. Llama 2:
Open foundation and fine-tuned chat models. CoRR, abs/2307.09288, 2023.
A¨
aron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-
tive coding. CoRR, abs/1807.03748, 2018.
Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through align-
ment and uniformity on the hypersphere. In Proceedings of International Conference on Machine
Learning, volume 119, pp. 9929–9939, 2020.
Xiao Wang, Guangyao Chen, Guangwu Qian, Pengcheng Gao, Xiao-Yong Wei, Yaowei Wang,
Yonghong Tian, and Wen Gao. Large-scale multi-modal pre-trained models: A comprehensive
survey. Mach. Intell. Res., 20(4):447–482, 2023. doi: 10.1007/s11633-022-1410-8.
Wetterstation. Weather. 2017. doi: https://www.bgc-jena.mpg.de/wetter/.
Kristoffer Wickstrøm, Michael Kampffmeyer, Karl Øyvind Mikalsen, and Robert Jenssen. Mixing
up contrastive learning: Self-supervised representation learning for time series. Pattern Recognit.
Lett., 155:54–61, 2022.
Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven C. H. Hoi. Cost: Contrastive
learning of disentangled seasonal-trend representations for time series forecasting. In Interna-
tional Conference on Learning Representations, 2022a.
Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven C. H. Hoi. Cost: Contrastive
learning of disentangled seasonal-trend representations for time series forecasting. In The Inter-
national Conference on Learning Representations, 2022b.
Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven C. H. Hoi. Etsformer: Expo-
nential smoothing transformers for time-series forecasting. CoRR, abs/2202.01381, 2022c.
Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition trans-
formers with auto-correlation for long-term series forecasting. In Advances in Neural Information
Processing Systems, pp. 22419–22430, 2021.
Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Timesnet:
Temporal 2d-variation modeling for general time series analysis. In International Conference on
Learning Representations, 2023.
Hao Xue and Flora D. Salim. Promptcast: A new prompt-based learning paradigm for time series
forecasting. CoRR, abs/2210.08964, 2023.
Ling Yang and Shenda Hong. Unsupervised time-series representation learning with iterative bilin-
ear temporal-spectral fusion. In International Conference on Machine Learning, volume 162 of
Proceedings of Machine Learning Research, pp. 25038–25054, 2022.
Xinyu Yang, Zhenguo Zhang, and Rongyi Cui. Timeclr: A self-supervised contrastive learning
framework for univariate time series representation. Knowl. Based Syst., 245:108606, 2022.
Jinsung Yoon, Daniel Jarrett, and Mihaela van der Schaar. Time-series generative adversarial net-
works. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural
Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC,
Canada, pp. 5509–5519, 2019.
13
Published as a conference paper at ICLR 2024
Zhihan Yue, Yujing Wang, Juanyong Duan, Tianmeng Yang, Congrui Huang, Yunhai Tong, and
Bixiong Xu. Ts2vec: Towards universal representation of time series. In AAAI Conference on
Artificial Intelligence, pp. 8980–8987, 2022.
Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series
forecasting?
In AAAI Conference on Artificial Intelligence, pp. 11121–11128, 2023. doi: 10.
1609/aaai.v37i9.26317.
George Zerveas, Srideepika Jayaraman, Dhaval Patel, Anuradha Bhamidipaty, and Carsten Eickhoff.
A transformer-based framework for multivariate time series representation learning.
In ACM
SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 2114–2124, 2021. doi:
10.1145/3447548.3467401.
Dejiao Zhang, Feng Nan, Xiaokai Wei, Shang-Wen Li, Henghui Zhu, Kathleen R. McKeown,
Ramesh Nallapati, Andrew O. Arnold, and Bing Xiang. Supporting clustering with contrastive
learning.
In Proceedings of Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, pp. 5419–5430, 2021.
doi:
10.18653/V1/2021.NAACL-MAIN.427.
Xuchao Zhang, Yifeng Gao, Jessica Lin, and Chang-Tien Lu. Tapnet: Multivariate time series
classification with attentional prototypical network. In AAAI Conference on Artificial Intelligence,
pp. 6845–6852, 2020. doi: 10.1609/AAAI.V34I04.6165.
Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min,
Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen,
Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-
Rong Wen. A survey of large language models. CoRR, abs/2303.18223, 2023. doi: 10.48550/
arXiv.2303.18223.
Xiaochen Zheng, Xingyu Chen, Manuel Sch¨
urch, Amina Mollaysa, Ahmed Allam, and Michael
Krauthammer. Simts: Rethinking contrastive representation learning for time series forecasting.
CoRR, abs/2303.18205, 2023.
Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang.
Informer: Beyond efficient transformer for long sequence time-series forecasting. In AAAI Con-
ference on Artificial Intelligence, pp. 11106–11115, 2021. doi: 10.1609/aaai.v35i12.17325.
Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. Fedformer: Frequency
enhanced decomposed transformer for long-term series forecasting. In International Conference
on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 27268–
27286, 2022.
Tian Zhou, PeiSong Niu, Xue Wang, Liang Sun, and Rong Jin. One fits all:power general time
series analysis by pretrained lm. In Conference and Workshop on Neural Information Processing
Systems, 2023.
Rundong Zuo, Guozhong Li, Byron Choi, Sourav S. Bhowmick, Daphne Ngar-yin Mah, and
Grace Lai-Hung Wong. SVP-T: A shape-level variable-position transformer for multivariate time
series classification. In AAAI Conference on Artificial Intelligence, pp. 11497–11505, 2023. doi:
10.1609/AAAI.V37I9.26359.
14
Published as a conference paper at ICLR 2024
A
APPENDIX
A.1
RELATED WORK
Our work mainly involves two research fields: Universal Representation Learning (URL) for time
series based on Contrastive Learning (CL) and Large Language Model (LLM) + Time Series (TS).
A.1.1
CL-BASED URL FOR TS
Unsupervised URL approaches aim to learn discriminative feature representations from unlabeled
data, without the requirement of annotating every sample. Enabling URL is extremely crucial for
time series data, due to its unique annotation bottleneck caused by its complex characteristics and
lack of visual cues compared with other data modalities.
Contrastive methods learn meaningful representations from time series by optimizing self-
discrimination tasks.
Instead of directly modeling the complex raw data, they employ pretext
tasks that leverage the underlying similarity between samples, which eliminates the need for re-
constructing the complete input and allows for the discovery of contextualized underlying factors of
variations. Contrastive methods typically generate augmented views of the raw data through vari-
ous transformations and then learn representations by contrasting positive samples against negative
samples.The existing CL-based URL for TS are listed in Table 4.
Instance-level contrastive models treat individual samples independently for the purpose of instance
discrimination. They utilize data augmentations to transform original inputs into a new embedding
space. Within this space, augmentations derived from the same sample are considered as positive
pairs, while those from different samples are treated as negative pairs. During training, these models
are optimized by maximizing the similarity between representations of positive pairs, while simul-
taneously minimizing the similarity between representations of negative pairs.
Prototype-level contrastive models break the independence between samples and explore to exploit
the implicit semantics shared by samples in the same cluster. They can address the limitation that
instance-level contrastive learning models tend to treat semantically similar samples as negatives.
Temporal-level contrastive models instead focus on capturing scale- invariant representations at each
individual timestamp. By cosidering both instance-level and temporal-level representation learning
strategies, researchers aim to enhance the capability of contrastive learning methods in capturing the
complexities inherent in time series data.
A.1.2
LLM+TS
Large models, specifically referred to as large language models (LLMs) and pre-trained foundation
models (PFMs), have witnessed remarkable success across a multitude of tasks and domains, such
as natural language processing (NLP), computer vision (CV). Given the remarkable achievements of
large models in these diverse fields, an intriguing question emerges: can large models be effectively
employed to analyze TS data?
TS data has long been studied and proven to be indispensable in a myriad of real-world applica-
tions, encompassing fields such as geoscience, transportation, energy, healthcare, environment, and
Category
Pros
Cons
Methods
Reconstruction-based
Disregard insignificant data
Collapse of embedding space;
TimeNetWu et al. (2023)
that may contain noise
Unable to measure feature relations
SimMTM Dong et al. (2023)
Adversarial
Eliminate the need for expensive
Difficulty in model convergence;
TimeGAN Yoon et al. (2019)
manual labeling
Unable to measure feature relations
TS-GAN Brophy et al. (2023)
Predicative
Self-supervised
Affected by noise
TST Zerveas et al. (2021)
TS-TCCEldele et al. (2021a)
Contrastive
Self-supervised
Different datasets require different
Table 4
data augmentation methods and
similarity evaluations
Table 3: Representation Learning Methods of Time Series Methods
15
Published as a conference paper at ICLR 2024
Type
Methods
Instance-level
SimCLR Chen et al. (2020)
TimeCLR Yang et al. (2022)
MoCo He et al. (2020)
BYOL Grill et al. (2020)
CPC van den Oord et al. (2018)
SimSiam Zheng et al. (2023)
MCL Wickstrøm et al. (2022)
Prototype-level
SwAV Caron et al. (2020b)
PCL Li et al. (2021b)
CCL Sharma et al. (2020)
SCCL Zhang et al. (2021)
CC Li et al. (2021c)
SLIC Khorasgani et al. (2022)
MHCCL Meng et al. (2022)
Temporal-level
TS2Vec Yue et al. (2022)
TS-TCC Eldele et al. (2021b)
TNC Tonekaboni et al. (2021)
TCL
T-Loss Franceschi et al. (2019b)
BTSF Yang & Hong (2022)
CoST Woo et al. (2022a)
Table 4: Contrastive Learning based Universal Representation Methods for Time Series
Means
Pros
Cons
Work
Training
Specialized,
Not universal,
Pre-training Ma et al. (2023)
accurate
large datasets
Earth transformer Bi et al. (2023)
TS Transformers Wu et al. (2023)
Tuning
End-to-end,
More experiments,
GPT4TSZhou et al. (2023)
accurate
lose language ability
LLM4TSChang et al. (2023)
LLMTime Gruver et al. (2023)
Time-LLM Jin et al. (2023)
Tool Augmented
Parameter-efficient,
less experiments
Need experts,
need annotation
PromptCast Xue & Salim (2023)
Health Learner Liu et al. (2023)
METS Li et al. (2024)
Text2ECGChung et al. (2023)
External Encoder
Parameter-efficient,
Weak robust
TEST
multiple abilities
Table 5: Existing Work about TS+LLM
Figure 5: Technical Route of LLM+TS
finance. While large models have made significant progress in various fields, the arena of time se-
ries analysis has followed a more gradual path. Traditional analytical methods have predominantly
relied on statistical models. The advent of deep learning has galvanized the research community to
explore more potent data-driven models, typically built on the basis of Recurrent Neural Networks
(RNNs), Convolutional Neural Networks (CNNs), and Transformers. Nonetheless, the majority of
these models remain relatively small in scale and are tailored for specific tasks, thereby lacking the
capacity to acquire comprehensive semantic and knowledge representations from large-scale data
for multi-task reasoning.
There hasn’t been much research done on TS+LLM because this field is still in its infancy. We
summarize the existing work in Table 5. Different from the main text, we category work here
through technical means.
16
Published as a conference paper at ICLR 2024
A.2
MODEL
https://github.com/SCXsunchenxi/TEST
A.2.1
ENCODER
The core of TEST is to train an encoder and a soft prompt. The encoder must can extract relevant
information from TS, needs to be time- and memory-efficient, and has to allow variable-length
inputs. Thus, as shown in Figure 6, we build a causal TCN with 10 layers of convolution blocks.
Each convolution block is a sequence of GELU, DilatedConv, BatchNorm, GELU, DilatedConv,
with skip connections across each block. The DilatedConvs have dilation of 2i in each layer i of
convolution block. A final convolution block is used to map the hidden channels to the output
channel whose size is the same as the LLM’s embedding size.
The detailed architecture is: Number of channels in the intermediary layers of the causal network
is 40; Number of layers (depth of the causal network) is 10; Kernel size of all convolutions is
3; Negative slope of the leaky ReLU activation is 0.01; Number of output channels of the causal
network (before max pooling) is 640; Dimension of the representations is the same as the LLM’s
embedding size (e.g. 1024 for gpt2).
Figure 6: Illustration of Three Stacked Dilated Causal Convolutions and Composition of the i-th
Layer of The Chosen Architecture
We train our models with the following parameters for time series classification. Note that no hy-
perparameter optimization was performed on the encoder hyperparameters: Optimizer is Adam
with learning rate α = 0.001 and decay rates β = (0.9, 0.999); Number of negative samples is
K ∈{1, 2, 5, 10} for for univariate time series, K ∈{5, 10, 20} for multivariate ones; Batch size is
10; Number of optimizations steps is 2000for K ≤10 (i.e., 20 epochs for a dataset of size 1000),
1500 otherwise.
A.2.2
LLM
The used LLMs are as listed in Table 6. Each encoder and soft prompt of LLM are trained using the
Adam optimizer on 20 NVIDIA Tesla V100-SXM2 GPU with CUDA 11.3.
A.3
FORECASTING TASKS
All the deep learning networks are implemented in PyTorch and trained on NVIDIA V100 32GB
GPUs. We use mean square error (MSE) and mean absolute error (MAE) as metrics. For zero-
shot learning, mean absolute percentage error (MAPE) is used for TOURISM; symmetric MAPE
(sMAPE) is used for M3 and M4; normalized deviation (ND) is used for ELECTR. All experiments
are repeated 3 times and the mean of the metrics is used in the final results.
17
Published as a conference paper at ICLR 2024
Model
Size
Embed. dimension
Bert Devlin et al. (2018)
110M, 335M
748, 1024
GPT2 Radford et al. (2019)
117M, 345M, 774M
768, 1024, 1280
ChatGLM Du et al. (2022)
6B
4096
LLaMa2 Touvron et al. (2023)
7B, 13B
4096
Table 6: The Used Language Model
A.3.1
DATASET DETAILS
The details of long-term forecasting and few-shot forecasting datasets are: ETT datasets Zhou et al.
(2021) contain electricity load of various resolutions (ETTh & ETTm) from two electricity stations;
Weather datasetWetterstation (2017) contains 21 meteorological indicators of Germany within 1
year; Illness datasetCDC (2021) contains the influenza-like illness patients in the United States.
ILI is not used for few-shot learning for the limited quantity that is hard to follow the definition
of few-shot; Electricity dataset SJ & B (2017) contains the electricity consumption; Traffic dataset
PeMS (2021) contains the occupation rate of freeway system across the State of California. Table 7
summarizes details of feature statistics.
Dataset
Length
Dimension
Frequency
ETTh
17420
7
1 hour
ETTm
69680
7
15 min
Weather
52696
22
10 min
ILI
966
7
7 days
Electricity
26304
321
1 hour
Traffic
17544
862
1 hour
Table 7: Long-term Forecasting and Few-shot Forecasting Dataset Details
Dataset
Mapping
Length
Horizon
M4
M3
M3 Yearly
645
6
Yearly
-
M3 Quarterly
756
8
Quarterly
-
M3 Monthly
1428
18
Monthly
-
M3 Others
174
8
Monthly
-
M4 Yearly
23000
18
-
Yearly
M4 Quarterly
6
24000
-
Quarterly
M4 Monthly
8
48000
-
Monthly
M4 Weekly
359
13
-
Monthly
M4 Daily
4227
14
-
Monthly
M4 Hourly
414
48
-
Monthly
TOURISM Yearly
518
4
Yearly
Yearly
TOURISM Quarterly
427
8
Quarterly
Quarterly
TOURISM Monthly
366
24
Monthly
Monthly
ELECTR
1311
168
Hourly
Monthly
Table 8: Zero-term Forecasting Datasets and Mapping Details of Zero-shot Learning
The details of zero-shot forecasting datasets are: M4 is a large and diverse dataset that contains time
series of various frequencies and fields, including business, financial and economic forecasting; M3
is smaller than M4, but also contains time series from diverse domains and frequencies; TOURISM
is the dataset of tourism activities with different frequencies and contains a much higher fraction of
erratic series compared with M4; ELECTR represents the electricity usage monitoring of 370 cus-
tomers over three years. Table 8 summarizes details of the datasets and zero-shot mapping between
source and target.
A.3.2
BASELINE DETAILS
For long-shot forecasting, we refer to the SOTA methods reported in Wu et al. (2023): TimesNet
Wu et al. (2023), ETSformer Woo et al. (2022c), DLinear Zeng et al. (2023), FEDformer Zhou et al.
(2022), Informer Zhou et al. (2021), and LLM for TS method GPT4TS Zhou et al. (2023).
18
Published as a conference paper at ICLR 2024
For few-shot forecasting, we refor to the SOTA methods reported in Zhou et al. (2023): DLinear
Zeng et al. (2023), PatchTST Nie et al. (2023), TimesNet Wu et al. (2023), FEDformer Zhou et al.
(2022), Autoformer Wu et al. (2021), Stationary Liu et al. (2022), ETSformer Woo et al. (2022c),
Informer Zhou et al. (2021), Reformer Kitaev et al. (2020)
For zero-shot forecasting, we refor to the SOTA methods reported in Zhou et al. (2023): N-BEATS
Oreshkin et al. (2020), DLinear Zeng et al. (2023), PatchTST Nie et al. (2023), TimesNet Wu et al.
(2023), FEDformer Zhou et al. (2022), Autoformer Wu et al. (2021), Stationary Liu et al. (2022),
ETSformer Woo et al. (2022c), Informer Zhou et al. (2021), Reformer Kitaev et al. (2020)
Methods
TEST
GPT4TS
TimesNet
ETSformer
DLinear
FEDformer
Informer
TCN
LSTM
ETTm1
96
0.293 0.346
0.292 0.346
0.325 0.398
0.338 0.375
0.345 0.372
0.375 0.398
0.672 0.571
0.863 0.664
0.863 0.664
192
0.332 0.369
0.332 0.372
0.324 0.387
0.408 0.410
0.380 0.389
0.426 0.441
0.795 0.669
0.837 0.700
1.113 0.776
336
0.368 0.392
0.366 0.394
0.360 0.411
0.435 0.428
0.413 0.413
0.445 0.459
1.212 0.871
1.124 0.832
1.267 0.832
720
0.418 0.420
0.417 0.421
0.428 0.450
0.499 0.462
0.474 0.453
0.543 0.490
1.166 0.823
1.153 0.820
1.324 0.858
Avg
0.353 0.382
0.352 0.383
0.350 0.406
0.429 0.425
0.403 0.407
0.448 0.452
0.961 0.734
0.929 0.725
1.142 0.782
ETTh1
96
0.372 0.400
0.376 0.397
0.384 0.402
0.494 0.479
0.386 0.400
0.376 0.419
0.865 0.713
0.878 0.740
1.044 0.773
192
0.414 0.422
0.416 0.418
0.436 0.429
0.538 0.504
0.437 0.432
0.420 0.448
1.008 0.792
1.037 0.824
1.217 0.832
336
0.422 0.437
0.442 0.433
0.491 0.469
0.574 0.521
0.481 0.459
0.459 0.465
1.107 0.809
1.238 0.932
1.259 0.841
720
0.447 0.467
0.477 0.456
0.521 0.500
0.562 0.535
0.519 0.516
0.506 0.507
1.181 0.865
1.135 0.852
1.271 0.838
Avg
0.414 0.431
0.427 0.426
0.458 0.450
0.542 0.510
0.456 0.452
0.440 0.460
1.040 0.795
1.072 0.837
1.198 0.821
ETTh2
96
0.275 0.338
0.285 0.342
0.340 0.374
0.340 0.391
0.333 0.387
0.358 0.397
3.755 1.525
2.116 1.197
2.522 1.278
192
0.340 0.379
0.354 0.389
0.402 0.414
0.430 0.439
0.477 0.476
0.429 0.439
5.602 1.931
4.315 1.635
3.312 1.384
336
0.329 0.381
0.373 0.407
0.452 0.452
0.485 0.559
0.594 0.541
0.496 0.487
4.721 1.835
1.124 1.604
3.291 1.388
720
0.381 0.423
0.406 0.441
0.462 0.468
0.500 0.497
0.831 0.657
0.463 0.474
3.647 1.625
3.188 1.540
3.257 1.357
Avg
0.331 0.380
0.354 0.394
0.414 0.427
0.439 0.452
0.559 0.515
0.4370.449
4.431 1.729
2.686 1.494
3.095 1.352
Electricity
96
0.132 0.223
0.139 0.238
0.168 0.222
0.187 0.304
0.197 0.282
0.193 0.308
0.274 0.368
0.258 0.357
0.375 0.437
192
0.158 0.241
0.153 0.251
0.184 0.239
0.199 0.196
0.285 0.201
0.315 0.296
0.386 0.266
0.368 0.348
0.442 0.473
336
0.163 0.260
0.169 0.266
0.198 0.260
0.212 0.329
0.209 0.301
0.214 0.329
0.300 0.394
0.280 0.380
0.439 0.473
720
0.199 0.291
0.206 0.297
0.220 0.300
0.233 0.345
0.245 0.333
0.246 0.355
0.373 0.439
0.283 0.376
0.980 0.814
Avg
0.162 0.253
0.167 0.263
0.192 0.245
0.208 0.323
0.212 0.300
0.214 0.327
0.311 0.397
0.313 0.401
0.559 0.549
Traffic
96
0.407 0.282 0
0.388 0.282
0.593 0.321
0.607 0.392
0.650 0.396
0.587 0.366
0.719 0.391
0.684 0.384
0.843 0.453
192
0.423 0.287
0.407 0.290
0.617 0.336
0.621 0.399
0.598 0.370
0.604 0.373
0.696 0.379
0.685 0.390
0.847 0.453
336
0.430 0.296
0.412 0.294
0.629 0.336
0.622 0.396
0.605 0.373
0.621 0.383
0.777 0.420
0.734 0.408
0.853 0.455
720
0.463 0.315
0.450 0.312
0.640 0.350
0.632 0.396
0.645 0.394
0.626 0.382
0.864 0.472
0.717 0.396
1.500 0.805
Avg
0.430 0.295
0.414 0.294
0.620 0.336
0.621 0.396
0.625 0.383
0.610 0.376
0.764 0.416
0.705 0.395
1.011 0.541
Weather
96
0.150 0.202
0.162 0.212
0.152 0.220
0.197 0.281
0.196 0.255
0.217 0.296
0.300 0.384
0.458 0.490
0.369 0.406
192
0.198 0.246
0.204 0.248
0.209 0.261
0.237 0.312
0.237 0.296
0.276 0.336
0.598 0.544
0.658 0.589
0.416 0.435
336
0.245 0.286
0.254 0.286
0.280 0.306
0.298 0.353
0.283 0.335
0.339 0.380
0.578 0.521
0.797 0.652
0.455 0.454
720
0.324 0.342
0.326 0.337
0.365 0.359
0.352 0.288
0.345 0.381
0.403 0.428
1.059 0.741
0.869 0.675
0.535 0.520
Avg
0.229 0.271
0.237 0.270
0.236 0.287
0.271 0.334
0.265 0.317
0.309 0.360
0.634 0.548
0.696 0.602
0.444 0.454
ILI
24
1.974 0.886
2.063 0.881
2.317 0.934
2.527 1.000
2.398 1.040
3.228 1.260
5.764 1.677
4.480 1.444
5.914 1.734
36
2.028 0.976
1.868 0.892
1.972 0.900
2.615 1.007
2.646 1.088
2.679 1.080
4.755 1.467
4.799 1.467
6.631 1.845
48
2.353 1.115
1.790 0.884
2.238 0.900
2.359 0.972
2.614 1.086
2.622 1.078
4.763 1.469
4.800 1.468
6.736 1.857
60
2.425 1.203
1.979 0.957
2.027 0.928
2.487 1.016
2.804 1.146
2.857 1.15
5.264 1.564
5.278 1.560
6.870 1.879
Avg
2.195 1.045
1.925 0.903
2.139 0.901
2.497 1.004
2.616 1.090
2.847 1.144
5.137 1.544
4.839 1.485
6.538 1.829
1st count
5
5
4
0
0
0
0
0
0
Table 9: Long-term Forecasting Results (MSE, MAE). TEST uses GPT2-Medium as the backbone.
The past sequence length is set as 36 for ILI and 96 for the others. All the results are averaged from
4 different prediction lengths, that is {24, 36, 48, 60} for ILI and {96, 192, 336, 720} for the others.
A.3.3
LONG-TERM FORECASTING
We follow the classical experiment settings and the results of SOTA models in Wu et al. (2023)
(ICLR 2023). The results are shown in Table 9. Overall, TEST achieves comparable performance
to SOTA models TimesNet and Dlinear, and outperforms other baselines.
A.3.4
FEW-SHOT FORECASTING
For the few-shot forecasting task, only 10% percentage timesteps of training data are used, and the
other two parts remain unchanged. We follow the classical experiment settings and the results of
SOTA models in Zhou et al. (2023) (NeurIPS 2023). The results are shown in Table 10. Overall,
TEST has comparable performance with the SOTA baselines PatchTST and Dlinear, and SOTA
LLM for TS method GPT4TS.
19
Published as a conference paper at ICLR 2024
Methods
TEST
GPT4TS
DLinear
PatchTST
TimesNet
FEDformer
Autoformer
Stationary
ETSformer
LightTS
Informer
Reformer
Weather
96
0.163 0.213
0.163 0.215
0.171 0.224
0.165 0.215
0.184 0.230
0.188 0.253
0.221 0.297
0.192 0.234
0.199 0.272
0.217 0.269
0.374 0.401
0.335 0.380
192
0.230 0.263
0.210 0.254
0.215 0.263
0.210 0.257
0.245 0.283
0.250 0.304
0.270 0.322
0.269 0.295
0.279 0.332
0.259 0.304
0.552 0.478
0.522 0.462
336
0.278 0.282
0.256 0.292
0.258 0.299
0.259 0.297
0.305 0.321
0.312 0.346
0.320 0.351
0.370 0.357
0.356 0.386
0.303 0.334
0.724 0.541
0.715 0.535
720
0.301 0.328
0.321 0.339
0.320 0.346
0.332 0.346
0.381 0.371
0.387 0.393
0.390 0.396
0.441 0.405
0.437 0.448
0.377 0.382
0.739 0.558
0.611 0.500
Avg
0.243 0.272
0.238 0.275
0.241 0.283
0.242 0.279
0.279 0.301
0.284 0.324
0.300 0.342
0.318 0.323
0.318 0.360
0.289 0.322
0.597 0.495
0.546 0.469
ETTh1
96
0.455 0.457
0.458 0.456
0.492 0.495
0.516 0.485
0.861 0.628
0.512 0.499
0.613 0.552
0.918 0.639
1.112 0.806
1.298 0.838
1.179 0.792
1.184 0.790
192
0.572 0.519
0.570 0.516
0.565 0.538
0.598 0.524
0.797 0.593
0.624 0.555
0.722 0.598
0.915 0.629
1.155 0.823
1.322 0.854
1.199 0.806
1.295 0.850
336
0.611 0.531
0.608 0.535
0.721 0.622
0.657 0.550
0.941 0.648
0.691 0.574
0.750 0.619
0.939 0.644
1.179 0.832
1.347 0.870
1.202 0.811
1.294 0.854
720
0.723 0.594
0.725 0.591
0.986 0.743
0.762 0.610
0.877 0.641
0.728 0.614
0.721 0.616
0.887 0.645
1.273 0.874
1.534 0.947
1.217 0.825
1.223 0.838
Avg
0.479 0.525
0.590 0.525
0.691 0.600
0.633 0.542
0.869 0.628
0.639 0.561
0.702 0.596
0.915 0.639
1.180 0.834
1.375 0.877
1.199 0.809
1.249 0.833
ETTh2
96
0.332 0.374
0.331 0.374
0.357 0.411
0.353 0.389
0.378 0.409
0.382 0.416
0.413 0.451
0.389 0.411
0.678 0.619
2.022 1.006
3.837 1.508
3.788 1.533
192
0.401 0.433
0.402 0.411
0.569 0.519
0.403 0.414
0.490 0.467
0.478 0.474
0.474 0.477
0.473 0.455
0.785 0.666
2.329 1.104
3.856 1.513
3.552 1.483
336
0.408 0.440
0.406 0.433
0.671 0.572
0.426 0.441
0.537 0.494
0.504 0.501
0.547 0.543
0.507 0.480
0.839 0.694
2.453 1.122
3.952 1.526
3.395 1.526
720
0.459 0.480
0.449 0.464
0.824 0.648
0.477 0.480
0.510 0.491
0.499 0.509
0.516 0.523
0.477 0.472
1.273 0.874
3.816 1.407
3.842 1.503
3.205 1.401
Avg
0.401 0.432
0.397 0.421
0.605 0.538
0.415 0.431
0.479 0.465
0.466 0.475
0.488 0.499
0.462 0.455
0.894 0.713
2.655 1.160
3.872 1.513
3.485 1.486
ETTm1
96
0.392 0.401
0.390 0.404
0.352 0.392
0.410 0.419
0.583 0.501
0.578 0.518
0.774 0.614
0.761 0.568
0.911 0.688
0.921 0.682
1.162 0.785
1.442 0.847
192
0.423 0.426
0.429 0.423
0.382 0.412
0.437 0.434
0.630 0.528
0.617 0.546
0.754 0.592
0.781 0.574
0.955 0.703
0.957 0.701
1.172 0.793
1.444 0.862
336
0.471 0.444
0.469 0.439
0.419 0.434
0.476 0.454
0.725 0.568
0.998 0.775
0.869 0.677
0.803 0.587
0.991 0.719
0.998 0.716
1.227 0.908
1.450 0.866
720
0.552 0.501
0.569 0.498
0.490 0.477
0.681 0.556
0.769 0.549
0.693 0.579
0.810 0.630
0.844 0.581
1.062 0.747
1.007 0.719
1.207 0.797
1.366 0.850
Avg
0.574 0.443
0.464 0.441
0.411 0.429
0.501 0.466
0.677 0.537
0.722 0.605
0.802 0.628
0.797 0.578
0.980 0.714
0.971 0.705
1.192 0.821
1.426 0.856
ETTm2
96
0.233 0.262
0.188 0.269
0.213 0.303
0.191 0.274
0.212 0.285
0.291 0.399
0.352 0.454
0.229 0.308
0.331 0.430
0.813 0.688
3.203 1.407
4.195 1.628
192
0.303 0.302
0.251 0.309
0.278 0.345
0.252 0.317
0.270 0.323
0.307 0.379
0.694 0.691
0.291 0.343
0.400 0.464
1.008 0.768
3.112 1.387
4.042 1.601
336
0.359 0.341
0.307 0.346
0.338 0.385
0.306 0.353
0.323 0.353
0.543 0.559
2.408 1.407
0.348 0.376
0.469 0.498
1.031 0.775
3.255 1.421
3.963 1.585
720
0.452 0.419
0.426 0.417
0.436 0.440
0.433 0.427
0.474 0.449
0.712 0.614
1.913 1.166
0.461 0.438
0.589 0.557
1.096 0.791
3.909 1.543
3.711 1.532
Avg
0.317 0.309
0.293 0.335
0.316 0.368
0.296 0.343
0.320 0.353
0.463 0.488
1.342 0.930
0.332 0.366
0.447 0.487
0.987 0.756
3.370 1.440
3.978 1.587
Electricity
96
0.143 0.235
0.139 0.237
0.150 0.253
0.140 0.238
0.299 0.373
0.231 0.323
0.261 0.348
0.420 0.466
0.599 0.587
0.350 0.425
1.259 0.919
0.993 0.784
192
0.158 0.255
0.156 0.252
0.164 0.264
0.160 0.255
0.305 0.379
0.261 0.356
0.338 0.406
0.411 0.459
0.620 0.598
0.376 0.448
1.160 0.873
0.938 0.753
336
0.176 0.275
0.175 0.270
0.181 0.282
0.180 0.276
0.319 0.391
0.360 0.445
0.410 0.474
0.434 0.473
0.662 0.619
0.428 0.485
1.157 0.872
0.925 0.745
720
0.230 0.311
0.233 0.317
0.223 0.321
0.241 0.323
0.369 0.426
0.530 0.585
0.715 0.685
0.510 0.521
0.757 0.664
0.611 0.597
1.203 0.898
1.004 0.790
Avg
0.176 0.269
0.176 0.269
0.180 0.280
0.180 0.273
0.323 0.392
0.346 0.427
0.431 0.478
0.444 0.480
0.660 0.617
0.441 0.489
1.195 0.891
0.965 0.768
Traffic
96
0.415 0.317
0.414 0.297
0.419 0.298
0.403 0.289
0.719 0.416
0.639 0.400
0.672 0.405
1.412 0.802
1.643 0.855
1.157 0.636
1.557 0.821
1.527 0.815
192
0.425 0.300
0.426 0.301
0.434 0.305
0.415 0.296
0.748 0.428
0.637 0.416
0.727 0.424
1.419 0.806
1.641 0.854
1.207 0.661
1.454 0.765
1.538 0.817
336
0.436 0.310
0.434 0.303
0.449 0.313
0.426 0.304
0.853 0.471
0.655 0.427
0.749 0.454
1.443 0.815
1.711 0.878
1.334 0.713
1.521 0.812
1.550 0.819
720
0.489 0.338
0.487 0.337
0.484 0.336
0.474 0.331
1.485 0.825
0.722 0.456
0.847 0.499
1.539 0.837
2.660 1.157
1.292 0.726
1.605 0.846
1.588 0.833
Avg
0.441 0.316
0.440 0.310
0.447 0.313
0.430 0.305
0.951 0.535
0.663 0.425
0.749 0.446
1.453 0.815
1.914 0.936
1.248 0.684
1.534 0.811
1.551 0.821
1st count
5
5
4
0
0
0
0
0
0
0
0
0
Table 10: Few-shot Forecasting Results (MSE, MAE). TEST uses GPT2-Medium as the backbone.
All the results are averaged from 4 different prediction lengths, that is {96, 192, 336, 720}.
Methods
M4
M3
TOURISM
ELECTR
Metric
sMAPE
sMAPE
MAPE
ND×100
Average
1st count
N-BEATS
11.70
12.44
18.82
17.8
15.19
2
DLinear
15.33
14.03
28.51
17.6
18.86
0
TimesNet
13.55
14.17
28.84
19.3
18.96
0
PatchTST
13.22
13.06
27.10
17.3
17.67
0
ETSformer
27.74
16.03
180.40
44.2
67.09
0
LightTS
13.62
17.90
66.99
19.6
29.52
0
Stationary
13.32
15.29
43.75
22.0
23.59
0
FEDformer
15.04
13.53
31.55
18.4
19.63
0
Autoformer
20.02
15.87
40.39
33.9
27.54
0
Informer
19.04
15.82
35.82
21.2
22.97
0
Reformer
14.09
13.37
25.48
21.6
18.63
0
GPT2(6)
13.12
13.06
22.14
17.2
16.38
1
TEST
13.10
12.56
18.17
17.9
15.93
1
Table 11: Zero-shot learning results. Dataset-specific metrics aggregated over each dataset. A lower
value indicates better performance. The source dataset of M3, Tourism, Electricity are M4. For M4,
the source data for N-BEATS is FRED, and M3 for other models.
A.3.5
ZERO-SHOT FORECASTING
Zero-shot Forecasting task can evaluate the cross datasets adaption ability. Which means that the
method is evaluated to perform on a dataset (without any training data from this dataset) when it is
trained from another dataset. The results are summarized in Table 11. TEST outperforms all recent
SOTA methods. TEST is comparable to N-BEATS without any meta-learning design and GPT4TS.
20
Published as a conference paper at ICLR 2024
A.4
CLASSIFICATION TASKS
All the deep learning networks are implemented in PyTorch and trained on NVIDIA V100 32GB
GPUs. We use Area Under Curve of Receiver Operating Characteristic (AUC-ROC) as metrics.
Meanwhile, we compute the average rank, the number of Top-1, Top-3, and Top-5 accuracy to show
the robustness of different methods. All experiments are repeated 3 times and the mean of the
metrics is used in the final results.
A.4.1
DATASET DETAILS
We present accuracy scores for all 30 kinds of multivariate TS datasets in UEA archive Bagnall et al.
(2018). UEA consists of 30 different datasets. Details of these datasets are shown in Table 12
Dataset
Train Cases
Test Cases
Dimensions
Length
Classes
ArticularyWordRecognition
275
30
9
144
25
AtrialFibrillation
15
15
2
640
3
BasicMotions
40
40
4
100
4
CharacterTrajectories
1422
1436
3
182
20
Cricket
108
72
6
17984
5
DuckDuckGeese
60
40
1345
270
5
EigenWorms
128
131
6
17984
5
Epilepsy
137
138
3
206
4
EthanolConcentration
261
263
3
1751
4
ERing
30
20
4
65
6
FaceDetection
5890
3524
144
62
2
FingerMovements
316
100
28
50
2
HandMovementDirection
320
147
10
400
4
Handwriting
150
850
3
152
26
Heartbeat
204
105
61
495
2
JapaneseVowels
270
370
12
29
9
Libras
180
280
2
45
15
LSST
2459
2466
6
36
14
InsectWingbeat
30000
20000
200
78
10
MotorImagery
278
100
64
3000
2
NATOPS
180
180
24
51
6
PenDigits
7494
3498
2
8
10
PEMS-SF
267
173
963
144
7
Phoneme
3315
3353
11
217
39
RacketSports
151
152
6
30
4
SelfRegulationSCP1
268
293
6
896
2
SelfRegulationSCP2
200
180
7
1152
2
SpokenArabicDigits
6599
2199
13
93
10
StandWalkJump
12
15
4
2500
3
UWaveGestureLibrary
120
320
3
315
8
Table 12: UEA Classification Dataset Details
A.4.2
BASELINE DETAILS
For classification, we refer to the SOTA methods: Three benchmarks Bostrom et al. (2018) (EDI,
DTWI, and DTWD) are based on Euclidean Distance, dimension-independent dynamic time warp-
ing, and dimension-dependent dynamic time warping; MLSTM-FCNs Karim et al. (2019) applies an
LSTM layer and stacked CNN layers to generate features; WEASEL-MUSE Sch¨
afer & Leser (2017)
is a bag-of-pattern based approach which extracts and represents features to words. Scalable Rep-
resentation Learning (SRL) Franceschi et al. (2019a) employs negative sampling techniques with
an encoder-based architecture to learn the representation; TapNet Zhang et al. (2020) is a recent
model with an attentional prototype learning in its deep learning-based network; ShapeNet Li et al.
(2021a) projects the subsequences into a unified space and applies clustering to find the shapelets;
Rocket and MiniRocket Dempster et al. (2021) use random convolutional kernels to extract features
from univariate time series; RL-PAM Gao et al. (2022) introduces reinforcement learning to the
pattern mining; TStamp Transformer Zerveas et al. (2021) takes the values at each timestamp as the
input for a transformer encoder; SVP-T Zuo et al. (2023) uses differnt variables and positions (time
interval) as the inputs (shape-level).
21
Published as a conference paper at ICLR 2024
A.4.3
MULTIVARIATE TIME SERIES CLASSIFICATION
We follow the classical experiment settings in multivariate time series classification tasks Bostrom
et al. (2018). The results are shown in Table 13. Overall, TEST achieves comparable performance
to SOTA models and outperforms most baselines.
EDI
DTWI
DTWD
MLSTM-FCNs
WEASEL+MUSE
SRL
TapNet
ShapeNet
Rocket
MiniRocket
RLPAM
TStamp
SVP-T
TEST
AWR
0.970
0.980
0.987
0.973
0.990
0.987
0.987
0.987
0.996
0.992
0.923
0.983
0.993
0.994
AF
0.267
0.267
0.220
0.267
0.333
0.133
0.333
0.400
0.249
0.133
0.733
0.200
0.400
0.420
BM
0.676
1.000
0.975
0.950
1.000
1.000
1.000
1.000
0.990
1.000
1.000
0.975
1.000
1.000
CT
0.964
0.969
0.989
0.985
0.990
0.994
0.997
0.980
N/A
0.993
0.978
N/A
0.990
0.989
CK
0.944
0.986
1.000
0.917
1.000
0.986
0.958
0.986
1.000
0.986
1.000
0.958
1.000
1.000
DDG
0.275
0.550
0.600
0.675
0.575
0.675
0.575
0.725
0.461
0.650
0.700
0.480
0.700
0.675
EW
0.549
N/A
0.618
0.504
0.890
0.878
0.489
0.878
0.863
0.962
0.908
N/A
0.923
0.878
EP
0.666
0.978
0.964
0.761
1.000
0.957
0.971
0.987
0.991
1.000
0.978
0.920
0.986
0.985
ER
0.133
0.914
0.929
0.133
0.133
0.133
0.133
0.133
0.981
0.981
0.819
0.933
0.937
0.937
EC
0.293
0.304
0.323
0.373
0.430
0.236
0.323
0.312
0.447
0.468
0.369
0.337
0.331
0.373
FD
0.519
0.000
0.529
0.545
0.545
0.528
0.556
0.602
0.694
0.620
0.621
0.681
0.512
0.512
FM
0.550
0.520
0.530
0.580
0.490
0.540
0.530
0.580
0.553
0.550
0.640
0.776
0.600
0.770
HMD
0.278
0.306
0.231
0.365
0.365
0.270
0.378
0.338
0.446
0.392
0.635
0.608
0.392
0.444
HW
0.200
0.316
0.286
0.286
0.605
0.533
0.357
0.452
0.567
0.507
0.522
0.305
0.433
0.431
HB
0.619
0.658
0.717
0.663
0.727
0.737
0.751
0.756
0.718
0.771
0.779
0.712
0.790
0.791
IW
0.128
N/A
N/A
0.167
N/A
0.160
0.208
0.250
N/A
0.595
0.352
0.684
0.184
0.572
JV
0.924
0.959
0.949
0.976
0.973
0.989
0.965
0.984
0.965
0.989
0.935
0.994
0.978
0.991
LB
0.833
0.894
0.870
0.856
0.878
0.867
0.850
0.856
0.906
0.922
0.794
0.844
0.883
0.884
LSST
0.456
0.575
0.551
0.373
0.590
0.558
0.568
0.590
0.632
0.643
0.643
0.381
0.666
0.595
MI
0.510
N/A
0.500
0.510
0.500
0.540
0.590
0.610
0.531
0.550
0.610
N/A
0.650
0.650
NT
0.850
0.850
0.883
0.889
0.870
0.944
0.939
0.883
0.885
0.928
0.950
0.900
0.906
0.902
PD
0.705
0.939
0.977
0.978
0.948
0.983
0.980
0.977
0.996
N/A
0.982
0.974
0.983
0.979
PM
0.973
0.734
0.711
0.699
0.000
0.688
0.751
0.751
0.856
0.522
0.632
0.919
0.867
0.860
PH
0.104
0.151
0.151
0.110
0.190
0.246
0.175
0.298
0.284
0.292
0.175
0.088
0.176
0.196
RS
0.868
0.842
0.803
0.803
0.934
0.862
0.868
0.882
0.928
0.868
0.868
0.829
0.842
0.851
SCP1
0.771
0.765
0.775
0.874
0.710
0.846
0.652
0.782
0.866
0.925
0.802
0.925
0.884
0.870
SCP2
0.483
0.533
0.539
0.472
0.460
0.556
0.550
0.578
0.514
0.522
0.632
0.589
0.600
0.579
SAD
0.967
0.959
0.963
0.990
0.982
0.956
0.983
0.975
0.630
0.620
0.621
0.993
0.986
0.982
SWJ
0.200
0.333
0.200
0.067
0.333
0.400
0.400
0.533
0.456
0.333
0.667
0.267
0.467
0.468
UGL
0.881
0.868
0.903
0.891
0.916
0.884
0.894
0.906
0.944
0.938
0.944
0.903
0.941
0.933
Avg.Rank
10.933
9.480
8.821
8.756
6.890
7.120
6.956
5.523
5.423
5.013
5.059
7.484
4.032
4.012
Num.Top-1
1
1
1
0
5
1
2
3
5
5
6
4
4
6
Num.Top-3
1
2
1
1
6
6
3
7
12
14
16
9
17
18
Num.Top-5
2
2
3
5
15
12
13
17
16
20
19
10
23
24
P-value
0.000
0.000
0.000
0.000
0.006
0.003
0.000
0.118
0.217
0.765
0.967
0.047
0.044
0.040
Table 13: Accuracies on All Datasets of the UEA Archive
A.5
REPRESENTATION TASKS
We assess the quality of our learned representations on supervised tasks in a standard manner by
using them for time series classification Franceschi et al. (2019b). All the deep learning networks
are implemented in PyTorch and trained on NVIDIA V100 32GB GPUs. We use Area Under Curve
of Receiver Operating Characteristic (AUC-ROC) as metrics.
A.5.1
DATASET DETAILS
We represent the results for all 128 kinds of univariate TS datasets in UCR archive Dau et al. (2019),
which is a standard set of varied univariate datasets.
A.5.2
BASELINE DETAILS
The compared method includes SOTAs of unsupervised time series representation:
T-Loss
Franceschi et al. (2019b), TS-TCC Eldele et al. (2021b), TST Zerveas et al. (2021) and TNC Tonek-
aboni et al. (2021), TS2Vec Yue et al. (2022).
A.5.3
CLASSIFICATION BASED ON REPRESENTATION
We assess the quality of our learned representations on supervised tasks in a standard manner by
using them for time series classification Franceschi et al. (2019b). In this setting, we show that our
22
Published as a conference paper at ICLR 2024
method outperforms SOTA unsupervised methods, and notably achieves performance close to the
supervised SOTA method as shown in Table 14.
For each considered dataset with a train / test split, we unsupervisedly train an encoder using its train
set. We then train an SVM with radial basis function kernel on top of the learned features using the
train labels of the dataset, and output the corresponding classification score on the test set.
TEST
TCN
TS2Vec
T-Loss
TNC
Adiac
0.776
0.768
0.765
0.675
0.726
ArrowHead
0.825
0.857
0.817
0.766
0.703
Beef
0.766
0.768
0.633
0.667
0.733
BeetleFly
0.853
0.900
0.900
0.800
0.850
BirdChicken
0.808
0.803
0.800
0.850
0.750
Car
0.883
0.834
0.700
0.833
0.683
CBF
1.000
1.000
1.000
0.983
0.983
ChlorineConcentration
0.810
0.832
0.812
0.749
0.760
CinCECGTorso
0.815
0.829
0.825
0.713
0.669
Coffee
1.000
1.000
1.000
1.000
1.000
Computers
0.632
0.660
0.660
0.664
0.684
CricketX
0.802
0.787
0.805
0.713
0.623
CricketY
0.754
0.749
0.769
0.728
0.597
CricketZ
0.787
0.794
0.790
0.708
0.682
DiatomSizeReduction
0.980
0.985
0.987
0.984
0.993
DistalPhalanxOutlineCorrect
0.776
0.761
0.757
0.775
0.754
DistalPhalanxOutlineAgeGroup
0.714
0.727
0.719
0.727
0.741
DistalPhalanxTW
0.662
0.698
0.683
0.676
0.669
Earthquakes
0.746
0.748
0.748
0.748
0.748
ECG200
0.893
0.920
0.880
0.940
0.830
ECG5000
0.935
0.935
0.934
0.933
0.937
ECGFiveDays
1.000
1.000
1.000
1.000
0.999
ElectricDevices
0.714
0.721
0.719
0.707
0.700
FaceAll
0.789
0.771
0.805
0.786
0.766
FaceFour
0.834
0.932
0.932
0.920
0.659
FacesUCR
0.939
0.924
0.926
0.884
0.789
FiftyWords
0.781
0.771
0.774
0.732
0.653
Fish
0.937
0.926
0.937
0.891
0.817
FordA
0.940
0.936
0.948
0.928
0.902
FordB
0.789
0.794
0.807
0.793
0.733
GunPoint
0.983
0.980
0.987
0.980
0.967
Ham
0.714
0.714
0.724
0.724
0.752
HandOutlines
0.918
0.925
0.930
0.922
0.930
Haptics
0.510
0.526
0.536
0.490
0.474
Herring
0.625
0.644
0.609
0.594
0.594
InlineSkate
0.389
0.418
0.407
0.371
0.378
InsectWingbeatSound
0.620
0.630
0.624
0.597
0.549
ItalyPowerDemand
0.969
0.925
0.960
0.954
0.928
LargeKitchenAppliances0
0.855
0.845
0.875
0.789
0.776
Lightning2
0.846
0.869
0.820
0.869
0.869
Lightning7
0.866
0.863
0.822
0.795
0.767
Mallat
0.915
0.944
0.873
0.951
0.871
Meat
0.950
0.952
0.967
0.950
0.917
MedicalImages
0.792
0.789
0.793
0.750
0.754
MiddlePhalanxOutlineCorrect
0.811
0.838
0.825
0.825
0.818
MiddlePhalanxOutlineAgeGroup
0.636
0.636
0.630
0.656
0.643
MiddlePhalanxTW
0.591
0.584
0.578
0.591
0.571
MoteStrain
0.857
0.861
0.863
0.851
0.825
NonInvasiveFetalECGThorax1
0.923
0.930
0.919
0.878
0.898
NonInvasiveFetalECGThorax2
0.940
0.938
0.935
0.919
0.912
OliveOil
0.903
0.901
0.940
0.867
0.833
OSULeaf
0.872
0.851
0.843
0.760
0.723
PhalangesOutlinesCorrect
0.794
0.809
0.823
0.784
0.787
Phoneme
0.296
0.312
0.309
0.276
0.180
Plane
1.000
1.000
0.990
0.990
1.000
ProximalPhalanxOutlineCorrect
0.876
0.887
0.900
0.859
0.866
ProximalPhalanxOutlineAgeGroup
0.844
0.837
0.829
0.844
0.854
ProximalPhalanxTW
0.785
0.824
0.805
0.771
0.810
RefrigerationDevices
0.587
0.586
0.589
0.515
0.565
ScreenType
0.405
0.414
0.397
0.416
0.509
ShapeletSim
0.989
1.000
0.994
0.672
0.589
ShapesAll
0.897
0.902
0.905
0.848
0.788
SmallKitchenAppliances
0.723
0.731
0.733
0.677
0.725
SonyAIBORobotSurface1
0.874
0.903
0.900
0.902
0.804
SonyAIBORobotSurface2
0.893
0.871
0.889
0.889
0.834
StarLightCurves
0.970
0.968
0.971
0.964
0.968
Strawberry
0.962
0.966
0.965
0.954
0.951
SwedishLeaf
0.939
0.945
0.942
0.914
0.880
Symbols
0.973
0.977
0.972
0.963
0.885
SyntheticControl
0.997
0.997
0.993
0.987
1.000
23
Published as a conference paper at ICLR 2024
ToeSegmentation1
0.933
0.917
0.947
0.939
0.864
ToeSegmentation2
0.915
0.899
0.900
0.900
0.831
Trace
1.000
1.000
1.000
0.990
1.000
TwoLeadECG
0.982
0.986
0.987
0.999
0.993
TwoPatterns
1.000
1.000
1.000
0.999
1.000
UWaveGestureLibraryX
0.810
0.795
0.801
0.785
0.781
UWaveGestureLibraryY
0.729
0.719
0.720
0.710
0.697
UWaveGestureLibraryZ
0.761
0.774
0.768
0.757
0.721
UWaveGestureLibraryAll
0.935
0.930
0.934
0.896
0.903
Wafer
0.995
0.998
0.998
0.992
0.994
Wine
0.788
0.880
0.889
0.815
0.759
WordSynonyms
0.699
0.679
0.704
0.691
0.630
Worms
0.704
0.701
0.701
0.727
0.623
WormsTwoClass
0.805
0.806
0.753
0.792
0.727
Yoga
0.883
0.883
0.877
0.837
0.812
ACSF1
0.849
0.910
0.910
0.900
0.730
AllGestureWiimoteX
0.744
0.777
0.751
0.763
0.703
AllGestureWiimoteY
0.754
0.796
0.774
0.726
0.699
AllGestureWiimoteZ
0.744
0.749
0.770
0.723
0.646
BME
0.979
0.992
0.980
0.993
0.973
Chinatown
0.969
0.964
0.959
0.951
0.977
Crop
0.753
0.754
0.758
0.722
0.738
EOGHorizontalSignal
0.544
0.569
0.522
0.605
0.442
EOGVerticalSignal
0.467
0.503
0.472
0.434
0.392
EthanolLevel
0.480
0.468
0.484
0.382
0.424
FreezerRegularTrain
0.983
0.996
0.983
0.956
0.991
FreezerSmallTrain
0.893
0.875
0.872
0.933
0.982
Fungi
0.967
0.958
0.946
1.000
0.527
GestureMidAirD1
0.637
0.608
0.615
0.608
0.431
GestureMidAirD2
0.508
0.479
0.515
0.546
0.362
GestureMidAirD3
0.346
0.492
0.300
0.285
0.292
GesturePebbleZ1
0.878
0.930
0.884
0.919
0.378
GesturePebbleZ2
0.842
0.873
0.848
0.899
0.316
GunPointAgeSpan
0.994
0.987
0.968
0.994
0.984
GunPointMaleVersusFemale
1.000
1.000
1.000
0.997
0.994
GunPointOldVersusYoung
1.000
1.000
1.000
1.000
1.000
HouseTwenty
0.944
0.917
0.941
0.933
0.782
InsectEPGRegularTrain
1.000
1.000
1.000
1.000
1.000
InsectEPGSmallTrain
1.000
1.000
1.000
1.000
1.000
MelbournePedestrian
0.954
0.959
0.956
0.944
0.942
MixedShapesRegularTrain
0.915
0.917
0.922
0.905
0.911
MixedShapesSmallTrain
0.884
0.861
0.856
0.860
0.813
PickupGestureWiimoteZ
0.800
0.823
0.760
0.740
0.620
PigAirwayPressure
0.524
0.630
0.683
0.510
0.413
PigArtPressure
0.962
0.966
0.966
0.928
0.808
PigCVP
0.803
0.815
0.870
0.788
0.649
PLAID
0.551
0.561
0.549
0.555
0.495
PowerCons
0.967
0.961
0.972
0.900
0.933
Rock
0.660
0.700
0.700
0.580
0.580
SemgHandGenderCh2
0.952
0.963
0.962
0.890
0.882
SemgHandSubjectCh2
0.897
0.860
0.891
0.789
0.593
SemgHandMovementCh2
0.944
0.952
0.942
0.920
0.820
SmoothSubspace
0.967
0.980
0.993
0.960
0.913
UMD
1.000
1.000
0.993
0.993
0.993
Avg
0.826
0.832
0.827
0.806
0.761
Table 14: Accuracies on All Datasets of the UCR Archive
A.6
ABLATION
TEST contains two contrastive learning strategies: instance-wise contrast and feature-wise contrast,
and can use different text embedding vectors as prototypes, we show the impact of these strategies.
A.6.1
CONTRASTIVE LEARNING STRATEGIES
As shown in Table 15 and 16, both two contrastive learning strategies can increase the accuracy.
A.6.2
TEXT PROTOTYPES
The number and the type of text prototypes will lead to different results.
As shown in Table 17. We randomly select 1, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22 prototypes. The
accuracy and number are basically positively correlated. The results of 10 prototypes are almost
optimal.
24
Published as a conference paper at ICLR 2024
ETTm1
ETTm2
ETTh1
ETTh2
Electricity
Traffic
Weather
ILI
Instance-wise
0.621 0.550
0.755 0.630
0.493 0.453
0.580 0.612
0.293 0.396
0.788 0.620
0.463 0.349
3.301 4.535
Feature-wise
0.741 0.559
0.793 0.634
0.699 0.493
0.585 0.628
0.286 0.390
0.821 0.629
0.453 0.388
3.139 5.931
TEST
0.353 0.382
0.293 0.334
0.414 0.431
0.331 0.380
0.162 0.253
0.430 0.295
0.229 0.271
2.195 1.045
Table 15: Long-term Forecasting Results (MSE, MAE). TEST uses different contrastive learning
stragegy. All the results are averaged from 4 different prediction lengths, that is {24, 36, 48, 60} for
ILI and {96, 192, 336, 720} for the others. The results are average.
TEST Instance-wise Feature-wise TimesNet N-BEATS ETSformer DLinear FEDformer Stationary Autoformer Informer Reformer
SMAPE 11.927
13.525
16.987
11.829
11.851
14.718
13.639
12.840
12.780
12.909
14.086
18.200
MASE
1.613
2.111
3.265
1.585
1.599
2.408
2.095
1.701
1.756
1.771
3.010
4.223
OWA
0.861
1.051
1.480
0.851
0.855
1.172
1.051
0.918
0.930
0.939
1.230
1.775
Table 16: Short-term Forecasting Task on M4. The prediction lengths are in [6, 48] and results are
averaged from several datasets.
As shown in Table 18. We randomly select 10 prototypes 10 times. The accuracy is basically
consistent. Therefore, the type of prototypes has almost no impact on the results.
1
2
4
6
8
10
12
14
16
18
20
22
SMAPE
30.901
20.201
17.415
16.997
13.820
11.927
11.710
11.638
11.094
11.098
10.953
10.885
MASE
6.590
4.515
3.910
3.595
2.580
1.613
1.408
1.195
1.301
1.306
1.471
1.310
OWA
3.779
2.050
1.451
1.484
0.990
0.861
0.872
0.801
0.910
0.902
0.838
0.830
Table 17: Short-term Forecasting Task on M4. The results are reported with different number of text
prototypes.
1
2
3
4
5
6
7
8
9
10
Avg.
Std.
SMAPE
11.907
11.920
11.927
11.926
11.925
11.925
11.950
11.890
11.728
11.910
11.901
0.059
MASE
1.612
1.610
1.653
1.603
1.619
1.620
1.625
1.623
1.613
1.591
1.617
0.016
OWA
0.870
0.872
0.872
0.872
0.872
0.872
0.849
0.862
0.876
0.870
0.868,
0.009
Table 18: Short-term Forecasting Task on M4. The results are reported with different types of text
prototypes.
Considering why the type of text prototype does not significantly affect results, we figure that in high
dimensional space, almost all vectors are pairwise orthogonal Hopcroft & Kannan (2013). Which
means that, in high-dimensional space, it is easy to generate a large number of almost orthogonal
vectors to represent different attributes. Thus, randomly selecting the same number of vectors, the
represented space size and expressed number of features are almost the same. Therefore, the key is
the number rather than the type.
In terms of probability, “two vectors orthogonal” is equivalent to “two vectors perpendicular” is
equivalent to “two vectors uncorrelated” is equivalent to “cos θ = 0”. For a n-dimensional space,
randomly two vectors have: ∀ϵ, limn→∞P(| cos θ| > ϵ) = 0. As shown in Figure 7, as the
dimension increases, the probability of two random vectors being similar decreases. For LLM,
n > 1024, P(θ = 0) < 0.00001.
Figure 7: Probability Density of the Angle between Two Random Vectors in n-dimensional Space
25
"
5,6,MEGAnno+: A Human-LLM Collaborative Annotation System,"Hannah Kim, Kushan Mitra, Rafael Li Chen, Sajjadur Rahman, Dan Zhang","Large language models (LLMs) can label data faster and cheaper than humans
for various NLP tasks. Despite their prowess, LLMs may fall short in
understanding of complex, sociocultural, or domain-specific context,
potentially leading to incorrect annotations. Therefore, we advocate a
collaborative approach where humans and LLMs work together to produce reliable
and high-quality labels. We present MEGAnno+, a human-LLM collaborative
annotation system that offers effective LLM agent and annotation management,
convenient and robust LLM annotation, and exploratory verification of LLM
labels by humans.","MEGAnno+: A Human-LLM Collaborative Annotation System
Hannah Kim, Kushan Mitra, Rafael Li Chen, Sajjadur Rahman, Dan Zhang
Megagon Labs
{hannah, kushan, rafael, sajjadur, dan_z}@megagon.ai
Abstract
Large language models (LLMs) can label data
faster and cheaper than humans for various
NLP tasks. Despite their prowess, LLMs may
fall short in understanding of complex, socio-
cultural, or domain-specific context, potentially
leading to incorrect annotations. Therefore,
we advocate a collaborative approach where
humans and LLMs work together to produce
reliable and high-quality labels. We present
MEGAnno+, a human-LLM collaborative an-
notation system that offers effective LLM agent
and annotation management, convenient and
robust LLM annotation, and exploratory verifi-
cation of LLM labels by humans. 1
1
Introduction
Data annotation has long been an essential step in
training machine learning (ML) models. Accurate
and abundant annotations significantly contribute
to improved model performance. Despite the re-
cent advancements of pre-trained Large Language
Models (LLM), high-quality labeled data remains
crucial in various use cases requiring retraining.
For instance, distilled models are often deployed
in scenarios where repeated usage of LLMs for
inference can be too costly (e.g., API calls) or time-
consuming (e.g., hosting on-premise). In special-
ized domains like medical and human resources, or-
ganizations often need customized models to meet
heightened accuracy requirements and ensure the
privacy of sensitive customer data. In addition to
the training step, accurate labeled data is also nec-
essary for evaluating and understanding of model
performance.
Recent explorations (Wang et al., 2021; Ding
et al., 2023) have showcased the potential of LLMs
in automating the data annotation process. Unlike
previous task-specific machine learning models,
LLMs exhibit remarkable flexibility to handle any
1Demo & video: https://meganno.github.io
textual labeling task as long as suitable prompts are
provided. Besides, compared to traditional annota-
tion relying solely on human labor, LLMs can usu-
ally generate labels faster and at a lower cost. For
example, hiring crowd workers for labeling may
encounter problems such as delays, higher cost,
difficulty in quality control (Douglas et al., 2023;
Sheehan, 2018; Litman et al., 2021; Garcia-Molina
et al., 2016). Studies (Gilardi et al., 2023) show
that LLMs can achieve near-human or even better-
than-human accuracy in some tasks. Furthermore,
downstream models trained with LLM-generated
labels may outperform directly using an LLM for
inference (Wang et al., 2021).
Despite these advancements, it is essential to
acknowledge that LLMs have limitations, neces-
sitating human intervention in the data annotation
process. One challenge is that the performance
of LLMs varies extensively across different tasks,
datasets, and labels (Zhu et al., 2023; Ziems et al.,
2023). LLMs often struggle to comprehend subtle
nuances or contexts in natural language, making
involvement of humans with social and cultural
understanding or domain expertise crucial. Addi-
tionally, LLMs may produce biased labels due to
potentially biased training data (Abid et al., 2021;
Sheng et al., 2021). In such cases, humans can
recognize potential biases and make ethical judge-
ments to correct them.
In this work, we present MEGAnno+, an anno-
tation system facilitating human-LLM collabora-
tion through efficient LLM annotation and selective
human verification. While LLM annotations are
gaining interest rapidly, a comprehensive investiga-
tion on how to onboard LLMs as annotators within
a human-in-the-loop framework in labeling tools
has not been conducted yet. For example, support-
ing LLM annotation requires not only user-friendly
communications with LLMs, but also a unified
backend capable of storing and managing LLM
models, labels, and additional artifacts. Efficient
1
arXiv:2402.18050v1  [cs.CL]  28 Feb 2024
human verification calls for a flexible search and
recommendation feature to steer human efforts to-
wards problematic LLM labels, along with a mech-
anism for humans to review and rectify LLM labels.
Throughout the paper, we explain how we achieve
this in our system, showcase a use case, and discuss
our findings.
We summarize our contributions as below:
• A human-LLM collaborative annotation sys-
tem that offers 1) effective management of
LLM agents, annotations, and artifacts, 2) con-
venient and robust interfacing with LLMs to
obtain labels, and 3) selective, exploratory ver-
ification of LLM labels by humans.
• A use case demonstrating the effectiveness of
our system.
• Practical considerations and discussion on
adopting LLMs as annotators.
2
Related Work
LLMs as annotators
There is growing interest
in utilizing LLMs as general-purpose annotators
for natural language tasks (Kuzman et al., 2023;
Zhu et al., 2023; Ziems et al., 2023). Wang et al.
(2021) find that GPT-3 can reduce labeling cost by
up to 96% for classification and generation tasks.
Similarly, Ding et al. (2023) evaluate GPT-3 for
labeling and augmenting data in classification and
token-level tasks. Other studies show that for some
classification tasks, LLMs can even outperform
crowdsourced annotators (Gilardi et al., 2023; He
et al., 2023; Törnberg, 2023).
Verification of LLM responses
To detect and
correct erroneous responses from LLMs, ap-
proaches to rank or filter LLM outputs have been
explored.
The most common method is using
model logits to measure model uncertainty (Wang
et al., 2021). More recently, Wang et al. (2024) pro-
pose training a verifier model using various signals
from LLMs’ input, labels, and explanations. Alter-
native methods include asking LLMs to verbalize
confidence scores (Lin et al., 2022) and calculating
consistency over prompt perturbations (Wang et al.,
2023; Xiong et al., 2023). Other line of works in-
vestigate self-verification, i.e., LLMs give feedback
on their own outputs and use them to refine them-
selves (Madaan et al., 2023; Cheng et al., 2023).
In our system, we focus on human verification of
LLM-generated labels and leave model verification
and self-verification as future work.
Annotation tools with AI/ML assistance
Ma-
chine learning models have proven effective in
assisting humans in various steps of the train-
ing data collection pipeline.
Annotation tools
and frameworks such as Prodigy (Montani and
Honnibal, 2018), HumanLoop (hum), Label Stu-
dio (Tkachenko et al., 2020-2022), and Label
Sleuth (Shnarch et al., 2022) all aim to enhance
the subset selection step with active learning ap-
proaches. ML models are also naturally used to
make predictions, serving as pre-labels. For in-
stance, INCEpTION (Klie et al., 2018) provides
annotation suggestions generated by ML models.
HumanLoop (hum) and Autolabel (aut) support the
annotation or augmentation of datasets using either
commercial or open-source LLMs. In this work,
we go beyond using LLMs to assist annotation for
human annotators or to replace human annotators.
Rather, MEGAnno+ advocates for a collaboration
between humans and LLMs with our dedicated sys-
tem design and annotation-verification workflows.
3
Design Considerations
Let us start with a motivating example of Moana,
a Data Scientist working at a popular newspaper.
Moana is tasked with training a model to analyze
the degree of agreement between user comments
and political opinion pieces — e.g., whether the
comments entail the opinion. Moana opts for LLM
annotation, but she encounters various challenges
in the process. Firstly, without any guidance for
prompting, she resorts to trial-and-error to eventu-
ally identify a suitable prompt for the task. Even so,
she must perform additional validations to ensure
that the annotated labels are within the space of
pre-defined labels. Moreover, the API calls to the
LLM can be unreliable, throwing exceptions such
as timing out and rate limit violations, requiring
her to handle such errors manually. Next, Moana
lacks the confidence to train a downstream model
without verifying the LLM annotations. However,
without any assistance in reviewing potential an-
notation candidates for verification, she has to go
through all the annotations, which can be time-
consuming. Finally, she has to manually save used
model configurations to reuse the model for addi-
tional datasets.
From Moana’s example, we summarize our de-
sign requirements for a human-LLM collaborative
annotation system as follows:
1. LLM annotation
2
Figure 1: MEGAnno+ system architecture and LLM-integrated workflow. With MEGAnno+ client, users can
interact with the back-end service that consists of web and database servers through programmatic interfaces and
UI widgets. The middle notebook shows our workflow where cell [2] is LLM annotation and cell [3] is human
verification.
(a) [Convenient] Annotation workflow in-
cluding pre-processing, API calling, and
post-processing is automated.
(b) [Customizable] Flexibly modify model
configuration and prompt templates.
(c) [Robust] Resolvable errors are handled
by the system.
(d) [Reusable] Store used LLM models and
prompt templates for reuse.
(e) [Metadata] LLM artifacts are captured
and stored as annotation metadata.
2. Human verification
(a) [Selective] Select verification candidates
by search query or recommendation.
(b) [Exploratory] Filter, sort, and search by
labels and available metadata program-
matically and in a UI.
To satisfy these design requirements,
we
implement
our
system
as
an
extension
to
MEGAnno (Zhang et al., 2022), an in-notebook
exploratory annotation tool. Its flexible search and
intelligent recommendations enable efficient allo-
cation of human and LLM resources toward crucial
data points (R 2a,2b). Additionally, MEGAnno
provides a cohesive backend for the storage of data,
annotations, and auxiliary information (R 1d,1e).
4
System
4.1
System Overview
MEGAnno+ is designed to provide a convenient
and robust workflow for users to utilize LLMs in
text annotation. To use our tool, users operate
within their Jupyter notebook (Kluyver et al., 2016)
with the MEGAnno+ client installed.
Our human-LLM collaborative workflow (Fig. 1)
starts with LLM annotation. This step involves
compiling a subset and using an LLM to anno-
tate it by interacting with the programmatic LLM
controller. The LLM controller takes care of 1)
agent registration and management (e.g., model
selection and validation) and 2) running annota-
tion jobs (e.g., input data pre-processing, initiating
LLM calls, post-processing and storing responses),
satisfying R 1a,1c. Once LLM annotation is com-
pleted, users can verify LLM labels. Users can
select a subset of LLM labels to verify by search
queries (R 2a), and inspect and correct them in a
verification widget in the same notebook (R 2b).
Data Model
MEGAnno+ extends MEGAnno’s
data
model
where
data
Record,
Label,
Annotation, Metadata (e.g., text embedding
or confidence score) are persisted in the service
database along with the task Schema.2
Anno-
tations are organized around Subsets, which
are slices of the data created from user-defined
searches or recommendations. To effectively inte-
grate LLM into the workflow, we introduce new
concepts: Agent, Job, and Verification.
An Agent is defined by the configuration of the
LLM (e.g., model’s name, version, and hyper-
parameters) and a prompt template. When an agent
2MEGAnno+ only supports full LLM-integrated work-
flows for record-level tasks.
3
Figure 2: UI for customizing a prompt template and
previewing generated prompts. Prompt is generated
based on the name and options of label schema.
is employed to annotate a selected data subset, the
execution is referred to as a Job (see Section 4.3.1).
Verification captures annotations from hu-
man users that confirm or update LLM labels (see
Section 4.4).
4.2
Agents: Model and Prompt Management
Since variation in either model configuration or
prompt may result in a variable output from an
LLM, we define an annotation Agent to be a
combination of a user-selected configuration and
prompt template. Used agents are stored in our
database3 and can be queried based on model con-
figuration. This allows users to reuse agents and
even compare the performance of different LLMs
on a particular dataset (R 1d).
Model configuration
MEGAnno+ enables users
to choose an LLM from a list of available models,
configure model parameters, and also provide a
validation mechanism to ensure the selected model
and parameters conform to the LLM API definition
and limitations. While MEGAnno+ is designed to
support any open-source LLM or commercial LLM
APIs, in this work, we only demonstrate OpenAI
Completion models for clarity and brevity.
Prompt template
To utilize LLMs as annotators,
an input record has to be transformed into a prompt
text. With MEGAnno+, prompts can be automat-
ically generated based on a labeling schema and
a prompt template for users’ convenience. We of-
fer a default template that contains annotation in-
struction, output formatting instruction, and input
slot, which can be edited programmatically. We
also provide a UI widget to interactively customize
the prompt template and preview the generated
3Note that for an agent, we store its prompt template (a
rule to build prompt text), not prompts (generated prompts for
a set of data records) to save storage.
prompts for selected data samples (Fig. 2, R 1b).
4.3
LLM Annotation
Unlike human annotation, LLM annotation goes
through a multi-step process to collect labels from
input data records. We execute this process as an
annotation Job using the LLM controller.
4.3.1
Initiating LLM Jobs
To start a job, users need to select a data subset to
annotate and an agent, i.e., an LLM model and a
prompt template. Users can utilize MEGAnno’s
sophisticated subset selection techniques, includ-
ing filtering by keywords or regular expressions, or
receiving suggestions of similar data records. One
can create a new agent or reuse one of previously
registered agents. By reusing subsets and agents for
new jobs, users can easily compare annotation per-
formance between different models or for different
data slices.
4.3.2
Pre-processing
The first step within a job is pre-processing. Using
the prompt template of a selected agent, a data sub-
set is converted into a list of prompts. All prompts
are validated (e.g., within max token limit) before
calling LLM APIs.
4.3.3
LLM API Calls: Error Handling
MEGAnno+ handles the calls to the external LLM
APIs to facilitate a smooth, robust, and fault-
tolerant experience for users, without having to
worry about making any explicit API calls or han-
dling error cases themselves. In order to ensure
a fault-tolerant procedure, errors encountered dur-
ing API calls are handled in two ways: handle
within our system or delegate to users. We han-
dle known LLM API errors that can be solved by
user-side intervention. This would be in cases such
as a Timeout or RateLimitError in OpenAI
models, or other similar errors which require the
user themselves to call to the LLM API again. On
encountering such errors, MEGAnno+ retries the
call to the LLM API itself. Delegated errors are the
ones that require interventions by external service
providers and are beyond our scope. For instance,
errors such as APIConnectionError in Ope-
nAI models occur because of an issue with the
LLM API server itself and requires intervention
from OpenAI. In this case, MEGAnno+ simply
notifies the user and relays the error message.
4
Figure 3: Example LLM responses and extraction re-
sults. Minor violations are processed as valid labels.
4.3.4
Post-processing LLM Responses and
Storing Labels and Metadata
Label extraction
LLM outputs are typically
unstructured (i.e., free-text) and can be noisy
and unusable for downstream applications, even
when prompted to adhere to a specific format.
This necessitates careful post-processing of LLM-
generated content, converting them into valid labels
(Fig. 3). MEGAnno+ conducts an automated post-
processing step on LLM responses, handling errors
in cases of syntax or formatting violations (i.e., not
adhering to the format specified in prompt instruc-
tions). Additionally, our tool checks for semantic
violations, ensuring that the generated label is valid
within the existing schema for the task.
Metadata extraction
MEGAnno+ can collect
model artifacts and store them as label metadata
(R 1e). Examples include model logits, costs asso-
ciated with inference, used random seed, and so on.
They can be useful for further analyses on LLM
annotation and human verification. By default, our
system only stores token logits to estimate the used
LLM’s confidence for generated labels. Calculated
confidence scores serve as additional signals for
decision-making in the human verification step.
Storing
in
database
Following
the
post-
processing step,
extracted valid labels and
metadata are sent to the backend service for persis-
tence in the database. Invalid labels are not stored
in the database to prevent label contamination, but
frequent invalid ones are still shown to the user
to guide the next iteration (e.g., update labeling
schema, improve instruction in prompts).
4.3.5
Monitoring Annotation Jobs
When running a job, we display the progress and
statistics of each step of the job for monitoring
(Fig. 4). These include 1) agent details such as
the selected model and prompt template, 2) input
summary such as sample prompts generated using
Figure 4: Annotation progress and summary.
the template along with how many prompts are
valid or invalid, 3) API call progress such as the
time taken to retrieve responses from the API calls,
and 4) output summary such as the numbers of
valid and invalid responses from API and label
distribution of valid responses.
4.4
Verification
LLM labels can be unreliable, requiring human
verification to ensure the quality of the collected
labeled data.
In-notebook verification widget
MEGAnno+
provides a verification widget to complete the
LLM annotation workflow in the same notebook.
Leveraging MEGAnno+’s robust and customizable
search functionality, users can retrieve a subset of
LLM labels based on keywords, regular expres-
sions, assigned labels, or metadata. Then utilizing
the verification widget (as illustrated in Fig. 5),
users can explore the selected subset and decide
whether to confirm or correct their LLM-generated
labels. The verification UI includes both a table
view for exploratory and batch verification, as well
as a single view.
Verification priority
Human verification, while
less expensive than direct annotation, can still be
time- and cost-consuming. Therefore, it is crucial
5
Figure 5: The table view in verification UI. Users can
explore LLM annotations via filtering by labels, sorting
by confidence scores, or keyword search on text input.
to prioritize and direct human efforts toward more
“suspicious” outputs from LLMs. Our widget fa-
cilitates this process by presenting metadata, such
as model confidence or token logit scores, in a
separate column. Users can freely sort and filter
rows based on labels or metadata, enabling them to
prioritize or focus on labels with low confidence.
Query and export verified labels
MEGAnno+
offers flexible query interfaces, allowing users to
search for verification by LLM agents (i.e., model
and prompt config), as well as jobs. Both the origi-
nal LLM-generated labels and any potential human
corrections are stored in the database, enabling
users to filter and retrieve labels “confirmed” or
“corrected” by human verifiers. These features es-
tablish a foundation for easy in-notebook model
and prompt comparison. Ultimately, the query re-
sults serve as a view of the labeling project, ready
to be exported to downstream applications.
5
Use Case: Natural Language Inference
Moana, the aforementioned data scientist who
needs to collect training data quickly, decides to
use MEGAnno+ to leverage LLM-powered anno-
tation. First, she imports her unlabeled data and
sets the labeling schema as entailment or not entail-
ment. She selects a GPT-3 davinci model with the
default parameters and prompt template. To test
this setting, she runs the model on 10 samples.
1 c = Controller(<service>, <auth>)
2 model_config = {'model': 'davinci'}
3 template = PromptTemplate(label_schema)
4 agent = c.create_agent(model_config,
template)
5 subset = <service>.search(limit=10)
6 job = c.run_job(agent, subset)
After the job is finished, the annotation summary
(Fig. 4) shows that all samples are successfully an-
notated by GPT-3 and 40% are entailment. Also,
one response is annotated with ‘notentailed’, ex-
emplifying the instability of LLMs even with clear
instructions. With MEGAnno+’s table view wid-
get, she examines data and labels (Fig. 5). She
realizes that some of the records labeled as ‘not
entailment’ are contradictory whereas the rest are
neutral. She updates the labeling schema to con-
tain entailment, neutral, and contradiction. Next,
she wonders if changing the model’s temperature
would improve the accuracy of annotation. She cre-
ates another agent, GPT-3 with temperature with
zero and re-runs annotation on the same subset.
1 model_config2 = {'model': 'davinci', '
temperature': 0}
2 agent2 = c.create_agent(model_config2,
template)
3 job2 = c.run_job(agent2, subset)
She exports the annotations from both jobs and
compares them. She concludes that the second
model is good enough for her project. She im-
ports her entire data and uses the agent to label
them. Since the size of the data is huge, she has
to wait till the annotations are done. Fortunately,
with MEGAnno+, she can track the progress in the
output cell while the job is running. To review the
annotations, she sorts the annotations in an ascend-
ing order of confidence and manually verifies low
confidence (< 95%) annotations.
6
Discussion
How to design an annotation task?
Based on
our experience, we find that designing an anno-
tation task and a prompt similar to more widely
used and standardized NLP tasks is beneficial. For
example, framing Moana’s problem as a natural lan-
guage inference task is more effective than framing
it as a binary classification of agreement and dis-
agreement. Also, the selection of label options may
work better if it is similar to common options for
given tasks, such as [positive, neutral, negative] >
[super positive, positive, ..., negative] for sentiment
classification. Lastly, it is recommended that the
format of a prompt be similar to the one used in
training as some LLMs have different prompt for-
mat than the others. We plan to conduct more sys-
tematic test to discover reasonable default prompts
for different models.
Are LLMs consistent and reliable annotators?
We expect human annotators to maintain a consis-
tent mental model. In other words, when humans
6
are presented with the same question rephrased,
we anticipate consistent answers. However, LLMs
are known to be sensitive to semantic-preserving
perturbations in prompts. For instance, changes in
prompt design, the selection and order of demon-
strations, and the order of answer options can result
in different outputs (Zhao et al., 2021; Pezeshkpour
and Hruschka, 2023).
Moreover, commercial
LLMs can undergo real-time fine-tuning, meaning
that prompting with the same setup today may yield
different results than prompting yesterday (Chen
et al., 2023). Therefore, LLM annotators and hu-
man annotators should not be treated the same, and
annotation tools should carefully design their data
models and workflows to accommodate both types
of annotators.
Limitations
Our system has several limitations.
Our post-processing mechanism may not be ro-
bust to cover all tasks and prompts entered by the
user. Furthermore, MEGAnno+’s ability to cap-
ture metadata is contingent on the LLM model
used. For example, GPT-4 models do not yet pro-
vide any form of token logprobs or other metadata
which can be captured.
7
Conclusion
MEGAnno+ is a text annotation system for human-
LLM collaborative data labeling. With our LLM an-
notation →Human verification workflow, reliable
and high-quality labels can be collected efficiently.
Our tool supports robust LLM annotation, selective
human verification, and effective management of
LLMs, labels, and metadata.
As future work, we are currently working
on adding more LLM agents (e.g., open-source
LLMs), supporting customized extraction of meta-
data (e.g., custom uncertainty metric), and improv-
ing prompt template UI for data-aware in-context
learning. Additionally, we plan to incorporate di-
verse annotation workflows such as Multi-agent
LLM annotation →LLM label aggregation →Hu-
man verification; and LLM augmentation →Hu-
man verification.
Ethics Statement
First, labels generated by LLMs can exhibit bias
or inaccuracy. These models are pre-trained on
vast amount of data, which are typically not acces-
sible to the public. Biases present in the training
data can be transferred to LLM labels. Also, if the
training data lacks relevant or up-to-date knowl-
edge, the model may produce incorrect annotation.
Since we cannot access models’ inner workings
or their training data, it is difficult to identify and
understand how and why LLMs make biased or
inaccurate labeling decisions. Second, the use of
commercial LLMs for labeling data containing sen-
sitive information or intellectual property may pose
risks. Data shared with commercial LLMs, such as
ChatGPT, may be collected and utilized for retrain-
ing these models. To prevent potential data leakage
and mitigate associated legal consequences, it is ad-
visable to either mask any confidential information
or only use in-house LLMs.
Acknowledgements
We would like to thank Pouya Pezeshkpour and
Estevam Hruschka for their helpful comments.
References
Autolabel. Github.com/refuel-ai/autolabel.
Humanloop.com. Humanloop.com.
Abubakar Abid, Maheen Farooqi, and James Zou. 2021.
Persistent anti-muslim bias in large language models.
In Proceedings of the 2021 AAAI/ACM Conference
on AI, Ethics, and Society, AIES ’21, page 298–306,
New York, NY, USA. Association for Computing
Machinery.
Lingjiao Chen, Matei Zaharia, and James Zou. 2023.
How is chatgpt’s behavior changing over time?
Jiale Cheng, Xiao Liu, Kehan Zheng, Pei Ke, Hongning
Wang, Yuxiao Dong, Jie Tang, and Minlie Huang.
2023.
Black-box prompt optimization: Aligning
large language models without model training.
Bosheng Ding, Chengwei Qin, Linlin Liu, Yew Ken
Chia, Boyang Li, Shafiq Joty, and Lidong Bing. 2023.
Is GPT-3 a good data annotator?
In Proceedings
of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers),
pages 11173–11195, Toronto, Canada. Association
for Computational Linguistics.
Benjamin D Douglas, Patrick J Ewell, and Markus
Brauer. 2023.
Data quality in online human-
subjects research: Comparisons between mturk, pro-
lific, cloudresearch, qualtrics, and sona. Plos one,
18(3):e0279720.
Hector Garcia-Molina, Manas Joglekar, Adam Mar-
cus, Aditya Parameswaran, and Vasilis Verroios.
2016.
Challenges in data crowdsourcing.
IEEE
Transactions on Knowledge and Data Engineering,
28(4):901–911.
7
Fabrizio Gilardi, Meysam Alizadeh, and Maël Kubli.
2023.
Chatgpt outperforms crowd workers for
text-annotation tasks. Proceedings of the National
Academy of Sciences, 120(30).
Xingwei He, Zhenghao Lin, Yeyun Gong, A-Long Jin,
Hang Zhang, Chen Lin, Jian Jiao, Siu Ming Yiu, Nan
Duan, and Weizhu Chen. 2023. AnnoLLM: Making
large language models to be better crowdsourced
annotators.
Jan-Christoph Klie, Michael Bugert, Beto Boullosa,
Richard Eckart de Castilho, and Iryna Gurevych.
2018. The INCEpTION platform: Machine-assisted
and knowledge-oriented interactive annotation. In
Proceedings of the 27th International Conference on
Computational Linguistics: System Demonstrations,
pages 5–9, Santa Fe, New Mexico. Association for
Computational Linguistics.
Thomas Kluyver, Benjamin Ragan-Kelley, Fernando
Pérez, Brian Granger, Matthias Bussonnier, Jonathan
Frederic, Kyle Kelley, Jessica Hamrick, Jason Grout,
Sylvain Corlay, Paul Ivanov, Damián Avila, Safia
Abdalla, and Carol Willing. 2016. Jupyter notebooks
– a publishing format for reproducible computational
workflows. In Positioning and Power in Academic
Publishing: Players, Agents and Agendas, pages 87 –
90. IOS Press.
Taja Kuzman, Igor Mozetiˇ
c, and Nikola Ljubeši´
c. 2023.
Chatgpt: Beginning of an end of manual linguistic
data annotation? use case of automatic genre identifi-
cation.
Stephanie Lin, Jacob Hilton, and Owain Evans. 2022.
Teaching models to express their uncertainty in
words. Transactions on Machine Learning Research.
Leib Litman, Aaron Moss, Cheskie Rosenzweig, and
Jonathan Robinson. 2021. Reply to mturk, prolific
or panels? choosing the right audience for online
research. SSRN Electronic Journal.
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler
Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,
Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
Shashank Gupta, Bodhisattwa Prasad Majumder,
Katherine Hermann, Sean Welleck, Amir Yazdan-
bakhsh, and Peter Clark. 2023. Self-refine: Iterative
refinement with self-feedback.
Ines Montani and Matthew Honnibal. 2018. Prodigy: A
new annotation tool for radically efficient machine
teaching. Artificial Intelligence to appear.
Pouya Pezeshkpour and Estevam Hruschka. 2023.
Large language models sensitivity to the order of
options in multiple-choice questions.
Kim Bartel Sheehan. 2018. Crowdsourcing research:
data collection with amazon’s mechanical turk. Com-
munication Monographs, 85(1):140–156.
Emily Sheng, Kai-Wei Chang, Prem Natarajan, and
Nanyun Peng. 2021. Societal biases in language
generation: Progress and challenges. In Proceedings
of the 59th Annual Meeting of the Association for
Computational Linguistics and the 11th International
Joint Conference on Natural Language Processing
(Volume 1: Long Papers), pages 4275–4293, Online.
Association for Computational Linguistics.
Eyal Shnarch, Alon Halfon, Ariel Gera, Marina
Danilevsky, Yannis Katsis, Leshem Choshen, Martin
Santillan Cooper, Dina Epelboim, Zheng Zhang, and
Dakuo Wang. 2022. Label sleuth: From unlabeled
text to a classifier in a few hours. In Proceedings of
the 2022 Conference on Empirical Methods in Nat-
ural Language Processing: System Demonstrations,
pages 159–168, Abu Dhabi, UAE. Association for
Computational Linguistics.
Maxim
Tkachenko,
Mikhail
Malyuk,
Andrey
Holmanyuk,
and
Nikolai
Liubimov.
2020-
2022.
Label
Studio:
Data
labeling
soft-
ware.
Open source software available from
https://github.com/heartexlabs/label-studio.
Petter Törnberg. 2023. Chatgpt-4 outperforms experts
and crowd workers in annotating political twitter mes-
sages with zero-shot learning.
Shuohang Wang, Yang Liu, Yichong Xu, Chenguang
Zhu, and Michael Zeng. 2021. Want to reduce la-
beling cost? GPT-3 can help. In Findings of the
Association for Computational Linguistics: EMNLP
2021, pages 4195–4205, Punta Cana, Dominican Re-
public. Association for Computational Linguistics.
Xinru Wang, Hannah Kim, Sajjadur Rahman, Kushan
Mitra, and Zhengjie Miao. 2024. Human-LLM col-
laborative annotation through effective verification of
LLM labels. In Proceedings of the 2024 CHI Confer-
ence on Human Factors in Computing Systems, CHI
’24, New York, NY, USA. Association for Computing
Machinery.
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le,
Ed H. Chi, Sharan Narang, Aakanksha Chowdhery,
and Denny Zhou. 2023. Self-consistency improves
chain of thought reasoning in language models. In
The Eleventh International Conference on Learning
Representations.
Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie
Fu, Junxian He, and Bryan Hooi. 2023. Can llms
express their uncertainty? an empirical evaluation of
confidence elicitation in llms.
Dan Zhang, Hannah Kim, Rafael Li Chen, Eser Kan-
dogan, and Estevam Hruschka. 2022. MEGAnno:
Exploratory labeling for NLP in computational note-
books. In Proceedings of the Fourth Workshop on
Data Science with Human-in-the-Loop (Language
Advances), pages 1–7, Abu Dhabi, United Arab Emi-
rates (Hybrid). Association for Computational Lin-
guistics.
8
Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and
Sameer Singh. 2021. Calibrate before use: Improv-
ing few-shot performance of language models. In
Proceedings of the 38th International Conference
on Machine Learning, volume 139 of Proceedings
of Machine Learning Research, pages 12697–12706.
PMLR.
Yiming Zhu, Peixian Zhang, Ehsan-Ul Haq, Pan Hui,
and Gareth Tyson. 2023.
Can chatgpt reproduce
human-generated labels? a study of social computing
tasks. arXiv preprint arXiv:2304.10145.
Caleb Ziems, William Held, Omar Shaikh, Jiaao Chen,
Zhehao Zhang, and Diyi Yang. 2023. Can Large
Language Models Transform Computational Social
Science? Computational Linguistics, pages 1–53.
9
"
6,7,Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking,"Anjali Khurana, Hari Subramonyam, Parmit K Chilana","Large Language Model (LLM) assistants, such as ChatGPT, have emerged as
potential alternatives to search methods for helping users navigate complex,
feature-rich software. LLMs use vast training data from domain-specific texts,
software manuals, and code repositories to mimic human-like interactions,
offering tailored assistance, including step-by-step instructions. In this
work, we investigated LLM-generated software guidance through a within-subject
experiment with 16 participants and follow-up interviews. We compared a
baseline LLM assistant with an LLM optimized for particular software contexts,
SoftAIBot, which also offered guidelines for constructing appropriate prompts.
We assessed task completion, perceived accuracy, relevance, and trust.
Surprisingly, although SoftAIBot outperformed the baseline LLM, our results
revealed no significant difference in LLM usage and user perceptions with or
without prompt guidelines and the integration of domain context. Most users
struggled to understand how the prompt's text related to the LLM's responses
and often followed the LLM's suggestions verbatim, even if they were incorrect.
This resulted in difficulties when using the LLM's advice for software tasks,
leading to low task completion rates. Our detailed analysis also revealed that
users remained unaware of inaccuracies in the LLM's responses, indicating a gap
between their lack of software expertise and their ability to evaluate the
LLM's assistance. With the growing push for designing domain-specific LLM
assistants, we emphasize the importance of incorporating explainable,
context-aware cues into LLMs to help users understand prompt-based
interactions, identify biases, and maximize the utility of LLM assistants.","Why and When LLM-Based Assistants Can Go Wrong:
Investigating the Effectiveness of Prompt-Based Interactions for
Software Help-Seeking
Anjali Khurana
anjali_khurana@sfu.ca
Simon Fraser University
BC, Canada
Hari Subramonyam
harihars@stanford.edu
Stanford University
USA
Parmit K Chilana
pchilana@cs.sfu.ca
Simon Fraser University
BC, Canada
ABSTRACT
Large Language Model (LLM) assistants, such as ChatGPT, have
emerged as potential alternatives to search methods for helping
users navigate complex, feature-rich software. LLMs use vast train-
ing data from domain-specific texts, software manuals, and code
repositories to mimic human-like interactions, offering tailored as-
sistance, including step-by-step instructions. In this work, we inves-
tigated LLM-generated software guidance through a within-subject
experiment with 16 participants and follow-up interviews. We com-
pared a baseline LLM assistant with an LLM optimized for particu-
lar software contexts, SoftAIBot, which also offered guidelines for
constructing appropriate prompts. We assessed task completion,
perceived accuracy, relevance, and trust. Surprisingly, although
SoftAIBot outperformed the baseline LLM, our results revealed no
significant difference in LLM usage and user perceptions with or
without prompt guidelines and the integration of domain context.
Most users struggled to understand how the prompt’s text related
to the LLM’s responses and often followed the LLM’s suggestions
verbatim, even if they were incorrect. This resulted in difficulties
when using the LLM’s advice for software tasks, leading to low task
completion rates. Our detailed analysis also revealed that users re-
mained unaware of inaccuracies in the LLM’s responses, indicating
a gap between their lack of software expertise and their ability to
evaluate the LLM’s assistance. With the growing push for designing
domain-specific LLM assistants, we emphasize the importance of
incorporating explainable, context-aware cues into LLMs to help
users understand prompt-based interactions, identify biases, and
maximize the utility of LLM assistants.
CCS CONCEPTS
• Human-centered computing →Empirical studies in HCI.
KEYWORDS
feature-rich software; large language models; prompt-based inter-
actions; help-seeking
ACM Reference Format:
Anjali Khurana, Hari Subramonyam, and Parmit K Chilana. 2024. Why
and When LLM-Based Assistants Can Go Wrong: Investigating the Ef-
fectiveness of Prompt-Based Interactions for Software Help-Seeking. In
IUI ’24, March 18–21, 2024, Greenville, SC, USA
© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
This is the author’s version of the work. It is posted here for your personal use. Not
for redistribution. The definitive Version of Record was published in 29th International
Conference on Intelligent User Interfaces (IUI ’24), March 18–21, 2024, Greenville, SC,
USA, https://doi.org/10.1145/3640543.3645200.
29th International Conference on Intelligent User Interfaces (IUI ’24), March
18–21, 2024, Greenville, SC, USA. ACM, New York, NY, USA, 14 pages.
https://doi.org/10.1145/3640543.3645200
1
INTRODUCTION
Learning to use feature-rich software applications for tasks such
as advanced word processing, data analysis, image manipulation,
and video editing can be challenging for end-users. Users currently
turn to various software help resources to learn and seek help for
such software tasks. For example, they usually begin by querying
online search engines using keywords to locate specific resources,
such as video and text-based tutorials, forums posts, and blogs and
articles [3, 22, 35, 36]. However, online software-help seeking is
a complex endeavour, demanding precise queries to pinpoint the
most pertinent information that can be directly applied within the
application [18, 22].
The recent emergence of Generative AI and pre-trained Large
Language Model (LLM)-based assistants like ChatGPT [2, 8] offers
a novel approach to support end-users’ software help-seeking by
leveraging these assistant’s advanced natural language understand-
ing [52]. For example, LLMs provide the potential for step-by-step
instructions and explanations tailored to specific task needs, saving
users time and effort searching through online resources. In the
past two years, LLMs have demonstrated promising capabilities
in assisting users in various domains, including tasks related to
programming and software development [50], language generation
[2], and question answering [2]. However, the effectiveness of LLM-
based assistance and how end-users employ LLMs to seek help for
feature-rich applications remain important open questions. Fur-
thermore, there have been calls [29] for a better understanding of
the dynamics of user interaction across diverse AI usage scenarios,
aiming to avoid false assumptions.
In this work, we investigate how software users make use of
LLMs with prompt-based interactions for seeking help for feature-
rich applications. In particular, we investigate the effectiveness
of recently emerging prompt guidelines [42, 44] offered by Ope-
nAI, Microsoft, and others, which, when prepended to the user
prompts, can potentially enhance LLM output to provide desired
assistance [52]. Furthermore, we also consider the impact of inte-
grating additional domain context, such as software documentation,
into LLMs to improve the precision of the LLM output. For exam-
ple, such techniques are currently being explored in in-application
LLM assistants, such as Copilot in Microsoft 365 applications [43],
Firefly in Adobe applications [1], etc.). For this investigation, we
developed SoftAIBot, our implementation of a state-of-the-art LLM
assistant that integrates prompt guidelines and domain context,
arXiv:2402.08030v1  [cs.HC]  12 Feb 2024
IUI ’24, March 18–21, 2024, Greenville, SC, USA
Khurana et al.
and compared it with a Baseline ChatGPT Plus LLM assistant. By
analyzing users’ interactions, behaviors, and challenges when using
these state-of-the-art LLMs for software help-seeking, we aim to
help end users harness the full potential of LLM-based assistants
for feature-rich software [31]. The research questions guiding this
exploration were:
• RQ1: How do end users make use of prompt-based interac-
tions when finding software help ?
• RQ2: To what extent can SoftAIBot generate accurate and
relevant software help for end-users of feature-rich applica-
tions?
• RQ3: How do end users’ mental models of LLMs influence
their use of LLM-generated software help?
In this paper, we report on the results from a controlled study
and follow-up interviews that illustrate how end-users make use
of prompt-based interactions when using LLM-based assistance in
concert with tasks that involve visual interactions (e.g., Microsoft
PowerPoint) and advanced data analysis/visualizations (e.g., Mi-
crosoft Excel). We ran a within-subject experiment with 16 par-
ticipants from varied backgrounds without expertise in Machine
Learning (ML) or Natural Processing Language (NLP). We hypoth-
esized that our implementation of SoftAIBot with ChatGPT Plus
(GPT-4) as the underlying LLM, state-of-the-art prompt guidelines,
and integrated domain context will generate accurate and relevant
assistance for users’ prompts and help users finish their software
tasks. However, our findings showed that even though SoftAIBot
performed better than BaseLine in generating more accurate and
relevant LLM output, users could not recognize these differences
and struggled in mapping the LLM instructions to the software
application, leading to poor task completion. The impact of the
prompt text on the quality of the LLM output was not clear to users,
and they rather followed LLMs’ suggestions blindly, even when
the output was inaccurate. Lacking an accurate mental model of
LLMs, users tended to over-trust the LLM assistance without much
contemplation.
The main contributions of this research are in providing em-
pirical insights that: (1) demonstrate how software users employ
new-generation LLM assistants to seek software help, both with
and without prompt guidelines as well as integrated software con-
text; (2) illustrate the challenges that software users experience
with prompt-based interaction (e.g., crafting prompts, comprehend-
ing how prompts bias LLM output, mapping LLM-suggested steps
to software, overtrusting output correctness); (3) identify gaps in
users’ mental models as they try to seek and apply assistance from
LLMs to software tasks, which affect both their use of LLMs and per-
ception of software features, regardless of implicit enhancements in
the underlying model or explicit prompt guidelines (SoftAIBot). We
discuss the implications of these findings for designing and imple-
menting LLM help tailored to feature-rich applications, the need to
incorporate transparent and responsible LLM assistants, and ways
to bridge the disparity between mental models and LLM interfaces,
ultimately helping end-users form accurate mental models of LLMs.
2
RELATED WORK
This research drew upon insights from prior work on how users
learn and seek help for feature-rich software, the emergence of
LLMs for task-based assistance, and the use of prompt-based inter-
actions.
2.1
Software Help-seeking evolution
HCI research has a rich history of investigating the challenges
that users experience when learning and seeking help for complex
feature-rich applications [11, 18, 22, 35]. Help-seeking resources
and approaches have evolved over the years: from formal documen-
tation and manuals [36, 39] to the use of videos [23, 25], interactive
tutorials [35], Google Search, Q&A or FAQ sites, blogs, dedicated
forums [22] and even contextual help systems embedded within
applications [7, 11, 17, 19, 24].
However, studies have shown that although users have increased
access to help information, they often get lost in search results and
forum posts and still face difficulty in recognizing effective and rel-
evant [22, 34] resources for completing their software tasks. Users
also face numerous issues with articulating search queries using pre-
cise keywords [16, 18, 22], often termed as vocabulary problem [16].
Another related issue that users face is using the located help in co-
ordination with the application and going back and forth between
the two to accomplish their software tasks [22]. Past studies have
shown that users tend to find step-by-step guidance within the con-
text of feature-rich applications useful and trustworthy [11, 21, 22].
There have been constant innovations in devising help resources in
the form of in-context help and video tutorials [17, 25, 35]. However,
recent developments in Generative AI have opened a new outlet
of help-seeking for users through LLM-based assistance, which is
much more forgiving in letting users describe their queries using
natural language [41, 52]. These assistants tend to provide more
specific and in-context assistance [52] to users, unlike traditional
resources that may be scattered and require precise queries for
retrieval. While there is a rich history of work supporting software
learnability and easing help-seeking processes [11, 14, 17, 25], it is
unclear whether users’ learning approaches and strategies apply
to LLM-based assistants. We extend this prior work by examining
how novices employ LLM assistants for software help-seeking and
the types of challenges they experience.
2.2
LLM use for Task-Based Assistance
The emergence of powerful LLM assistants, such as ChatGPT, has
significantly impacted task-based assistance for a range of domains,
including programming, text generation, and text summarization
[47]. Recent studies have attempted to understand the use of LLM
assistants for programming and software development-related help-
seeking [5, 51]. For example, Xu, Vasilescu, & Neubig (2022) [51]
investigated the use of LLMs for programming-related tasks and
found that users struggle to generate assistance especially for com-
plex queries as users struggled in formulating specific code-related
input queries. Another study [48] highlighted that users mostly rely
on trial and error while debugging their code using LLM assistance
and often do not feel confident about applying the output.
Recent interest in LLMs has inspired initiatives [1, 43] to utilize
their capabilities for assisting users with software tasks as well
[1, 43]. For example, some experimental work is being explored by
integrating LLMs directly into feature-rich applications, such as
Copilot in Microsoft 365 [43] and Firefly in Adobe [1]. This process
Why and When LLM-Based Assistants Can Go Wrong
IUI ’24, March 18–21, 2024, Greenville, SC, USA
has shown that developers can face new challenges in ensuring
accurate and effective use of this new avenue of conversational
UX experience [29, 52]. As many of these interfaces are still at a
nascent stage, it is unclear how this current practice (i.e., integrat-
ing the context of these feature-rich applications) can help novice
end-users in seeking accurate and relevant assistance from LLMs.
Furthermore, to harness the full potential of these LLM assistants
for seeking help for their software tasks, we need more insights on
where and how users struggle with these LLMs [31]. Our study con-
tributes new knowledge on how non-AI expert end-users employ
LLMs’ generated software guidance assistance in accomplishing
tasks for feature-rich applications by comparing our own imple-
mented, SoftAIBot, an LLM optimized for particular domain context
(e.g., software documentation) with the Baseline ChatGPT.
2.3
Prompt-based interactions
To leverage the potential of LLMs, a lot of the focus in HCI and AI
research is turning to prompt-based interactions as users generally
have to provide input or queries in the form of prompts that are
then processed or responded to by a conversational AI system
[52]. Recent studies on the usability of prompt-based interactions
[5, 41, 51] reveal that prompts have a significant impact on pre-
trained language models’ ability to produce desired outputs, even
though the prompts themselves are simple textual instructions for
the task at hand [52]. For example, Advait et. al’s (2022) [41] study
on the use of LLM-assisted tools for programming tasks revealed
that the crucial concern is crafting effective prompts that elevate the
probability of an LLM model to generate efficient code. Thus, the
big challenge for end-users, especially novices and non-AI experts,
is to define the appropriate prompts and learn prompting strategies
to get the desired assistance from these LLMs.
Unlike traditional help-seeking mediums that rely on keyword
matching, prompt-based interactions within LLMs offer human-
like language capabilities [29], which is unique, but can also be
unreliable. This unreliability comes from the biases (e.g., hallucinat-
ing and non-deterministic output) inherent within prompt-based
interactions of LLMs. Considering LLMs are a tremendous leap
from traditional help-seeking mediums that most users are famil-
iar with, there have been calls to investigate users’ mental models
as they interact with LLMs [29]. This becomes necessary when
seeking assistance for feature-rich software tasks, where there is
an interplay between the mental model of LLM vs the software
application [22, 29]. Recent studies have focused on understanding
users’ prompting strategies and proposing a catalogue of prompting
guidelines [42, 44, 50] for allowing users to craft better prompts
and seek desired LLM assistance. However, the use of these prompt
guidelines in practice and their effectiveness remains unclear. Our
study complements the existing research by observing users with
prompt-based interactions and assessing the efficacy of prompt-
based guidelines and integration of domain context in enhancing
LLM assistance for software tasks.
3
METHOD: CONTROLLED EXPERIMENT
AND FOLLOW-UP INTERVIEWS
We conducted a two-part user study with 16 users that consisted
of a controlled experiment and follow-up interviews. Our main
goal in this study was to investigate the effectiveness of two recent
advancements: 1) prepending prompt guidelines to user prompts
to enhance the accuracy of LLM output, as advocated by OpenAI,
Microsoft and others to enhance Generative AI tools [42, 44]; and, 2)
directly integrating domain context (e.g., software documentation)
into LLM assistants (e.g., as demonstrated in Copilot in Microsoft
365 applications [43] and Firefly in Adobe applications [1], etc.) to
enhance the relevance and accuracy of LLM output for software-
related tasks. For our investigation, we implemented both of these
advancements in a new GPT-4-based assistant, which we call Soft-
AIBot, that offers in-context prompt guidelines (See Figure 1) and
enhances the LLM output by making it specific to the feature-rich
application (See Figure 1.c). For comparison, we also implemented
Baseline ChatGPT based on the pre-trained state-of-the-art LLM
assistants, ChatGPT. (SoftAIBot is explained in more detail below.)
The prompt suggestions were not included in the Baseline ChatGPT
because it was a control condition for the experiment.
Based on our research questions, we derived the following hy-
potheses for users seeking software help:
• H1: Users will perceive SoftAIBot as being more accurate
than Baseline ChatGPT.
• H2: Users will perceive SoftAIBot as being more relevant
than Baseline ChatGPT.
• H3: Users will trust SoftAIBot more than Baseline ChatGPT.
• H4: Users will find the output provided by SoftAIBot to be
easier to apply in the software application than Baseline
ChatGPT.
3.1
Participants
We recruited 16 participants (9F|7M) for our study, focusing on non-
AI expert users who had little to no prior experience or knowledge
of ML or NLP. Our participants came from different backgrounds
(CS, Engineering, Business, Arts) and professions (administrative
services, business analytics, information designers, client services,
students, and researchers). Participants were familiar with LLM-
based assistant, ChatGPT (10/16), and a range of traditional chatbots
(12/16) such as Siri, and Google Assistant (12/14). They had used
ChatGPT before for text generation, text summarization, and pro-
gramming tasks, but none of them used it for software tasks used in
the study before. About half of the participants (7/16) had frequently
used PowerPoint and Excel applications and the remaining were
occasional users. Our participants covered a range of age groups:
18-24 (25%), 25-34 (62%), 35-44 (13%) and had different levels of ed-
ucation (2 Diploma, 4 Bachelor’s, 5 Master’s, 5 PhD). We recruited
participants mainly from our university’s mailing lists and found
additional participants through snowball sampling.
3.2
Design and Implementation of SoftAIBot
and Baseline ChatGPT
In this section, we describe the design and system implementation
of our two interventions.
3.2.1
SoftAIBot Intervention (GPT-4 with Prompt Guidelines
and Software Documentation) . SoftAIBot LLM intervention
suggests in-context prompt guidelines for constructing prompts
IUI ’24, March 18–21, 2024, Greenville, SC, USA
Khurana et al.
Figure 1: SoftAIBot integrates domain context via documentation and offers prompt guidelines to construct better prompts: (a)
allows users to type in the prompt text and submit it; (b) generates prompt suggestions in-response to a user’s text (in this
case, also shows a sample transformed query that users can directly use); (c) formats the response as step-by-step instructions
optimized for particular software contexts, in this case PowerPoint. To see the contrast in LLM response, please see Baseline in
Figure 2.
while interacting with GPT-4 as well as generates guidance for
particular software contexts.
Automatic Prompt Guidelines: The SoftAIBot UI interface
(See Figure 1) lays out the various user interface components and
receives the user’s prompt. Next, this user’s prompt gets transmitted
to our custom API developed in Python connected to GPT-4 for
generating prompt suggestions in-context to user’s prompt and
a sample transformed query based on suggested guidelines that
users can directly use. We used the prompt guidelines provided
by OpenAI, Microsoft and others [42, 44]. The generated prompt
suggestions are displayed as an overlay (as shown in Figure 1.b) on
the top left of the SoftAIBot interface via a Chrome Extension which
triggers with the click of “send” button component. These prompt
suggestions appear after every prompt-based interaction initiated
by the user. To allow users freedom and control in accessing the
prompt suggestions, we included the minimize, maximize, and close
options for either using the suggestion or simply closing it anytime
during the interaction.
Implicit Domain Context Integration:To optimize SoftAI-
Bot for a particular software context (as shown in Figure 1.c), we
leveraged GPT-4 augmented with corresponding software docu-
mentation. When a user submits the prompt, the relevant textual
information or pertinent excerpts are extracted from software doc-
umentation by using Facebook AI Similarity Search (FAISS) index
and vector search. Our custom API send this software context in-
formation along with user’s original prompt, together as a payload,
to OpenAI’s GPT-4 8k to generate better tailored responses with
reduced hallucinations. This process of using LLM with extracted
relevant text is known as Retrieval Augmented Generation (RAG)
Vector search [26]. We used open source model BGE-smal-en [32]
for producing chunks of the software documentation and trans-
forming it to dense contextual vectors with 384 dimensions. For
the search approach used in RAG, we tried different approaches
such as text chunking, and embedding models. The chunk size of
512 and BGE-smal-en approach provided us better context and fast
retrieval. Next, we searched this vector against the FAISS index to
retrieve the most similar text vectors. Based on the indices of these
similar vectors, we fetch the corresponding original text data from
software documentation.We fed this extracted text from software
documentation, along with the user’s query, as a prompt to GPT-4
(See Appendix ?? for details).
3.2.2
Baseline ChatGPT intervention (ChatGPT plus). Our
Baseline ChatGPT mimics the existing ChatGPT (See Figure 2.a)
plus based on GPT-4, where users can type in their query and LLM
provide assistance to users for variety of tasks through out-of-the
box multi-turn conversations using OpenAI GPT-4 8k API [2, 8].
To maintain the user’s history and conversations when interacting
with the OpenAI GPT-4 API, we designed Python algorithm using
chat completion objects [40] (See Appendix ?? for details).
To make the LLM UIs consistent for the experiment, we simu-
lated both the interventions using Gradio framework [46], an open
source framework, for developing intuitive web-based UI interfaces,
making it easier to gather feedback, showcase results, and enable
end-users to interact with LLMs during the user study.
Why and When LLM-Based Assistants Can Go Wrong
IUI ’24, March 18–21, 2024, Greenville, SC, USA
Figure 2: Overview of sample user task with PowerPoint application: (a) Users were asked to look up and use instructions from
LLM intervention. In this case, Baseline ChatGPT mimics the existing ChatGPT plus based on GPT-4, where users can type in
their prompt in the textbox and LLM provide assistance to users for variety of tasks; (b) Use LLM assistance to develop shown
project timeline in Microsoft PowerPoint that is visual and animated.
3.3
Choice of Application and Tasks
To choose tasks and applications, we explored different productivity-
related feature-rich applications (e.g., PowerPoint, Excel, Photoshop,
Teams, etc.) popular among everyday novice users. After our ini-
tial exploration, we selected Microsoft PowerPoint and Excel to
cover a range of tasks involving visual interactions, interactions
involving application of statistical functions or formula and other
visualization related tasks.
To assess the users’ help-seeking approaches and observe any
potential challenges when using LLM assistants for software help,
we selected tasks that would require multiple steps for completion,
and would necessitate multi-stage help and prompts (e.g., help
within different steps needed for task completion). For example,
one of the Excel tasks asked participants to use instructions from
the LLM for analyzing and visualizing the predictive analytics of the
sales values based on income using linear regression. Similarly, one
of the PowerPoint tasks (See Figure 2) asked users use instructions
from the LLM to develop a project timeline in Microsoft PowerPoint
that is visual and animated.
3.4
Study Design and Procedure
We used a within-subject design to minimize the effect of inter-
participant variability. To eliminate order effects, we used a Latin
Square counterbalancing [38] with 2 LLM conditions (total possible
order= 4) to balance the order in which tasks were presented. During
the experiment, each participant completed two tasks with each
LLM interventions (4 tasks in total) . The participants performed all
tasks using one feature-rich application, subsequently transitioning
to a second feature-rich application, but the order of the tasks and
associated LLM were randomized.
Each study session began by introducing the participant to the
LLM assistants, and provided some general tips to interact with
the application (e.g., using the prompts). We conducted the study
remotely through Zoom and participants were each given a $15
Amazon gift card in appreciation of their time. Participants were
provided instructions to install our LLM interventions via a Chrome
extension. Next, participants completed a demographic question-
naire on their background and prior experiences with LLM assis-
tants, chatbots and software applications.
We presented each LLM intervention along with software ap-
plication to the participant in a random order, one by one. We
designed the complicated software tasks so that participants would
be able to spend at least 8 minutes for completing each software
task regardless of their familiarity and experience with the soft-
ware application using given LLM intervention. After completing
each of the 4 tasks, participants were asked to fill the post-task
questionnaire hosted on the SurveyMonkey to assess the overall
experience seeking assistance from LLM interventions for software
tasks along with their perceptions of accuracy, relevancy, ease of
use, and trust of the assistance provided by LLM intervention. We
encouraged participants to think aloud [33] throughout the session
and reminded participants that the study was seeking to understand
how they seek assistance from LLM interventions for their software
IUI ’24, March 18–21, 2024, Greenville, SC, USA
Khurana et al.
tasks rather than their performance or ability to master software
applications or use the LLM.
Lastly, we conducted follow-up interviews to further probe any
difficulties that impacted the use of prompt-based interaction and
LLM assistance for software tasks, and any potential gaps in mental
models about how LLMs work. Each session lasted approximately
one hour and sessions were video and audio-recorded for transcrip-
tion, and the participants were asked to share their screen through
Zoom (only during the usability test).
3.5
Data Collection and Analysis
Throughout the study session, we recorded the participant’s screen
and audio recorded their interview responses. We captured screen
recordings to evaluate two key aspects: how users sought help from
LLM assistants (e.g., formulated their prompts) and how they used
the LLM output to complete the prescribed software tasks in Excel
and Powerpoint.
We used a combination of statistical tests and inductive analysis
approach to make sense of the data captured from the user study.
We ran Pearson’s Chi-square test for independence with nominal
variable “LLM Interventions” (having two levels: SoftAIBot, and
Baseline ChatGPT) and ordinal variable (having three collapsed
levels: Agree, Neutral and Disagree) to quantitatively determine
the significance of the results.
We used an expert-rating approach where the experimenter (in
consultation with all authors) analyzed the tasks performed by users
in the software application and compared all of our metrics against
the ground truth for all tasks. The ground truth in our context refers
to the correct or optimal sequence of steps for task completion,
the ideal application of software features, and the most relevant
responses from the LLM to user queries. Our metrics included:
(1) Task Completion: To measure the task completion, we evalu-
ated how many of the software task steps (e.g., sequence of
features/ functions) users completed using the LLM help.
(2) Task Accuracy: To measure the participants’ success in apply-
ing LLM assistance to complete the study tasks accurately,
we evaluated how accurately users identified the approach
or functionality (e.g., macro, animation, motion paths, par-
ticular statistical function) from the LLM output and then
applied the help using the corresponding software menu
options and features in the software application.
(3) Accuracy and Relevance of LLM Assistance: We compared
queries and LLM response logs against pre-established ground
truth to assess: a) how accurately the LLM provided instruc-
tions needed for successful software task completion; and,
b) how relevant the LLM response was to the users’ input
prompt.
Finally, to complement our experimental findings, we corrob-
orated the data with participants’ think-aloud verbalizations and
probed into the reasons behind users’ decisions and identify any
potential gaps in their mental models about how LLMs work. We
used an inductive analysis approach [12] and affinity diagrams [12]
along with discussions amongst the research team to categorize
the interview findings and identify key recurring themes. In par-
ticular, our coding approach for the inductive analysis considered
reasons influencing users’ perception of how prompts impact out-
put, recognition of accurate vs. hallucinated output, and difficulties
in applying the LLM assistance to the software task.
4
RESULTS
4.1
Task Completion, Accuracy and Relevance
of LLM Assistance
Expert-rated Accuracy and Relevancy of LLM assistance: As
predicted, we found that the assistance provided by SoftAIBot was
more accurate than Baseline ChatGPT for both PowerPoint (SoftAI-
Bot: Mean=64.4%; Baseline ChatGPT: Mean=37.5%) and Excel (Soft-
AIBot: Mean=65.7%; Baseline ChatGPT: Mean=45%) tasks. These
differences between accuracy scores and LLM interventions were
significant for PowerPoint (t(21.3) =4.0, p=0.0006, two-tailed) and
Excel tasks(t(23.4) =3.7, p=0.0011, two-tailed). Similarly, we found
that the assistance provided by SoftAIBot had higher average rele-
vancy score than Baseline ChatGPT for both PowerPoint (SoftAIBot:
Mean=74.7%; Baseline ChatGPT: Mean=44.4%) and Excel (SoftAIBot:
Mean=78.2%; Baseline ChatGPT: Mean=55.4%) tasks, with signif-
icant differences (PowerPoint: t(24.5) =5.4, p<0.0001, two-tailed;
Excel: t(24.8) =4.8, p<0.0001, two-tailed).
Since the Baseline lacked relevant domain context, it typically
failed to offer relevant and accurate steps available within the soft-
ware (e.g., macros, animations, motion paths, etc.). Instead, it often
provided references to features and functionality that did not ex-
ist. This phenomenon of the LLM providing information that is
somewhat relevant to the user’s query but not accurate to the users’
intent for performing the task has been termed as an hallucination
[4, 6, 29] in the literature (See Figure 4). On the other hand, while
SoftAIBot provided step-by-step instructions on how to implement
the required functionality in the software, it also demonstrated
instances of hallucination. Furthermore, it did not always provide
specific and relevant instructions (for example, where to locate
menu functions within the UI).
In subsequent sections, we shed light on users’ performance
and qualitative perceptions of both LLMs, highlighting various
inconsistencies and misconceptions among users that impacted
their use of prompt-based interactions.
Task Completion and Task Accuracy: None of the partic-
ipants were able to completely finish either of the tasks in Pow-
erpoint or Excel, even though each participant made full use of
both LLM assistants (Baseline ChatGPT and SoftAIbot). On average,
participants completed 35% of the PowerPoint tasks (maximum=
50% and minimum= 0%) with Baseline ChatGPT and 45% of the
tasks (maximum= 60% and minimum= 0%) with SoftAIBot with no
significant difference across the two LLM interventions (t (28.1)=
1.1, p= 0.28, two-tailed). Similarly, participants completed 40% of
the Excel tasks (maximum= 55% and minimum= 0%) with Baseline
ChatGPT and 55% of the tasks (maximum= 75% and minimum=
0%) with SoftAIBot with no significant difference across the two
interventions (t(28.7)=1.9, p=0.07, two-tailed).
Among the portion of the task completed by each participant,
the accuracy scores were low. For the PowerPoint tasks, the average
task accuracy score across all participants was 31.3% using SoftAI-
Bot (maximum= 50%, minimum =0%) and only 17.2% using Baseline
ChatGPT (maximum= 50% and minimum= 0%). Although users
Why and When LLM-Based Assistants Can Go Wrong
IUI ’24, March 18–21, 2024, Greenville, SC, USA
Figure 3: Overview of participants’ responses to post-task questionnaire. Pearson Chi-Squared test showed no significant
difference for each metric across both LLM interventions for completing both Excel and PowerPoint tasks. Despite having
low completion rate and low task accuracy, the majority of users perceived that they obtained accurate (a) and relevant (b)
assistance from both LLM interventions. Still, the majority of participants (c) found it difficult to apply LLM assistance and
instructions to the software application to complete their task; (d) Participants overall did not find it difficult to craft prompts;
a few participants did indicate that they struggled to find the correct words for Powerpoint tasks that were more visual and
interactive. Although expert ratings showed that users did not finish the task accurately, most users believed that it was easier
for them to finish the task using both forms of LLM assistance; (f) The majority of users trusted both LLMs - this was surprising
to see because expert ratings showed that both LLMs frequently provided inaccurate assistance.
achieved better task accuracy scores with SoftAIBot in comparison
to Baseline, paired-sample t-test showed no significant difference
across both interventions (t(26.6) =1.7, p=0.10, two-tailed), and we
did not observe any order effects. The trend of non-significant
accuracy persisted in Excel tasks as well (t(29.9) =1.9, p=0.06).
Users’ Perceptions of Accuracy and Relevancy of LLM as-
sistance: Even though task completion scores were poor across
both LLM interventions, in the self-report data, surprisingly, the ma-
jority of users perceived assistance from both LLMs to be accurate
(SoftAIBot: 12/16 participants; Baseline ChatGPT: 8/16 participants)
and relevant (SoftAIBot: 13/16 participants; Baseline ChatGPT: 9/16
participants) in completing the PowerPoint tasks. Pearson Chi-
Squared test showed no significant difference in perceived accuracy
(𝜒2(4, 𝑁= 28) = 3.05, p =0.55) and relevance of LLM assistance
(𝜒2(3, 𝑁= 28) = 1.46, p =0.69). Similarly for Excel tasks, there was
no significant difference in perceived accuracy (𝜒2(3, 𝑁= 28) =
3.96, p =0.27) and relevancy (𝜒2(2, 𝑁= 28) = 0.29, p =0.86) across
both LLM interventions.
In terms of other self-report data, such as users’ perceptions
of difficulty in applying assistance across both interventions, diffi-
cultly in figuring out correct input prompt, and ease of completing
the task using LLM assistance, we did not observe any statistical
difference across SoftAIbot and Baseline (See Figure 3 for the detail
statistical results and test on remaining metrics). The one exception
was the perception of trust as, interestingly, we observed that most
participants (14/16) trusted SoftAIbot more than the Baseline Chat-
GPT (4/16) for the PowerPoint tasks and this result was significant
(𝜒2(4, 𝑁= 32) = 14.40, p =0.006), but for the Excel task, there was
no significant difference in users’ perceptions of trust.
The key takeaways from our experiment were that while the
expert rating showed that SoftAIBot performed better than the
Baseline ChatGPT in producing more accurate results, users could
not recognize the differences in accuracy and relevance among
both LLMs. Furthermore, having more accurate and relevant LLM
output did not impact task completion nor task accuracy across
both software applications. Next, we use our qualitative findings
to explain factors that impacted user performance and highlight
some of the key challenges that users experienced in figuring out
IUI ’24, March 18–21, 2024, Greenville, SC, USA
Khurana et al.
Figure 4: LLM Hallucination evidence (P15): In response to P15’s prompt, “I want you to give me instructions on how to animate
a shape that rotates from top to lower middle side and then come back up almost like a zigzag.”, Baseline ChatGPT generated
the hallucinated response of Zigzag menu option (highlighted in red) which did not even exist in the software application.
appropriate prompts and applying LLM assistance for different
software tasks.
4.2
Inconsistency in Prompt-based Interaction
Prompt guidelines and prompt engineers usually suggest that break-
ing down the task and prompting it as a process for LLM can usu-
ally lead to desired LLM outputs. However, with the exception of 2
participants in our study who followed this approach and were suc-
cessful (See example of Participant P02 in Figure 6), all of the other
participants were inconsistent and varied in how they constructed
prompts, failing to leverage the in-context prompt guidelines.
Constructing Prompts as Search Queries: Most participants
(10/16) started with a generic “how to do the [task]” prompt, such as
(e.g., “how to animate in Microsoft PowerPoint” (P14), “how to do
correlation in Microsoft excel” (P04)). Most users were translating
their mental model from other query-based systems, relying on
similar queries they would issue on Google: “I think it [LLM] relies
on the keywords that I am giving...at the beginning I just formulated
a very vague question because it’s easier to get started with the vague
question and then I can refine it as I go.” (P08)
Even after users experimented with different phrasings, they
could not understand why the LLMs were producing nearly identi-
cal responses. These participants did not have an accurate mental
model of how LLMs work and did not appear to recognize the im-
pact of the prompt text on the quality of the LLM output: “I think it
works same as a search engine. It has a back end and it takes your
question through tons of data...it tries to give you an answer with
all of that data that it has in the back end..it’s so quick that it goes
through it within nanoseconds..” (P15)
Some participants (4/16), without even interpreting the task,
just copy-pasted the entire task instructions along with some data
sample (e.g., for Excel tasks) in hopes that the LLM would simplify
the task and provide some instructions. However, both of these
approaches were not that successful (as shown in Figure 5), as users
had to ask follow-up questions on figuring out the correct steps.
For example, when P07 (Figure 5) prompted the LLM using the
keywords interpreted from the task, they struggled in getting an
relevant response and went through several rounds of clarifications
with the LLM. Only 2/16 participants who broke down the task and
drafted prompts as a process for SoftAIBot LLM obtained desired
LLM outputs (See example of P02 in Figure 6)
Using Trial and Error Due to the Vocabulary Problem: The
majority of participants (11/16) also struggled in crafting prompts
because they did not know how to express their task intent us-
ing software-specific terminology (often termed as the vocabulary
problem [16]). This was especially prevalent during the PowerPoint
tasks that involved references to different visual and interactive
elements and participants frequently engaged in long trial-and-
error episodes. Participants found it challenging to articulate their
intended actions accurately and frequently blamed themselves: “I
did not know how to describe those visual graphics in PowerPoint...I
said words like flip and move but I am not sure if it was right for
these kind of tasks . It took me a lot of time to try to understand which
menu to select and not being sure if my prompt was ok or not... If the
problem was my prompt or my system or the problem was the solu-
tion provided, I was not sure which one of them was making mistake.”
(P05)
Ignoring Prompt Guidelines: Contrary to our hypothesis,
only a handful of participants (5/16) employed the in-context prompt
guidelines provided by SoftAIBot within the context of their queries
and the majority simply ignored them. Participants expressed that
the prompt guidelines were “not necessary” and “not useful” as it
was faster for them to iterate on their own prompts. This behaviour
was similar to the phenomenon commonly referred to as the “active
user paradox” [10]. In fact, about half of the participants (7/16) were
confident that they already possess the knowledge and experience
required for generating prompts: “I would not say it is hurtful to have
it [prompt guidelines] but is not necessary. I knew what to search for.
I do not think prompt guidelines would have helped...” (P06) Some
participants (4/16) noted that they were confident about crafting
their own prompts because the LLMs were able to accept “any in-
put” and generate corresponding output. If needed, they can revisit
the generated response and assess their prompt alignment with
their intended outcome for further improvement: “It is not difficult
to figure out words because it [LLM] was accepting anything I typed.
I can go back and verify whether that is what I need.” (P11)
4.3
User Perception of LLM Assistance
Some of the surprising results from our experiment were that users
did not recognize the differences in accuracy and relevance between
the two LLMs nor were they able to leverage the assistance to
Why and When LLM-Based Assistants Can Go Wrong
IUI ’24, March 18–21, 2024, Greenville, SC, USA
Figure 5: Unsuccessful Prompting by using keyword-based approach with Baseline ChatGPT: (a) P07 prompted the LLM using
the keywords interpreted from the task, and struggled in getting an relevant response and went through several rounds of
clarifications with the LLM; (b) Eventually, user could not even get started and failed to perform the task on the software
application (P07).
complete the software tasks accurately. Below, we discuss some
factors that shaped users’ perceptions and use of both LLMs.
LLMs’ Ability to Generate Response Created an Impression
of Credibility: Most participants (13/16) perceived that both LLMs
produced relevant responses because in contrast to systems like Siri
and Google, which do not always provide response for every user’s
query, both LLM consistently generated a reasonable response
matching the user’s prompt. Thus, most participants formed an
impression that the LLM outputs were credible: “... it [LLM] gave me
what was relevant to my query...like some steps to find those options.
This bot [LLM] was more detailed even than Siri. When you ask Siri,
it’s not giving what exactly I am looking for and keeps on giving me
some different options...which is unnecessary about the topic. But in
this one [LLM], what you put that’s what you’re getting. So the output
is like, 90 to 95% near to what you just asked...so that made me trust
it.” (P12)
Difficulty in Applying LLM Assistance to the Software
Application: In addition to the tensions in forming an accurate
mental model of both LLMs (as described in Section 4.2), our par-
ticipants who were infrequent users of Powerpoint and Excel also
struggled because they lacked an accurate mental model of these
applications. We observed that participants were quick to blame
themselves for not being able to apply the instructions given by
LLM due to lack of their familiarity with software: “For some reason,
I got what it [LLM] tells me but...maybe it’s giving me correct step,
but I am not able to apply to Excel because I’m not a regular user.
I need to figure out [myself] how to use this formula; where can I
put my formula and how to apply it.” In comparison to SoftAIBot,
users felt that the Baseline ChatGPT generated more generic or
vague responses which users struggled to apply to software tasks.
In fact, 9/16 participants began to doubt the credibility of Baseline
ChatGPT: “Unless you apply the steps, you do not know whether that
[Baseline ChatGPT] works or not.” (P06)
In cases where the LLM output was actually relevant and ac-
curate, our participants still struggled to locate menu options and
apply LLM instructions. They explained that it was due to the lack
of visual cues or guidance that are typically provided in tutorial
videos and other forms of visual help: “...the instructions were here,
but sometimes it gets difficult to use...If I get an image or graphical
help along with screenshots of where to look for particular option, I
could have made it slightly faster.” (P11) Due to this difficulty in
locating and applying LLM assistance, few participants (6/16) came
up with different theories on whether LLM output did not consid-
ered their system version, as commented by P04: “It [LLM] did not
provide instructions for the version of the software I’m using. Some-
times it’s little bit difficult to apply the assistance, sometimes I would
not find the button he [LLM] asked me to look for.” (P04)
When the LLMs hallucinated or produced an incorrect set of
instructions, we observed that more than half of the participants
(10/16) could not map the (wrongly generated) LLM output to the
intended features and menu functions in the application. As a result,
they formed an incorrect mental model of the software application’s
IUI ’24, March 18–21, 2024, Greenville, SC, USA
Khurana et al.
Figure 6: Example of successful prompting by Participant P02. They started by breaking down the task, beginning with asking
steps to (a) implement shape, followed by (b) animations in PowerPoint task. User prompted SoftAIBot as a step-by-step process
asking steps for each functionality at one time (P02).
user interface. Instead of having awareness of this LLM’s bias of
hallucination, these users felt burdened to make LLM instructions
work: “For someone who is very new to PowerPoint...I could not find
like the stuff, the menu names [ZigZag] that it was mentioning. It
was not there in my application. I am not sure why all the burden of
[finding] came to me.”(P15) Only a few participants (3/16) were able
to recognize that the LLM does not always give factual information.
They compared LLMs with Stack Overflow and Google search which
they considered to be more credible than LLMs: “ ...if it’s a Stack
Overflow, I would know that it’s just one person’s comment so I have
ways to verify how trustable that instruction is...with Google, I have
that much control over the source of information. But with LLM, I
have no way to verify that at first sight. I have to follow it and, and
decide on my own if it works or not. I prefer to be able to verify the
credibility of a solution before actually going through the steps and
putting more time to it.” (P07)
4.4
Coherent LLM Output Leads to Blind Faith
The presentation of LLM output fosters trust: The most sur-
prising finding from our study was that after receiving an output
from the LLM, most users blindly followed the provided steps with-
out a critical evaluation of the output’s veracity. For example, as
illustrated in Figure 4, even when Baseline ChatGPT produced the
hallucinated response of the Zigzag menu option which did not
even exist in the software application, P15 still demonstrated unwa-
vering trust of this LLM. The boundary between right and wrong
for the participants while seeking LLM assistance, even in cases
where it might yield incorrect results, was blurred as both LLMs
consistently produced output that is coherent and in plain English,
which enhances its perceived credibility. Especially for SoftAIBot,
most participants (14/16) assumed that LLM is correct just because
it generated a well-formatted response in response to their query:
“I trust the system because once I get the match answer from him
[LLM]...makes me feel he [LLM] is helping and he is better than me.”
(P03) Another user who followed SoftAIBot’s step-by-step instruc-
tions for PowerPoint’s visual tasks commented: “I was able to trust
it [SoftAIBot] because I liked the way it gave these steps. SoftAIBot is
more specific and gave me step-by-step sort of directions...it was user
friendly and easy to follow. I could find all the steps that it [SoftAIBot]
was referring to. Because this is what you need when you ask AI for
help. You look for baby steps.” (P14)
5
DISCUSSION
We have contributed insights into how novices employ new-generation
LLM assistants to seek software help, highlighting many of the chal-
lenges that users face while crafting prompts, comprehending how
prompts bias LLM output, and mapping the LLM-suggested steps to
software. Our key findings suggest that even though SoftAIBot out-
performed the Baseline ChatGPT in providing relevant and accurate
software-related assistance, there were no significant differences in
users’ task completion rates or task accuracy scores between the
two conditions. Notably, as opposed to our hypotheses H1-H2, there
was no difference in users’ perceptions of accuracy and relevance
for both LLMs, and users mostly failed to recognize instances where
the models provided incorrect answers, including hallucinations.
Our qualitative findings further shed light on our research questions
and reveal a lack of awareness among participants regarding LLMs’
biases and limitations. Participants attributed their inability to com-
plete a task and locate suggested features in the application to their
personal shortcomings rather than recognizing instances of LLM
hallucinations offering nonexistent options. Additionally, users mis-
understood the prompt text’s influence on LLM output, likening
prompts to traditional search engine keywords. User perception of
LLM responses as contextually relevant stemmed from a bias to-
wards the query context. Importantly, our study highlights the risks
of undue trust in LLMs, as users frequently exhibited unwarranted
confidence in LLM-generated responses due to their human-like
Why and When LLM-Based Assistants Can Go Wrong
IUI ’24, March 18–21, 2024, Greenville, SC, USA
nature and consistent, contextual relevance, distinguishing them
from traditional chatbots or virtual assistants like Siri.
The implications of our research extend beyond the immediate
findings and have far-reaching significance for the broader IUI
research community. Our observations highlight the need for end-
users to exercise caution and critical thinking when relying on
LLMs for software-related assistance. In an era where LLMs are
increasingly integrated into various facets of daily life, from virtual
assistants to content generation tools, understanding user percep-
tions and misconceptions about these systems is imperative. By
shedding light on the lack of awareness regarding LLM biases and
hallucinations, our study calls for a fundamental reevaluation of
the way we design, deploy, and educate users about AI-powered
assistants.
We now reflect on our key insights and highlight opportunities
for designing LLM assistance for feature-rich software tasks while
promoting transparent, responsible LLM interfaces to enhance user
understanding and mental model formation. Our findings will be
valuable for IUI and HCI researchers, interface designers, developers
and others working on LLM-powered assistants.
5.1
Integrating LLM help into feature-rich
applications
Our results demonstrate the value that LLM-based assistants, such
as ChatGPT, can provide in generating relevant software-related
assistance within a single platform. Unlike traditional help-seeking
resources and chatbots (e.g., Google search, blogs, Siri, etc.) where
users have to assimilate help content through multiple outlets, our
participants appreciated receiving relevant detailed instructions by
typing in a prompt. Having said that, our findings resonates with
the speculations of other researchers [20] that Baseline GPT-4 is not
meant to provide assistance for all types of tasks. Our SoftAIBot,
that was optimized for particular feature-rich software guidance
context, performed better and generated more relevant and accurate
step-by-step software assistance. In our approach, we employed
Retrieval Augmented Generation (RAG) on standard software doc-
umentation. Future developments could involve instruction tuning
[53], which includes pairing more specific instructions with the
software-specific steps and correlating this with expected output.
The onus should transition from users to software developers and
customer support to create such instructional pairs for ensuring
them to be crafted in a manner that allows general-purpose LLMs
to be fine-tuned [30] for generating user-centered and optimized
software guidance.
While SoftAIBot generated relevant and accurate software assis-
tance compared to Baseline ChatGPT, there were obvious limita-
tions as users were not able to leverage this information to complete
the software tasks accurately. In particular, users found the textual
LLM output to be limiting compared to other visual-based help-
seeking mediums. For example, software instructions on YouTube
allow users to follow procedural steps “as is” without needing to
verify and locate specific features. However, with LLMs, users had
difficulty in mapping the LLM output to features in the software,
especially in cases where the LLM was hallucinating and referred to
non-existent features. Video-based help-seeking mediums should
not be dismissed as instructional tools. Instead, to mitigate the issue
of locating the exact instructions that users experience with videos,
there is an opportunity for technologies like ChatGPT to aid in video
summarization tasks and to extract more relevant snippets from
videos [15], enhancing the ease of locating specific information.
5.2
Transparent and Responsible Interface
Design of LLMs
At a more fundamental level, our study raises some caution: while
developers and researchers are investing in improving AI models
and how LLMs can provide context-specific guidance, users may
not always perceive these enhancements as substantial improve-
ments in accuracy and relevance. Recent literature has already
raised concerns about users’ over-reliance on AI systems, such as
in the context of AI-based maze-solving tasks [49]. Although the
landscape of user behaviors and mental models is more multifaceted
with LLMs, our study demonstrates a similar phenomena of over-
trust with LLMs. Furthermore, we extend prior works by revealing
the nuances in users’ mental models of the LLMs triggered by the
inherent biases of LLMs, leading to overtrust and users’ failure to
recognize erroneous or hallucinated output: “Because it is AI, how
can it be wrong? I am going to stop using my brain as I literally gave
it gibberish and still it works. I [will] doubt myself before doubting AI.”
(P15) Such overtrust in LLM assistants can be dangerous, especially
for novice users who do not have familiarity with the underlying
powerful AI technology. These findings from our study underscore
the complexity of user interactions with AI and highlight the need
for more transparency in addressing users’ expectations and mis-
conceptions. This can be as critical as advancing the underlying AI
technologies and it is essential to design interfaces that are more
transparent and responsible [45]. There is need to consider more in-
novative user-centered solutions for mitigating bias and enhancing
transparency in AI systems, thereby contributing to the responsible
and ethical development of AI technologies.
To enhance the transparency of LLMs among end-users, one ap-
proach could be to embed interpretability within these systems. By
articulating why and how LLMs derive specific recommendations,
users can gain perspectives into the underlying mechanisms and
trust the provided instructions with a higher degree of certainty.
One possible direction is through the illustration of confidence
percentage scores for each LLM instruction as these scores have
shown to enhance the perception of transparency and trust [21].
Other potential direction is through explainability techniques [37]
(e.g., visual example-based explanations [9, 21]) that can indicate
why the system did what it did and verify an AI’s recommendation
[13] by demonstrating the similarities between users’ intent and
examples in the training set [9, 21, 27, 28]. Training datasets of
these LLMs needs to be designed such that confidence scores or
visual examples are part of the dataset to enhance transparency
within the LLM technologies. Such innovative advancements in
LLM development not only pave the way for enhanced user inter-
action but also ensure that the model’s suggestions are verifiable
and reliable.
5.3
Bridging the Gap Between Mental Models
and LLM Interfaces
With the rapid pace of innovations in Generative AI and LLMs,
there is need for HCI research to focus on understanding user
IUI ’24, March 18–21, 2024, Greenville, SC, USA
Khurana et al.
perceptions and users’ mental models of LLM-based assistants for
software help-seeking. Our study complements existing works high-
lighting usability issues in crafting prompts [5, 41, 51] and provides
initial insights into the gaps in end-users’ mental model when us-
ing prompt-based interactions in context of software help-seeking.
The affordances of LLMs can be misleading as they are designed
to be walk-up-and-use and support natural language interaction.
However, similar to insights from recent work on non-ML expert
designers prototyping ML apps [52], we also found that crafting
effective prompts is cumbersome, especially for non-AI experts.
Our study contributes new knowledge: non-AI expert end-users
of ML/LLM applications must adapt their existing mental models
from traditional help-seeking mediums to understand the new inter-
face of LLMs. To bridge the gap between users’ mental models and
LLM user interfaces, there is an urgent need to leverage strategies
such as think-aloud studies to further understand nuances in users’
mental models of LLMs [33]. Although we did not see significant
individual differences in our sample, it may be worth investigating
how different sub-groups of software users might benefit from LLM
help seeking. Our study demonstrates that we cannot assume users
will intuitively grasp the capabilities and limitations of LLMs. There
is a clear need for comprehensive user training and education and
clear communication about how Generative AI systems operate.
With software help-seeking, the challenge for end-users lies not
only in flawed mental models of LLMs but also in the absence of
a clear understanding of the underlying software application. Our
research complements the emerging work in this space, being novel
in documenting the challenges users encounter with LLMs for soft-
ware help tasks, including their mental models and overtrust, which
impacts both their utilization of LLMs and perception of software
features, regardless of LLM optimization or explicit prompt guide-
lines (SoftAIBot). Users found it difficult to understand, map, and
apply LLM instructions to software features. To address this, en-
hancing the UX design of LLM interfaces by highlighting relevant
software UI sections during onboarding can improve user interac-
tion, particularly for feature-rich software[21, 27, 28]. Exploring
the interplay between users’ understanding of LLMs and the under-
lying software presents new human-AI design possibilities. Overall,
our findings show that LLMs optimized for generating software spe-
cific guidance (e.g., Copilot [43]) and embedded inside feature-rich
applications, could be promising for learning and using complex
features. Once these systems become available, future studies can
compare our findings from SoftAIBot with such systems and fur-
ther investigate the level of guidance and automation that may be
appropriate for LLMs generating software guidance.
6
LIMITATIONS
In this paper, we experimented with two LLM-based assistants to
explore how end-users make use of them for software tasks related
to feature-rich applications. While our findings shed new light on
users struggles in employing LLMs for software help-seeking, some
caution should be used in interpreting our results. For instance, our
findings could be constrained by the specific applications utilized
during the experimentation. Whether our findings would generalize
beyond the state-of-the-art LLM implementations used in the study
should be investigated in future work. Given the rapid evolution and
variability among modern LLMs, the outcomes may be constrained
to currently available LLMs. Although we recruited end-users who
were non-AI experts, we did not control for other individual differ-
ences, such as expertise or familiarity with underlying feature-rich
software applications. Future studies should conduct experiments
and more qualitative studies with larger and varying populations
to better capture these individual differences.
7
CONCLUSIONS
In this research, we investigated the effectiveness of the LLM-
generated software guidance and prompt guidelines, by comparing
the Baseline LLM assistant with our own implemented SoftAIBot
in providing accurate and relevant assistance for software tasks.
Our results highlight the challenges users faced in following LLM
assistance and point to instances of users attributing flaws to them-
selves instead of recognizing LLM biases. Our study highlights the
pressing need for interdisciplinary collaboration among researchers,
designers, developers, and educators to bridge the gap between user
expectations and AI realities. By addressing these challenges head-
on, we can foster a future where AI systems are not only more
powerful but also more comprehensible and accountable to their
users, ultimately facilitating human-AI interaction across a range
of domains.
ACKNOWLEDGMENTS
We thank the Natural Sciences and Engineering Research Council
of Canada (NSERC) for funding this research.
REFERENCES
[1] Adobe. 2023.
Adobe unveils Firefly, a family of new creative generative
ai.
https://news.adobe.com/news/news-details/2023/Adobe-Unveils-Firefly-
a-Family-of-new-Creative-Generative-AI/default.aspx
[2] Open AI. 2022. Introducing chatgpt. https://openai.com/blog/chatgpt
[3] Oscar D. Andrade, Nathaniel Bean, and David G. Novick. 2009. The Macro-
Structure of Use of Help. In Proceedings of the 27th ACM International Con-
ference on Design of Communication (Bloomington, Indiana, USA) (SIGDOC
’09). Association for Computing Machinery, New York, NY, USA, 143–150.
https://doi.org/10.1145/1621995.1622022
[4] Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan
Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. 2023. A multitask,
multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and
interactivity. arXiv preprint arXiv:2302.04023 (2023).
[5] Shraddha Barke, Michael B. James, and Nadia Polikarpova. 2023. Grounded
Copilot: How Programmers Interact with Code-Generating Models. Proc. ACM
Program. Lang. 7, OOPSLA1, Article 78 (apr 2023), 27 pages. https://doi.org/10.
1145/3586030
[6] Ali Borji. 2023.
A categorical archive of chatgpt failures.
arXiv preprint
arXiv:2302.03494 (2023).
[7] Joel Brandt, Mira Dontcheva, Marcos Weskamp, and Scott R. Klemmer. 2010.
Example-Centric Programming: Integrating Web Search into the Development
Environment. In Proceedings of the SIGCHI Conference on Human Factors in Com-
puting Systems (Atlanta, Georgia, USA) (CHI ’10). Association for Computing Ma-
chinery, New York, NY, USA, 513–522. https://doi.org/10.1145/1753326.1753402
[8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter,
Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin
Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya
Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners.
In Advances in Neural Information Processing Systems, H. Larochelle, M. Ran-
zato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates,
Inc., 1877–1901.
https://proceedings.neurips.cc/paper_files/paper/2020/file/
1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf
[9] Carrie J. Cai, Jonas Jongejan, and Jess Holbrook. 2019. The Effects of Example-
Based Explanations in a Machine Learning Interface. In Proceedings of the 24th
Why and When LLM-Based Assistants Can Go Wrong
IUI ’24, March 18–21, 2024, Greenville, SC, USA
International Conference on Intelligent User Interfaces (Marina del Ray, California)
(IUI ’19). Association for Computing Machinery, New York, NY, USA, 258–262.
https://doi.org/10.1145/3301275.3302289
[10] John M. Carroll and Mary Beth Rosson. 1987. Paradox of the Active User. MIT
Press, Cambridge, MA, USA, 80–111.
[11] Parmit K. Chilana, Amy J. Ko, and Jacob O. Wobbrock. 2012. LemonAid: Selection-
Based Crowdsourced Contextual Help for Web Applications. In Proceedings of the
SIGCHI Conference on Human Factors in Computing Systems (Austin, Texas, USA)
(CHI ’12). Association for Computing Machinery, New York, NY, USA, 1549–1558.
https://doi.org/10.1145/2207676.2208620
[12] Juliet M Corbin and Anselm Strauss. 1990. Grounded theory research: Procedures,
canons, and evaluative criteria. Qualitative sociology 13, 1 (1990), 3–21.
[13] Raymond Fok and Daniel S Weld. 2023. In Search of Verifiability: Explanations
Rarely Enable Complementary Performance in AI-Advised Decision Making.
arXiv preprint arXiv:2305.07722 (2023).
[14] Adam Fourney, Ben Lafreniere, Parmit Chilana, and Michael Terry. 2014. In-
terTwine: Creating Interapplication Information Scent to Support Coordinated
Use of Software. In Proceedings of the 27th Annual ACM Symposium on User
Interface Software and Technology (Honolulu, Hawaii, USA) (UIST ’14). As-
sociation for Computing Machinery, New York, NY, USA, 429–438.
https:
//doi.org/10.1145/2642918.2647420
[15] C. Ailie Fraser, Julia M. Markel, N. James Basa, Mira Dontcheva, and Scott Klem-
mer. 2020. ReMap: Lowering the Barrier to Help-Seeking with Multimodal Search.
In Proceedings of the 33rd Annual ACM Symposium on User Interface Software and
Technology (Virtual Event, USA) (UIST ’20). Association for Computing Machin-
ery, New York, NY, USA, 979–986. https://doi.org/10.1145/3379337.3415592
[16] G. W. Furnas, T. K. Landauer, L. M. Gomez, and S. T. Dumais. 1987. The Vocabulary
Problem in Human-System Communication. Commun. ACM 30, 11 (nov 1987),
964–971. https://doi.org/10.1145/32206.32212
[17] Tovi Grossman and George Fitzmaurice. 2010. ToolClips: An Investigation of
Contextual Video Assistance for Functionality Understanding. In Proceedings of
the SIGCHI Conference on Human Factors in Computing Systems (Atlanta, Georgia,
USA) (CHI ’10). Association for Computing Machinery, New York, NY, USA,
1515–1524. https://doi.org/10.1145/1753326.1753552
[18] Tovi Grossman, George Fitzmaurice, and Ramtin Attar. 2009. A Survey of Software
Learnability: Metrics, Methodologies and Guidelines. In Proceedings of the SIGCHI
Conference on Human Factors in Computing Systems (Boston, MA, USA) (CHI ’09).
Association for Computing Machinery, New York, NY, USA, 649–658.
https:
//doi.org/10.1145/1518701.1518803
[19] Björn Hartmann, Daniel MacDougall, Joel Brandt, and Scott R. Klemmer. 2010.
What Would Other Programmers Do: Suggesting Solutions to Error Messages. In
Proceedings of the SIGCHI Conference on Human Factors in Computing Systems
(Atlanta, Georgia, USA) (CHI ’10). Association for Computing Machinery, New
York, NY, USA, 1019–1028. https://doi.org/10.1145/1753326.1753478
[20] Samantha Murphy Kelly. 2023. 5 jaw-dropping things GPT-4 can do that chatgpt
couldn’t | CNN business.
https://www.cnn.com/2023/03/16/tech/gpt-4-use-
cases/index.html
[21] Anjali Khurana, Parsa Alamzadeh, and Parmit K. Chilana. 2021. ChatrEx: De-
signing Explainable Chatbot Interfaces for Enhancing Usefulness, Transparency,
and Trust. In 2021 IEEE Symposium on Visual Languages and Human-Centric
Computing (VL/HCC). 1–11. https://doi.org/10.1109/VL/HCC51201.2021.9576440
[22] Kimia Kiani, George Cui, Andrea Bunt, Joanna McGrenere, and Parmit K. Chilana.
2019. Beyond ""One-Size-Fits-All"": Understanding the Diversity in How Software
Newcomers Discover and Make Use of Help Resources. In Proceedings of the
2019 CHI Conference on Human Factors in Computing Systems (Glasgow, Scotland
Uk) (CHI ’19). Association for Computing Machinery, New York, NY, USA, 1–14.
https://doi.org/10.1145/3290605.3300570
[23] Juho Kim, Phu Tran Nguyen, Sarah Weir, Philip J. Guo, Robert C. Miller, and
Krzysztof Z. Gajos. 2014. Crowdsourcing Step-by-Step Information Extraction
to Enhance Existing How-to Videos. In Proceedings of the SIGCHI Conference
on Human Factors in Computing Systems (Toronto, Ontario, Canada) (CHI ’14).
Association for Computing Machinery, New York, NY, USA, 4017–4026. https:
//doi.org/10.1145/2556288.2556986
[24] Benjamin Lafreniere, Parmit K. Chilana, Adam Fourney, and Michael A. Terry.
2015.
These Aren’t the Commands You’re Looking For: Addressing False
Feedforward in Feature-Rich Software. In Proceedings of the 28th Annual ACM
Symposium on User Interface Software and Technology (Charlotte, NC, USA)
(UIST ’15). Association for Computing Machinery, New York, NY, USA, 619–628.
https://doi.org/10.1145/2807442.2807482
[25] Benjamin Lafreniere, Tovi Grossman, and George Fitzmaurice. 2013. Community
Enhanced Tutorials: Improving Tutorials with Multiple Demonstrations. In Pro-
ceedings of the SIGCHI Conference on Human Factors in Computing Systems (Paris,
France) (CHI ’13). Association for Computing Machinery, New York, NY, USA,
1779–1788. https://doi.org/10.1145/2470654.2466235
[26] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,
Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel,
Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-Augmented Generation
for Knowledge-Intensive NLP Tasks. In Proceedings of the 34th International
Conference on Neural Information Processing Systems (Vancouver, BC, Canada)
(NIPS’20). Curran Associates Inc., Red Hook, NY, USA, Article 793, 16 pages.
[27] Toby Jia-Jun Li, Jingya Chen, Haijun Xia, Tom M. Mitchell, and Brad A. Myers.
2020. Multi-Modal Repairs of Conversational Breakdowns in Task-Oriented
Dialogs. In Proceedings of the 33rd Annual ACM Symposium on User Interface Soft-
ware and Technology (Virtual Event, USA) (UIST ’20). Association for Computing
Machinery, New York, NY, USA, 1094–1107.
https://doi.org/10.1145/3379337.
3415820
[28] Toby Jia-Jun Li, Igor Labutov, Xiaohan Nancy Li, Xiaoyi Zhang, Wenze Shi,
Wanling Ding, Tom M. Mitchell, and Brad A. Myers. 2018.
APPINITE: A
Multi-Modal Interface for Specifying Data Descriptions in Programming by
Demonstration Using Natural Language Instructions. In 2018 IEEE Sympo-
sium on Visual Languages and Human-Centric Computing (VL/HCC). 105–114.
https://doi.org/10.1109/VLHCC.2018.8506506
[29] Q Vera Liao and Jennifer Wortman Vaughan. 2023. AI Transparency in the Age
of LLMs: A Human-Centered Research Roadmap. arXiv preprint arXiv:2306.01941
(2023).
[30] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang,
Mohit Bansal, and Colin A Raffel. 2022.
Few-Shot Parameter-Efficient
Fine-Tuning is Better and Cheaper than In-Context Learning. In Advances
in Neural Information Processing Systems, S. Koyejo, S. Mohamed, A. Agar-
wal, D. Belgrave, K. Cho, and A. Oh (Eds.), Vol. 35. Curran Associates,
Inc., 1950–1965.
https://proceedings.neurips.cc/paper_files/paper/2022/file/
0cde695b83bd186c1fd456302888454c-Paper-Conference.pdf
[31] HARRY MCCRACKEN. 2023. Microsoft’s Satya Nadella is winning Big Tech’s
AI War. here’s how.
https://www.fastcompany.com/90931084/satya-nadella-
microsoft-ai-frontrunner
[32] Niklas Muennighoff, Nouamane Tazi, Loïc Magne, and Nils Reimers. 2022. MTEB:
Massive text embedding benchmark. arXiv preprint arXiv:2210.07316 (2022).
[33] Don Norman. 2013. The design of everyday things: Revised and expanded edition.
Basic books.
[34] David G. Novick, Oscar D. Andrade, and Nathaniel Bean. 2009. The Micro-
Structure of Use of Help. In Proceedings of the 27th ACM International Confer-
ence on Design of Communication (Bloomington, Indiana, USA) (SIGDOC ’09).
Association for Computing Machinery, New York, NY, USA, 97–104.
https:
//doi.org/10.1145/1621995.1622014
[35] David G. Novick, Oscar D. Andrade, Nathaniel Bean, and Edith Elizalde. 2008.
Help-Based Tutorials. In Proceedings of the 26th Annual ACM International Con-
ference on Design of Communication (Lisbon, Portugal) (SIGDOC ’08). Association
for Computing Machinery, New York, NY, USA, 1–8. https://doi.org/10.1145/
1456536.1456538
[36] David G. Novick and Karen Ward. 2006. Why Don’t People Read the Manual?. In
Proceedings of the 24th Annual ACM International Conference on Design of Com-
munication (Myrtle Beach, SC, USA) (SIGDOC ’06). Association for Computing
Machinery, New York, NY, USA, 11–18. https://doi.org/10.1145/1166324.1166329
[37] Samir Passi and Mihaela Vorvoreanu. 2022. Overreliance on AI: Literature Review.
Technical Report MSR-TR-2022-12. Microsoft. https://www.microsoft.com/en-
us/research/publication/overreliance-on-ai-literature-review/
[38] Michael L Raulin and Anthony M Graziano. 2019. Quasi-experiments and correla-
tional studies. In Companion Encyclopedia of Psychology. Routledge, 1122–1141.
[39] Marc Rettig. 1991. Nobody Reads Documentation. Commun. ACM 34, 7 (jul 1991),
19–24. https://doi.org/10.1145/105783.105788
[40] Ted Sanders. 2023. How to format inputs to CHATGPT models. https://cookbook.
openai.com/examples/how_to_format_inputs_to_chatgpt_models
[41] Advait Sarkar, Andrew D Gordon, Carina Negreanu, Christian Poelitz, Sruti Srini-
vasa Ragavan, and Ben Zorn. 2022. What is it like to program with artificial
intelligence? arXiv preprint arXiv:2208.06213 (2022).
[42] Jessica Shieh. 2023. Best practices for prompt engineering with openai API:
Openai help center. https://help.openai.com/en/articles/6654000-best-practices-
for-prompt-engineering-with-openai-api
[43] Jared Spataro. 2023.
Introducing Microsoft 365 copilot – your copilot for
work. https://blogs.microsoft.com/blog/2023/03/16/introducing-microsoft-365-
copilot-your-copilot-for-work/
[44] Suhridpalsule. 2023. Prompt engineering techniques with Azure Openai - Azure
Openai Service.
https://learn.microsoft.com/en-us/azure/ai-services/openai/
concepts/advanced-prompt-engineering?pivots=programming-language-chat-
completions
[45] Jiao Sun, Q. Vera Liao, Michael Muller, Mayank Agarwal, Stephanie Houde,
Kartik Talamadupula, and Justin D. Weisz. 2022. Investigating Explainability of
Generative AI for Code through Scenario-Based Design. In 27th International
Conference on Intelligent User Interfaces (Helsinki, Finland) (IUI ’22). Association
for Computing Machinery, New York, NY, USA, 212–228.
https://doi.org/10.
1145/3490099.3511119
[46] Gradio Team. 2023. https://www.gradio.app/
[47] Haoye Tian, Weiqi Lu, Tsz On Li, Xunzhu Tang, Shing-Chi Cheung, Jacques
Klein, and Tegawendé F Bissyandé. 2023. Is ChatGPT the Ultimate Programming
Assistant–How far is it? arXiv preprint arXiv:2304.11938 (2023).
IUI ’24, March 18–21, 2024, Greenville, SC, USA
Khurana et al.
[48] Priyan Vaithilingam, Tianyi Zhang, and Elena L. Glassman. 2022. Expectation
vs. Experience: Evaluating the Usability of Code Generation Tools Powered by
Large Language Models. In Extended Abstracts of the 2022 CHI Conference on
Human Factors in Computing Systems (New Orleans, LA, USA) (CHI EA ’22).
Association for Computing Machinery, New York, NY, USA, Article 332, 7 pages.
https://doi.org/10.1145/3491101.3519665
[49] Helena Vasconcelos, Matthew Jörke, Madeleine Grunde-McLaughlin, Tobias
Gerstenberg, Michael S. Bernstein, and Ranjay Krishna. 2023. Explanations
Can Reduce Overreliance on AI Systems During Decision-Making. Proc. ACM
Hum.-Comput. Interact. 7, CSCW1, Article 129 (apr 2023), 38 pages.
https:
//doi.org/10.1145/3579605
[50] Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert,
Ashraf Elnashar, Jesse Spencer-Smith, and Douglas C Schmidt. 2023. A prompt
pattern catalog to enhance prompt engineering with chatgpt. arXiv preprint
arXiv:2302.11382 (2023).
[51] Frank F. Xu, Bogdan Vasilescu, and Graham Neubig. 2022. In-IDE Code Genera-
tion from Natural Language: Promise and Challenges. ACM Trans. Softw. Eng.
Methodol. 31, 2, Article 29 (mar 2022), 47 pages. https://doi.org/10.1145/3487569
[52] J.D. Zamfirescu-Pereira, Richmond Y. Wong, Bjoern Hartmann, and Qian Yang.
2023. Why Johnny Can’t Prompt: How Non-AI Experts Try (and Fail) to Design
LLM Prompts. In Proceedings of the 2023 CHI Conference on Human Factors in
Computing Systems (Hamburg, Germany) (CHI ’23). Association for Computing
Machinery, New York, NY, USA, Article 437, 21 pages. https://doi.org/10.1145/
3544548.3581388
[53] Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang,
Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, et al. 2023. Instruction tuning for
large language models: A survey. arXiv preprint arXiv:2308.10792 (2023).
"
7,8,"On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models","Sarah Gao, Andrew Kean Gao","Since late 2022, Large Language Models (LLMs) have become very prominent with
LLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs
are announced each week, many of which are deposited to Hugging Face, a
repository of machine learning models and datasets. To date, nearly 16,000 Text
Generation models have been uploaded to the site. Given the huge influx of
LLMs, it is of interest to know which LLM backbones, settings, training
methods, and families are popular or trending. However, there is no
comprehensive index of LLMs available. We take advantage of the relatively
systematic nomenclature of Hugging Face LLMs to perform hierarchical clustering
and identify communities amongst LLMs using n-grams and term frequency-inverse
document frequency. Our methods successfully identify families of LLMs and
accurately cluster LLMs into meaningful subgroups. We present a public web
application to navigate and explore Constellation, our atlas of 15,821 LLMs.
Constellation rapidly generates a variety of visualizations, namely
dendrograms, graphs, word clouds, and scatter plots. Constellation is available
at the following link: https://constellation.sites.stanford.edu/.","1
On the Origin of LLMs:
An Evolutionary Tree and Graph for 15,821 Large Language Models
Sarah R Gao, Andrew K Gao
Canyon Crest Academy, Stanford University
2
Abstract
Since late 2022, Large Language Models (LLMs) have become very prominent
with LLMs like ChatGPT and Bard receiving millions of users. Hundreds of new
LLMs are announced each week, many of which are deposited to Hugging Face, a
repository of machine learning models and datasets. To date, nearly 16,000 Text
Generation models have been uploaded to the site. Given the huge influx of
LLMs, it is of interest to know which LLM backbones, settings, training methods,
and families are popular or trending. However, there is no comprehensive index
of LLMs available. We take advantage of the relatively systematic nomenclature
of
Hugging
Face
LLMs
to
perform hierarchical clustering and identify
communities amongst LLMs using n-grams and term frequency-inverse document
frequency. Our methods successfully identify families of LLMs and accurately
cluster LLMs into meaningful subgroups. We present a public web application to
navigate and explore Constellation, our atlas of 15,821 LLMs. Constellation
rapidly generates a variety of visualizations, namely dendrograms, graphs, word
clouds, and scatter plots. Constellation is available at the following link:
https://constellation.sites.stanford.edu/.
The dataset we created will be shared publicly on Github, under @andrewgcodes
(https://github.com/andrewgcodes).
Introduction
Large language models (LLMs) are trained to generate realistic text given a user prompt [1].
Popular LLMs include ChatGPT, Bard, and the LLaMa family of models [2]. In addition to large
companies like OpenAI and Google, smaller research groups and individuals can also train
LLMs and share them through Hugging Face, a popular machine learning repository [3,4]. As of
July 18, 2023 at 12 PM (GMT -5), 15,821 LLMs (or at least, Text Generation models) were
available publicly on Hugging Face. To our knowledge, few attempts have been made to
organize these LLMs, perhaps due to the immense number of models. Inspired by the
bioinformatics technique of using hierarchical clustering on DNA sequences, we apply
hierarchical clustering to the Hugging Face model names, assuming that similar names indicate
similarity [5]. We also construct a graph of LLMs and detect communities using the Louvain
method. Additionally, we generate other visualizations and explore the data.
3
Methods
Libraries
●
BeautifulSoup [6]
●
Pandas [7]
●
Streamlit [8]
●
Scipy [9]
●
Plotly [10]
●
Numpy [11]
●
Scikit-learn [12]
●
Radial Tree [13]
●
NLTK [14]
●
Matplotlib [15]
●
Python-Louvain [16]
●
NetworkX [17]
●
Wordcloud [18]
●
RegEx [19]
Data Collection
Python's BeautifulSoup library was used to retrieve the names, number of likes, and number of
downloads of Hugging Face models labeled with “Text Generation”. Data collected included
model names, Readme links, number of downloads, and the number of likes. Data collection was
performed on July 18, 2023 around 12 PM (US ET; GMT -5). Note that data collection was not
instantaneous. In some instances, we failed to retrieve a number of likes or downloads for a
model.
Parameter Extraction
In addition to the above attributes, model parameters were inferred from the model name using a
regular expression (RegEx) pattern (\d+(\.\d+)?)(B|M|b|m). This pattern matches digit sequences
followed by ""B"", ""M"", ""b"", or ""m"", as model sizes are often included in the name (e.g.,
""falcon-7b""). The number of parameters in millions was recorded in a column named
'params_millions' in the dataset. If parameters couldn't be inferred, the corresponding field was
marked as 'NaN'.
4
Data Analysis and Visualization
We used libraries such as Scipy, Plotly, Numpy, Scikit-learn, Radial Tree, NLTK, and Matplotlib
to analyze the dataset and generate visualizations. A full list of imports is provided at the
beginning of Methods.
Text Feature Extraction:
The model names were converted into a matrix of Term Frequency-Inverse Document Frequency
(TF-IDF) features using Scikit-learn's TfidfVectorizer. The vectorizer was configured to break
down the model names into n-grams ranging from 2 to 8 characters.
Hierarchical Clustering:
Hierarchical clustering with single linkage was performed using the matrix of TF-IDF features.
Cosine distance was used as a similarity measure between the model names. The clustering result
was visualized as an interactive dendrogram using the Plotly library.
Agglomerative Clustering:
In addition to hierarchical clustering, agglomerative clustering was also performed with a
specified number of clusters. The size of each cluster was calculated and visualized as a bar
chart.
Word Clouds
To understand the contents of our agglomerative clusters, we generate Word Clouds the most
common n-grams in each cluster, with more frequent n-grams being presented with larger text
size.
Graph Visualization with Communities
A graph-based visualization was constructed to provide an intuitive understanding of the
relationship and similarity among the models. NetworkX, a Python library for the creation,
manipulation, and study of the structure, dynamics, and functions of complex networks, was
used to generate the graph.
Node Creation:
Each model name was represented as a node in the graph. The graph was initialized as an
undirected graph, and each model name was added as a node using the 'add_node' function. The
model name served as the node label.
5
Edge Creation:
Edges in the graph were used to represent the similarity between pairs of model names. After
calculating the cosine similarity matrix, an edge was added between two nodes (model names) if
their cosine similarity was above a specific threshold (0.2 in this case). The cosine similarity
value was set as the weight of the edge.
Community Detection:
The Louvain method, a popular community detection algorithm, was used to find communities
within the constructed graph. Communities represent groups of models that are more similar to
each other than to models in other groups. The detected communities were used for subsequent
visual enhancements.
Layout Calculation:
The Fruchterman-Reingold force-directed algorithm was employed to calculate the layout of
nodes in the graph. This algorithm arranges the nodes in such a way that all the edges are more
or less equally long and there are as few crossing edges as possible.
Interactive Visualization:
The generated graph was visualized interactively using the Plotly library. Each node represented
a model and was color-coded based on the community it belonged to. Edges between the nodes
indicated similarity, with their thickness corresponding to the cosine similarity score. Hovering
over the nodes displayed more details about the model.
Additional Enhancements:
The centroid (center point) of each community was computed to add a colored background for
each community cluster. The size of the background color patch represented the size of the
community.
Web Application
We built a public web application using the Streamlit framework to generate interactive
dendrograms,
word
clouds,
and
graphs
for
the
data,
which
is
available
here:
https://constellation.sites.stanford.edu/.
Results
There were 15,821 public models labeled with Text Generation on Hugging Face at the time of
data collection. We assembled a final Pandas dataframe containing seven columns: rank,
6
model_name, link, downloads, likes, ReadMeLink, and params_millions. Rank is assigned in
order of number of downloads. For instance, “gpt2” has the most downloads. Note that “gpt2”
does not have an inferred number of parameters because the model name does not contain any
evidence of parameter size. We were able to infer model parameters for 4,560 models (28.8%).
We expect our RegEx expression to result in few false positives. Not all links in ReadMeLink
lead
to
a
valid
Readme file. The links were automatically computed by appending
“/raw/main/README.md” to the model link. All model links should lead to a valid Hugging
Face page.
Figure 1. First five rows of our dataset in order of number of downloads.
We computed a Pearson correlation coefficient of 0.242 between the number of likes and
downloads a model receives. There is a clear positive but weak relationship. It is possible that
this weakness indicates a disparity between model usefulness and popularity. Alternatively,
larger, more powerful models may attract more attention (receiving more likes) but will garner
relatively few downloads because they are too large for most Hugging Face users to use. In
general, models tend to receive far more downloads than likes. This could be because there is no
benefit to the user to like a model on Hugging Face, while downloading the model is beneficial.
We generate a radial dendrogram on all models with over 5,000 downloads to compactly
visualize relationships and families. From the dendrogram, families of LLMs like Wizard,
Pythia, CausalLM, and Bloom can be observed. We suggest using the web application to view
the dendrogram since the large number of leaves makes it difficult to render on a single static
image clearly.
7
Figure 2. Scatter plot showing the relationship between the number of likes and downloads a model receives.
Both axes received a log scale transformation.
​
Figure 3. Radial dendrogram of models with over 5,000 downloads. High resolution image available on Constellation web site.
8
We do not show all the word clouds here due to space, but here are some of the example word
clouds for clusters generated from all models with over 1,000 downloads (clusters = 20). The
word clouds are helpful in understanding which model families are prominent.
Figure 4. Word clouds show clusterization of LLaMa models and code-specific LLMs.
We generate a graph of the models, with similar models receiving an edge. We use the Louvain
method to detect communities.
Figure 5. Graph of models with more than 10,000 downloads, with communities detected using the Louvain method.
We present a publicly available web application (https://constellation.sites.stanford.edu/) to
dynamically explore the data. The web application enables the user to specify the minimum
number of downloads an LLM must have to be considered in the analysis. The web app quickly
generates a dendrogram, word clouds, and graph. Hovering over graph nodes reveals additional
metadata about the model located at that node. The web application also displays useful statistics
and an interactive scatter plot of likes versus downloads. Hovering over points reveals the model
name.
9
Figure 6. Screenshot of the web application.
Conclusion
The increasing number and diversity of Large Language Models (LLMs) necessitate a
comprehensive and systematic approach to organize, classify, and understand these models. In
this study, we have proposed an effective solution by creating Constellation, a user-friendly web
application that visualizes the hierarchical relationships among LLMs, helping to reveal
prominent LLM families and underlying structures.
Our approach is generally inspired by bioinformatics and sequence similarity. It utilizes
hierarchical and agglomerative clustering, combined with an array of techniques such as
dendrograms, word clouds, and graph-based representations. The word clouds provide a
high-level view of prominent model families, while the graph-based visualization depicts the
relationships and similarities among models in a more flexible format than the dendrogram.
The major limitation of our study is that it assumes that LLMs are only similar if they have
similar names. This is not completely true: LLMs can be named anything by the creator who
deposits it to Hugging Face. However, in general, we note that LLMs tend to be named in a
structured, logical fashion. Our results indicate that our assumption that in general similar LLMs
share similar names is sound. We acknowledge that our approach can miss similar LLMs,
especially if one of the LLMs is arbitrarily named. Another limitation is that not all models
labeled “Text Generation” are necessarily LLMs. Finally, a further caveat is that the dendrogram
is not a true “evolutionary” tree. While models in the same low-level cluster are generally
reliably related, this does not hold for higher-level clusters.
By making Constellation publicly available, we hope to encourage more systematic and
informed engagement with LLMs. As the landscape of LLMs continues to evolve rapidly, tools
10
such as Constellation will be instrumental in assisting the researcher and developer communities
in keeping pace with these developments.
References
1.
Gao, A. (2023, July 8). Prompt Engineering for Large Language Models. SSRN; SSRN.
https://doi.org/10.2139/ssrn.4504303
2.
Arancio, J. (2023, April 17). Llama, Alpaca and Vicuna: the new Chatgpt running on your laptop. Medium.
https://medium.com/@jeremyarancio/exploring-llamas-family-models-how-we-achieved-running-llms-on-l
aptops-16bf2539a1bb
3.
Hiter, S. (2023, June 6). What Is a Large Language Model? | Guide to LLMs. EWEEK.
https://www.eweek.com/artificial-intelligence/large-language-model/
4.
Hugging Face. (n.d.). Hugging Face – On a mission to solve NLP, one commit at a time. Huggingface.co.
https://huggingface.co/
5.
Wei, D., Jiang, Q., Wei, Y., & Wang, S. (2012). A novel hierarchical clustering algorithm for gene
sequences. BMC Bioinformatics, 13(1). https://doi.org/10.1186/1471-2105-13-174
6.
Beautiful Soup Documentation. (n.d.). Tedboy.github.io. Retrieved July 19, 2023, from
https://tedboy.github.io/bs4_doc/
7.
pandas documentation — pandas 1.0.1 documentation. (2023, June 28). Pandas.pydata.org.
https://pandas.pydata.org/docs/
8.
Streamlit Docs. (n.d.). Docs.streamlit.io. https://docs.streamlit.io/
9.
scipy. (2020, February 3). scipy/scipy. GitHub. https://github.com/scipy/scipy
10. plotly.py. (2021, September 28). GitHub. https://github.com/plotly/plotly.py
11. numpy/numpy. (2021, October 9). GitHub. https://github.com/numpy/numpy
12. Scikit-Learn. (2019). User guide: contents — scikit-learn 0.22.1 documentation. Scikit-Learn.org.
https://scikit-learn.org/stable/user_guide.html
13. koonimaru. (2023, June 15). radialtree. GitHub. https://github.com/koonimaru/radialtree
14. NLTK. (2009). Natural Language Toolkit — NLTK 3.4.4 documentation. Nltk.org. https://www.nltk.org/
15. Matplotlib. (n.d.). Matplotlib: Python plotting — Matplotlib 3.3.4 documentation. Matplotlib.org.
https://matplotlib.org/stable/index.html
11
16. Aynaud, T. (2023, July 11). Louvain Community Detection. GitHub.
https://github.com/taynaud/python-louvain
17. Hapberg, A., Schult, D., & Swart, P. (2008, August). Exploring network structure, dynamics, and function
using NetworkX. Conference.scipy.org. https://conference.scipy.org/proceedings/SciPy2008/paper_2
18. Mueller, A. (2020, May 7). amueller/word_cloud. GitHub. https://github.com/amueller/word_cloud
19. Ahmad, Z. (2023, July 19). ziishaned/learn-regex. GitHub. https://github.com/ziishaned/learn-regex
Appendix
Word
Occurrences
gpt2
1597
7b
889
13b
770
gpt
756
finetuned
611
llama
475
gptq
393
distilgpt2
383
pythia
381
model
309
wikitext2
297
small
294
base
285
instruct
262
neo
261
opt
252
vicuna
238
4bit
224
bloom
215
v2
214
30b
203
6b
191
12
alpaca
190
125m
182
codeparrot
178
rarity
172
v1
171
falcon
168
8k
167
sft
167
large
166
dialogpt
160
test
157
2
156
all
155
medium
154
lora
153
ds
146
merged
145
superhot
143
j
141
hf
141
fp16
133
chat
129
open
126
concat
126
owt2
126
350m
123
70m
123
chinese
122
128g
121
mpt
118
gpt4
118
3b
116
myawesomeeli5c
lm
108
tiny
106
13
1
106
4
105
8bit
103
headless
101
codegen
97
33b
97
deduped
95
sharded
89
airoboros
88
finetunedgpt2
85
v3
83
wizardlm
83
generator
83
no
82
560m
81
random
79
uncensored
74
finetune
74
xl
72
1b
71
mod
71
27b
69
65b
68
elonmusk
68
bloomz
66
v0
66
bert
65
guanaco
64
ft
64
imdb
63
gptj
62
160m
61
gptneo
61
redpajama
58
neox
57
14
ggml
57
the
55
5
55
dolly
53
12b
53
cut
53
guten
53
67b
52
delta
52
Table 1. Most common words/phrases among all Hugging Face LLMs
"
8,9,Combating Misinformation in the Age of LLMs: Opportunities and Challenges,"Canyu Chen, Kai Shu","Misinformation such as fake news and rumors is a serious threat on
information ecosystems and public trust. The emergence of Large Language Models
(LLMs) has great potential to reshape the landscape of combating
misinformation. Generally, LLMs can be a double-edged sword in the fight. On
the one hand, LLMs bring promising opportunities for combating misinformation
due to their profound world knowledge and strong reasoning abilities. Thus, one
emergent question is: how to utilize LLMs to combat misinformation? On the
other hand, the critical challenge is that LLMs can be easily leveraged to
generate deceptive misinformation at scale. Then, another important question
is: how to combat LLM-generated misinformation? In this paper, we first
systematically review the history of combating misinformation before the advent
of LLMs. Then we illustrate the current efforts and present an outlook for
these two fundamental questions respectively. The goal of this survey paper is
to facilitate the progress of utilizing LLMs for fighting misinformation and
call for interdisciplinary efforts from different stakeholders for combating
LLM-generated misinformation.","Combating Misinformation in the Age of LLMs:
Opportunities and Challenges
Canyu Chen
Illinois Institute of Technology
Chicago IL, USA
cchen151@hawk.iit.edu
Kai Shu
Illinois Institute of Technology
Chicago IL, USA
kshu@iit.edu
Abstract
Misinformation such as fake news and rumors is a serious
threat to information ecosystems and public trust. The emer-
gence of Large Language Models (LLMs) has great potential
to reshape the landscape of combating misinformation. Gen-
erally, LLMs can be a double-edged sword in the fight. On the
one hand, LLMs bring promising opportunities for combating
misinformation due to their profound world knowledge and
strong reasoning abilities. Thus, one emergent question is:
can we utilize LLMs to combat misinformation? On the other
hand, the critical challenge is that LLMs can be easily lever-
aged to generate deceptive misinformation at scale. Then,
another important question is: how to combat LLM-generated
misinformation? In this paper, we first systematically review
the history of combating misinformation before the advent
of LLMs. Then we illustrate the current efforts and present
an outlook for these two fundamental questions respectively.
The goal of this survey paper is to facilitate the progress
of utilizing LLMs for fighting misinformation and call for
interdisciplinary efforts from different stakeholders for com-
bating LLM-generated misinformation1.
1
Introduction
Misinformation has been a longstanding and serious con-
cern in the contemporary digital age [451]. With the pro-
liferation of social media platforms and online news out-
lets, the barriers to generating and sharing content have
significantly diminished, which also expedites the produc-
tion and dissemination of various kinds of misinformation
(e.g., fake news, rumors) and exaggerates its influence at
scale [127, 238, 248, 331, 430, 557, 646, 655]. As the conse-
quence of prevalent misinformation, the public’s belief in
truth and authenticity can be under threat. Thus, there is
a pressing need to combat misinformation to safeguard in-
formation ecosystems and uphold public trust, especially in
high-stakes fields such as healthcare [66] and finance [407].
The advent of LLMs [638] (e.g., ChatGPT, GPT-4 [49]) has
started to make a transformative impact on the landscape of
combating misinformation. In general, LLMs are a double-
edged sword in the fight against misinformation, indicating
that LLMs have brought both emergent opportunities and
1More resources on “LLMs Meet Misinformation” are on the website:
https://llm-misinformation.github.io/
challenges. On the one hand, the profound world knowl-
edge and strong reasoning abilities of LLMs suggest their
potential to revolutionize the conventional paradigms of mis-
information detection, intervention and attribution. In
addition, LLMs can be augmented with external knowledge,
tools, and multimodal information to further enhance their
power and can even operate as autonomous agents [563].
On the other hand, the capacities of LLMs to generate
human-like content, possibly containing hallucinated infor-
mation, and follow humans’ instructions [168] indicate that
LLMs can be easily utilized to generate misinformation in
an unintentional or intentional way. More seriously, recent
research [65] has found that LLM-generated misinformation
can be harder to detect for humans and detectors compared
to human-written misinformation with the same semantics,
implying that the misinformation generated by LLMs can
have more deceptive styles and potentially cause more harm.
In this paper, we first provide a comprehensive and sys-
tematic review of the history of combating misinformation
before the rise of LLMs with a focus on the detection as-
pect in Section 2. Then we delve into both the opportunities
and challenges of combating misinformation in the age of
LLMs. As for the opportunities, we will illustrate “can we
utilize LLMs to combat misinformation?” in Section 3.
We will present the motivation for adopting LLMs in the
fight against misinformation, the current efforts on utiliz-
ing LLMs for combating misinformation, which are mainly
around the detection aspect, and an outlook embracing the
intervention and attribution aspects. As for the challenges,
we will discuss “how to combat LLM-generated misinfor-
mation?” in Section 4. We will dive into the characterization,
emergent threats, and countermeasures of misinformation
generated by LLMs. Looking ahead, we also point out the
potential real-world devastating risks of LLM-generated mis-
information in the near future, which may not be exhibited
yet, and the desired interdisciplinary measures. Through this
survey paper, we aim to facilitate the adoption of LLMs
in combating misinformation and call for collective ef-
forts from stakeholders in different backgrounds to
fight misinformation generated by LLMs.
2
History of Combating Misinformation
In this section, we conduct a systematic and comprehensive
review of the techniques for detecting online misinformation
arXiv:2311.05656v1  [cs.CY]  9 Nov 2023
before the emergence of LLMs to provide an overview of the
history of combating misinformation in terms of the efforts
on detection. Generally, we propose to categorize the detec-
tion methods into seven classes based on real-world scenar-
ios: capturing linguistic features, leveraging neural models,
exploiting social context, incorporating external knowledge,
enhancing generalization ability, minimizing supervision
cost, and fusing multilingual and multimodality.
2.1
Capturing Linguistic Features
Numerous linguistic features have been studied for differ-
entiating misinformation from true information and can be
roughly categorized as stylistic features, complexity features
and psychological features [7, 180]. As for stylistic features,
prior research has found that misleading tweets are usu-
ally longer, use a more limited vocabulary, and have more
negative sentiment [22, 420]. Also, studies have shown that
fake news tends to favor informal, sensational, and affec-
tive language style since it aims to attract readers’ attention
for a short-term financial or political goal [17, 32, 393]. It is
discovered that misleading articles use more swear words,
subjective terms, superlatives, and modal adverbs to exag-
gerate a piece of news [410]. As for complexity features,
misinformation is likely to be linguistically less complex
and more redundant [22] when measured by textual lexi-
cal diversity (MTLD) and type-token ratio (TTR) [330]. The
typical psychological features are based on word counts cor-
related with different psychological processes and basic sen-
timent analyses such as Linguistic Inquiry and Word Count
(LIWC) dictionaries [485], which are shown to be strongly
associated with the possibility of being misleading [323].
Based on the linguistic patterns, multiple detectors are pro-
posed [7, 27, 134, 222, 257, 324, 324, 387]. For example, Mahy-
oob et al. proposed to leverage 16 linguistic attributes, which
include lexical, grammatical and syntactic features, to iden-
tify the nuance between fake and factual news [324].
2.2
Leveraging Neural Models
With the development of deep learning in natural language
processing, more recent works utilize neural models such
as Long Short-Term Memory (LSTM) [177] and Convolu-
tional Neural Network (CNN) [218] for feature extraction
and prediction instead of manually extracting linguistic pat-
terns [9, 76, 252, 318, 320, 327, 401, 498, 505, 558, 561]. For
example, Chen et al. built an attention-residual network
combined with CNN for rumor detection [76]. Vaibhav et al.
designed a graph neural network (GNN) based model to cap-
ture the sentence-level semantic correlation for fake news
detection [498]. Notably, as the burgeoning of pre-trained
language models (PLMs), more advanced neural models such
as Bidirectional Encoder Representations from Transform-
ers (BERT) [104] are also adopted for misinformation detec-
tion [37, 219, 383, 521, 600]. For example, FakeBERT com-
bines BERT and single-layer CNNs with different kernel sizes
and filters as the detector and outperforms conventional ma-
chine learning-based models [219].
2.3
Exploiting Social Context
Considering social media has been one of the major channels
for misinformation production and dissemination, it is essen-
tial to incorporate the social context for effectively detecting
misinformation and protecting the online information space.
Generally, social context can be divided into social engage-
ments and social networks. The social engagements refer to the
users’ interactions with content on social media including
tweeting, retweeting, commenting, clicking, liking, and dis-
liking. It is found that the user-news interactions are different
for fake and authentic news [453]. Thus, a series of works
has explored adopting social engagements as useful auxil-
iary information for detecting misinformation [84, 97, 268,
272, 285, 317, 409, 436, 444, 448, 497, 566, 576, 581, 582, 629].
For example, Shu et al. proposed a sentence-comment co-
attention sub-network to jointly model news content and
users’ comments for fake news detection [448]. Sheng et
al. developed a news environment perception framework
to exploit the user-news environment [444]. Another line
of works aims to leverage the social networks, which en-
compass multiple concepts such as propagation trajectories,
user-user networks, and user-post networks, to enhance
the detection performance. Since the structure of social net-
works can be captured and represented as graphs, a major-
ity of works focus on developing Graph Neural Network
(GNN) based models to detect various kinds of misinforma-
tion [42, 69, 92, 106, 130, 261, 333, 340, 361, 416, 425, 470,
471, 475, 476, 536, 537, 549, 580, 585, 609]. For example, Wu
et al. designed a new graph structure learning approach to
leverage the distinctive degree patterns of misinformation on
social networks [549]. Jeong et al. developed a hypergraph
neural network-based detector to capture the group-level
dissemination patterns [470]. Besides graph neural networks,
there are also some works modeling social context informa-
tion with a mixture marked Hawkes model [356], Markov
random field [360] or dual-propagation model [246].
2.4
Incorporating External Knowledge
There are generally two types of widely used external knowl-
edge embracing knowledge graphs and evidential texts for
assisting misinformation detection. The knowledge graphs
are usually constructed by domain experts and contain a
large number of entities and their relations, which is helpful
for checking the veracity of articles [89, 93, 111, 187, 329,
503, 556]. For example, Hu et al. proposed an end-to-end
graph neural network to compare the document graph with
external knowledge graphs for fake news detection [187].
The evidential texts refer to textual facts that can be used
for examining the authenticity of articles. Multiple works
have investigated evidence-based reasoning strategies for
misinformation detection [10, 67, 116, 138, 154, 163, 190, 215,
2
229, 234, 274, 354, 364, 392, 429, 437, 445, 506, 507, 513, 514,
552, 572, 654]. For example, Jin et al. designed a fine-grained
graph-based reasoning framework to incorporate multiple
groups of external evidence in the detection process [215].
2.5
Enhancing Generalization Ability
In the real world, misinformation can emerge and evolve
quickly, indicating that the distribution of misinformation
data will likely keep changing. Thus, a line of research works
aim to enhance the generalization ability of misinformation
detectors under domain shift [197, 297, 346, 355, 457, 571,
651] and temporal shift [184, 650, 656]. As for domain shift, for
example, Mosallanezhad et al. built a reinforcement learning-
based domain adaptation framework to adapt trained fake
news detectors from source domains to target domains [346].
As for temporal shift, one example is that Hu et al. proposed
to use the forecasted temporal distribution patterns of news
data to guide the misinformation detector [184].
2.6
Minimizing Supervision Cost
Another major challenge for misinformation detection in
practices is the lack of supervision labels due to the hardness
of checking the factuality of articles and the intention to
detect misinformation in the early stage of dissemination.
Previous works have explored various approaches to address
this challenge including data augmentation [169, 452], active
learning [118], prompt based learning [198, 286, 551, 617],
adversarial contrastive learning [284], transfer learning [251]
and meta learning [614]. In particular, multiple works have
studied the problem of early misinformation detection [204,
269, 304, 460, 564, 610, 613, 616]. For example, Huang et
al. designed a social bot-aware graph neural network to
capture bot behaviors for early rumor detection. In addition,
there are some other works exploiting the weak supervision
signals, which can be weak labels, constraints from heuristic
rules, or extrinsic knowledge sources, for misinformation
detection [277, 449, 454, 530, 579].
2.7
Fusing Multilingual and Multimodality
Recently, it has attracted increasing attention to fuse multi-
lingual and multimodal information for misinformation de-
tection. As for multilingual detection, previous research aim
to leverage the high-resource languages to help low-resource
languages [86, 95, 99, 107, 194] or build a universal misinfor-
mation detector across multiple languages [34, 98, 153, 155,
161, 353, 362, 373, 433, 500]. The multimodal detection gen-
erally covers various combinations of different modalities
including text, images, audio, video, networking and tem-
poral information [1, 14, 59, 75, 77, 83, 85, 128, 189, 225, 291,
293, 294, 348, 394, 399, 438, 474, 474, 478, 479, 481, 527, 540,
547, 562, 602, 620, 633, 640, 641, 648] and has multiple modal-
ity fusion strategies including early-fusion, late-fusion and
hybrid-fusion [12]. For example, Sun et al. proposed to model
the cross-modal and content-knowledge inconsistencies in
a unified framework for multimedia misinformation detec-
tion [474]. In particular, combating video misinformation
has gained growing interest due to the proliferation of video-
sharing platforms such as TikTok and YouTube [48, 83, 395].
One example is that Choi et al. integrated comment, title, and
video information with an adversarial learning framework
for misinformation detection on YouTube [83].
3
LLMs for Combating Misinformation
In this section, we aim to illustrate the opportunities of
combating misinformation in the age of LLMs, i.e., can we
utilize LLMs to combat misinformation? First, we will
introduce the motivation of adopting LLMs in the fight. Then,
we will delve into the booming works on leveraging LLMs for
misinformation detection. Finally, we will provide an outlook
on trustworthy misinformation detection with the assistance
of LLMs, utilizing LLMs for misinformation intervention
and attribution, and the adoption of multimodal LLMs, LLM
agents, and human-LLM collaboration in the future.
3.1
Why Adopting LLMs?
Large language models have demonstrated their strong ca-
pacities in various tasks such as machine translation [244],
summarization [622], and complex question answering [482].
With regard to the realm of combating misinformation, the
advent of LLMs has started to revolutionize the previous
paradigms of misinformation detection, intervention, and
attribution. As shown in Figure 1, we summarize the reasons
from three perspectives:
• First, LLMs contain a significant amount of world
knowledge. Since LLMs are usually pre-trained on a large
corpus (e.g., Wikipedia) and have billions of parameters,
they can store much more knowledge than a single knowl-
edge graph, which is shown in previous benchmarks [68,
70, 188, 279, 473, 490, 605, 618] and discussed in related sur-
veys [51, 152, 368, 370, 512]. Thus, LLMs have the potential
to detect factual errors in misleading texts. One example
is shown in Figure 2. Even if “Mercury” and “Aluminum”
are medical terminologies, ChatGPT has an accurate un-
derstanding of these terms, reflecting that LLMs have a
wide range of world knowledge.
• Second, LLMs have strong reasoning abilities, espe-
cially in a zero-shot way. Previous research has shown
that LLMs have powerful capacities in arithmetic rea-
soning, commonsense reasoning, and symbolic reason-
ing [87, 191, 400, 568, 608], and can also decompose the
problem and reason based on rationales with prompts such
as “Let’s think step by step” [231]. Thus, LLMs can
potentially reason based on their intrinsic knowledge to
determine the authenticity of articles. The example in Fig-
ure 2 shows that LLMs such as ChatGPT can reason and
explain why a piece of misinformation is misleading. In
addition, LLMs’ strong zero-shot reasoning ability also
3
Opportunities: LLMs for Combating Misinformation
Challenges: Combating LLM-Generated Misinformation
Fake News
Rumors
Clickbait
Propaganda
Hallucination
Unintentional 
Generation
Intentional 
Generation
Combating Misinformation
Detection
Attribution
Intervention
Intrinsic Abilities
Augmented Abilities
World
Knowledge
Reasoning
Ability
Retrieval
Augmentation
Tool Use
Multimodality
Agent
Figure 1. Opportunities and challenges of combating misinformation in the age of LLMs.
largely solves the challenges of distribution shifts and lack
of supervision labels in the real world.
• Third, LLMs can be augmented with external knowl-
edge, tools, and multimodal information, and can
even operate as autonomous agents. One major limi-
tation of LLMs is that they can potentially generate hal-
lucinations, which refer to the LLM-generated texts con-
taining nonfactual information. One of the main reasons
for hallucinations is that LLMs cannot get access to up-
to-date information and may have insufficient knowledge
in specialized domains such as healthcare [210, 414, 632].
Recent research has shown that LLMs’ hallucinations can
be mitigated with the augmentation of retrieved external
knowledge [296, 298, 352, 439, 625] or tools (e.g., search en-
gines such as Google) to get access to up-to-date informa-
tion [131, 143, 200, 398, 402, 403, 611]. Furthermore, LLMs
can be tuned to reason based on multimodal information
including images, code, tables, audio, and graphs [588, 637],
which indicates LLMs can also be applicable to combating
multimodal misinformation. LLMs have also been shown
to have the capacity to serve as autonomous agents in
various tasks [21, 301, 309, 519, 563, 573], which has great
potential to be used for autonomizing the process of fact-
checking and misinformation detection.
3.2
LLMs for Misinformation Detection
Recently, it has already witnessed increasing efforts explor-
ing how to utilize LLMs for misinformation detection. Ini-
tially, some works have investigated directly prompting GPT-
32 [50, 271], InstructGPT [369], ChatGPT-3.53 [33, 53, 178,
2gpt-3: https://platform.openai.com/docs/models/gpt-3
3gpt-3.5: https://platform.openai.com/docs/models/gpt-3-5
202, 213, 232, 276, 516, 627, 630] and GPT-44 [65, 384, 405]
for misinformation detection. For example, Pan et al. [369]
presented a program-guided fact-checking framework that
leverages the in-context learning ability of LLMs to generate
reasoning programs to guide veracity verification. Chen et
al. [65] have studied ChatGPT-3.5 and GPT-4 with the stan-
dard prompting (“No CoT”) strategy and zero-shot chain-of-
thought (“CoT”) prompting strategy for both human-written
misinformation and LLM-generated misinformation. The ex-
tensive experiments show that the “CoT” strategy mostly
outperforms the “No CoT” strategy. Also, a few recent works
have started to leverage LLMs for detecting multimodal mis-
information. One example is that Wu et al. used GPT-3.5 as
the feature extractor to detect out-of-context images [546].
Besides directly prompting LLMs, Pavlyshenko et al. [381]
adopted the parameter-efficient fine-tuning LoRA [185] on
an open-sourced LLM Llama 2 [488] for multiple tasks in-
cluding fact-checking and fake news detection.
Since the knowledge contained in LLMs may not be up-to-
date or sufficient in detecting factual errors, some works have
explored augmenting LLMs with external knowledge [82] or
tools [81] for misinformation detection. Specifically, Cheung
et al. combined the retrieved knowledge from a search engine
and the reasoning ability of Llama to predict the veracity
of claims [82]. Chern et al. proposed a fact-checking frame-
work integrated with multiple tools (e.g., Google Search,
Google Scholar, code interpreters, Python) to detect the fac-
tual errors of texts generated by LLMs [81]. In addition, some
works studied utilizing LLMs to assist conventional super-
visedly trained detectors via generating weak labels [256],
rationales [183] or instances [6, 193, 456]. For example, Leite
4gpt-4: https://platform.openai.com/docs/models/gpt-4
4
Figure 2. An example of leveraging ChatGPT to detect mis-
information and give explanations.
et al. employed a weakly-supervised learning framework
Snorkel [411] to leverage LLM-generated supervision sig-
nals for training misinformation detectors [256].
3.3
Outlook
In this subsection, we provide an outlook on combating mis-
information in the age of LLMs. First, we can further harness
multilingual and multimodal LLMs to build effective and
trustworthy detectors. Second, although the existing works
mainly focus on the detection of misinformation, LLMs have
great potential to be adopted in misinformation intervention
and attribution. In addition, we will discuss the application
of human-LLM collaboration in combating misinformation.
3.3.1
Trustworthy Misinformation Detection. Though
previous misinformation detectors have achieved relatively
high performance, it is under exploration on how to en-
sure trustworthiness in the detection process including ro-
bustness, explainability, fairness, privacy, and transparency,
which is essential for gaining the public trust [79, 292, 295,
487]. Some previous works have explored the robustness [249,
311, 316, 461, 515] and explainability [126, 226, 267, 313, 448,
576, 589] of misinformation detectors. However, all these
works are based on conventional supervisedly trained de-
tectors, the emergence of LLMs has brought new opportu-
nities for building trustworthy detectors. For example, as
shown in Figure 2, LLMs such as ChatGPT can generate
fluent natural language-based explanations for the given
misinformation while predicting the authenticity, which is
more human-friendly than previous extraction-based expla-
nation methods [448]. The other aspects of trustworthiness
for LLM-based detectors are still under study.
3.3.2
Harnessing Multilingual and Multimodal LLMs.
It has been demonstrated that LLMs can be naturally ex-
tended to multilingual languages [28, 78, 136, 484, 538, 539,
545] and multimodalities [72, 125, 259, 260, 308, 517, 525, 588,
596, 624, 628]. First, multilingual LLMs have shown strong
generalization ability across different languages including
many low-resource ones. For example, one LLM named
Phoenix [78] can generalize to both Latin (e.g., Deutsch)
and non-Latin languages (e.g., Arabic). Thus, multilingual
LLMs can largely alleviate the low-resource challenges in
cross-lingual misinformation detection. Second, recent stud-
ies have demonstrated the impressive multi-sensory skills
of multimodal LLMs [596]. In particular, GPT-4V [588] has
manifested surprising capacities of visual-language under-
standing and reasoning, indicating the profound potential
in combating multimodal misinformation in the real world.
3.3.3
LLMs for Misinformation Intervention. Differ-
ent from misinformation detection methods that mainly focus
on checking the veracity of given texts, misinformation inter-
vention approaches go beyond the pure algorithmic solutions
and aim to exert a direct influence on users [4, 31, 165, 427,
428], which is also a critical component of the lifecycle of
combating misinformation. Generally, there are two lines
of intervention measures. The most standard intervention
measures follow the pipeline of fact-checking and debunk-
ing after humans are already exposed to the misinforma-
tion [62, 216, 382, 382, 509, 510, 603]. The potential usage of
LLMs is to improve the convincingness and persuasive power
of the debunking responses. For example, He et al. [166] pro-
posed to combine reinforcement learning and GPT-2 to gen-
erate polite and factual counter-misinformation responses.
However, one drawback of these post-hoc intervention meth-
ods is that they may cause a psychological “backfiring effect”,
suggesting that humans end up believing more in the orig-
inal misinformation [103, 258, 480]. Thus, another line of
intervention methods aims to leverage inoculation theories
to immunize the public against misinformation [499]. Karin-
shak et al. pointed out the potential of employing LLMs to
generate persuasive anti-misinformation messages (e.g., pro-
vaccination messages) in advance to enhance the public’s
immunity against misinformation [223].
3.3.4
LLMs for Misinformation Attribution. Misinfor-
mation attribution refers to the task of identifying the author
or source of given misinformation, based on the assump-
tion that the texts written by different authors are likely to
have distinct stylometric features and these features will be
preserved in different texts for the same author [493, 494].
Misinformation attribution plays a vital role in combating
misinformation because it can be leveraged to trace the origin
of propaganda or conspiracy theories and hold the publish-
ers accountable. Although there are still no works adopt-
ing LLMs in misinformation attribution, LLMs have already
5
exhibited great power in identifying [378] and manipulat-
ing [314, 377, 415, 421] stylometric features, indicating the
promise for tracing the authorship of misinformation. For
example, Patel et al. [378] performed stylometric analysis on
a large number of texts via prompting GPT-3 and created a
human-interpretable stylometry dataset, which shows that
LLMs have a deep understanding of the stylometric features.
3.3.5
Human-LLM Collaboration. The research in the
realm of human-AI collaboration and teaming aims to lever-
age the strengths of both humans and AI [18, 19, 181]. First,
human guidance helps steer the development of AI to maxi-
mize AI’s benefits to humans and ensure AI will not cause
unintended harm, especially for minority groups. Second,
AI can boost humans’ analytic and decision-making abilities
by providing useful auxiliary information. There are already
some works studying the adoption of human-AI collabora-
tion in combating misinformation [233, 336, 359, 462, 495].
For example, Mendes et al. proposed a human-in-the-loop
evaluation framework for early detection of COVID-19 mis-
information on social media, which combines both modern
NLP methods and experts’ involvement [336]. In the age of
LLMs, we call for more research to leverage the best of both
LLMs and humans in fighting misinformation.
4
Combating LLM-Generated
Misinformation
In this section, we will delve into the emerging challenges
in the age of LLMs. i.e., how to combat LLM-generated
misinformation? First, we will provide a characterization
of misinformation generated by LLMs, which has attracted
increasing attention in recent works [26, 65, 115, 141, 142,
160, 162, 202, 213, 228, 372, 463, 468, 477, 535, 550, 644]. Then,
we will illustrate the new threats brought by LLM-generated
misinformation in different fields, and the countermeasures
against LLM-generated misinformation including alleviating
hallucination, improving LLMs’ safety and detecting LLM-
generated misinformation. Finally, looking ahead, we antici-
pate the misinformation generated by LLMs and other large
generative AI models will cause more devastating real-world
impacts, which may not appear yet. Thus, we will discuss the
potential risks in the near future and the desired measures.
4.1
Characterization
In general, LLM-generated misinformation can be divided
into unintentional generation and intentional generation by
different intents [65]. As shown in Figure 1, the misinforma-
tion generated via unintentional generation methods mainly
refers to hallucinations, i.e., the nonfactual texts generated by
LLMs. Since hallucinations can occur in any generation pro-
cess of LLMs due to the intrinsic properties of auto-regressive
generation and lack of up-to-date information [210, 414, 594,
632], users without malicious intents may also generate non-
factual content when prompting LLMs. An example is shown
LLM-Generated Hallucination
Figure 3. An example of unintentionally prompting ChatGPT
to generate misinformation (i.e., hallucination).
Figure 2: An example of using ChatGPT for detecting misinformation
LLM-Generated Misinformation
Figure 4. An example of intentionally prompting ChatGPT
to generate Misinformation.
in Figure 3. When users adopt prompts such as “please
write a piece of short news”, LLMs (e.g., ChatGPT) will
probably generate content containing hallucinated informa-
tion, in particular the fine-grained information such as dates,
names, addresses, numbers, and quotes, even if the main
message may seem to be correct (e.g., the “Byline”, “Date”
and “City” in Figure 3 are fabricated). The intentional gener-
ation methods suggest that malicious users can knowingly
prompt LLMs to generate various kinds of misinformation
including fake news, rumors, conspiracy theories, clickbait,
misleading claims, or propaganda. One example is shown
in Figure 4. When the users use prompts such as “please
give an example of misinformation ...”, LLMs (e.g.,
ChatGPT) potentially will generate a piece of misinforma-
tion such as “Eating apple seeds can cure all types
of cancer because they contain a magic compound
called amygdalin ...”, though LLMs can also possibly re-
ply with “As an AI language model, I cannot provide
misinformation” owing to the intrinsic safety guard mech-
anisms of LLMs. Notably, recent research [65] has found
that the misinformation generated by LLMs (e.g., ChatGPT)
can be harder to detect for humans and detectors compared
with human-written misinformation with the same seman-
tics, indicating that LLM-generated misinformation can have
6
more deceptive styles and potentially cause more harm. An-
other work [463] also shows that GPT-3 can generate both
accurate information that is easier to understand and misin-
formation that is more compelling.
4.2
Emerging Threats
LLM-generated misinformation has already posed serious
threats in the real world [36, 38, 122, 142, 335, 458, 542, 543].
In this subsection, we will discuss the immediate threats
of the LLM-generated misinformation on a variety of fields
including journalism, healthcare, finance, and politics consid-
ering its characteristics of deceptiveness and easy production.
4.2.1
Journalism. Journalism may be one of the fields
that LLM-generated misinformation has the most substantial
impact on. For example, in April 2023, NewsGuard identified
49 LLM-powered news websites in 7 languages including
English, Chinese, Czech, French, Portuguese, Tagalog, and
Thai [358]. These websites can possibly produce hundreds of
clickbait articles a day to optimize the advertisement revenue,
which causes vast amounts of pollution to online information
ecosystems. Since LLM-generated misinformation can have
more deceptive styles than human-written misinformation
with the same semantics [65], it is challenging for readers,
fact-checkers, and detection algorithms to effectively discern
truth from the misleading information generated by LLMs.
In the long run, as the line between human-written news
and LLM-generated news blurs, the public trust in legitimate
news sources could be undermined and the journalistic ethos
– centered on accuracy, accountability, and transparency –
might be put to the test. Thus, it is imperative for news
outlets to guarantee authenticity and uphold public trust.
4.2.2
Healthcare. Recent works have pointed out the rise
of adoption of LLMs in healthcare applications [167, 221,
266, 299, 357, 423, 426], however, they can also inadvertently
be a tool for the generation and propagation of health mis-
information [96]. For example, it is found that LLMs such
as GPT-3 can be used to generate totally fabricated health
articles that appear remarkably authentic [325]. Compared
with the informatics driven by human-written misinforma-
tion [13, 56, 66, 245, 380, 388], it can be even tougher to
combat LLM-driven informatics for the following reasons.
First, it is hard for unsuspecting users, who may lack the
nuanced understanding of clinical context and medical re-
search, to distinguish LLM-generated hallucinated health
content from authentic medical information. If they rely on
LLMs to seek health advice, it may lead to potential misinter-
pretations and adverse health outcomes. Second, malicious
actors can manipulate LLMs to craft plausible-sounding yet
erroneous medical content, promoting alternative healthcare
treatments or disproven theories for profit. This will not only
undermine the credibility of genuine health information but
also pose significant risks to public health.
4.2.3
Finance. Previous research has shown that human-
written financial misinformation can cause various detri-
mental consequences such as disrupting markets, misleading
investors, and amplifying economic instability [407]. In the
age of LLMs, the financial sector faces an even more escalat-
ing threat from LLM-generated misinformation, because bad
actors, who are potentially motivated by profit, sabotage, or
other malicious intents, can easily leverage LLMs to spread
disinformation campaigns, create counterfeit financial state-
ments, or even impersonate legitimate financial analysis.
Furthermore, considering the prevalence of high-frequency
trading and algorithm-driven investment decisions, even
short-lived misinformation may trigger automated trades
misled by the fabricated content. For example, it is reported
that the stock price of an artificial intelligence company
iFlytek has a deep drop due to a piece of chatbot-generated
misinformation [434]. Thus, stakeholders in the financial
industry should increase their awareness of the potential
threats of LLM-generated misleading content.
4.2.4
Politics. Misinformation has a longstanding grave
impact on the political spheres [2, 8, 135, 144, 159, 217, 328,
345, 374, 390, 391, 466]. The advent of LLMs can potentially
usher in a new age of misinformation and disinformation in
the realm of politics. The reasons can be summarized as the
following two points. The first threat of LLM-generated mis-
information is distorting democracy. LLMs can be easily
weaponized to generate deceptive narratives about candi-
dates, policies, or events at scale. When people are exposed to
such content, their perception of election candidates might
be altered, leading them to vote differently. More seriously,
the flooding of LLM-generated misinformation can possibly
weaken the citizens’ trust in the whole democratic process
and eventually erode the foundations of democratic systems.
The second threat is amplifying polarization. Bad actors
may leverage LLMs to craft personalized misinformation
tailored to individual biases and beliefs, which may resonate
with specific audiences and increase the likelihood of spread-
ing among targeted communities. This can result in exac-
erbating echo chambers and confirmation biases, driving
wedges between different groups and making consensus-
building even harder in the political spheres.
4.3
Countermeasures
In this subsection, we will discuss four major countermea-
sures against LLM-generated misinformation including alle-
viating hallucination of LLMs, improving the safety of LLMs,
detecting LLM-generated misinformation, and public educa-
tion, through which we hope to inspire more future works
on combating LLM-generated misinformation.
4.3.1
Alleviating Hallucination of LLMs. Hallucination
is the main source of unintentional LLM-generated misin-
formation. Recently, increasing works start to design ap-
proaches for evaluating [80, 81, 108, 210, 265, 287, 289, 349,
7
413, 414, 464, 592, 594, 597, 623] or mitigating hallucina-
tion [5, 112, 114, 119, 147, 211, 255, 270, 290, 315, 326, 351,
626, 635]. In general, there are two lines of works on hallu-
cination mitigation. In the training stage, previous research
has explored training data curation or knowledge grounding
methods to incorporate more knowledge [186, 370, 607, 647].
In the inference stage, recent works have investigated meth-
ods including confidence estimation (or uncertainty estima-
tion) [201, 501, 567], knowledge retrieval [120, 214, 264, 306,
338, 508, 526, 601], enabling citations [132, 192], model edt-
ing [275, 337, 424, 593], multi-agent collaboration [90, 110],
prompting [105, 544] and decoding strategy design [88, 253].
4.3.2
Improving Safety of LLMs. The safety guard of
LLMs aims to prevent malicious users exploiting LLMs to
generate harmful content including various types of misin-
formation, which has been emphasized in a variety of survey
papers [15, 44, 52, 63, 64, 73, 91, 101, 102, 113, 117, 151, 171,
196, 224, 237, 281, 305, 347, 435, 446, 522, 529, 548, 577]. A
line of works has evaluated or benchmarked the safety of
various LLMs [203, 205, 288, 386, 412, 511, 523, 524, 569, 595,
621, 636, 649, 653]. Generally, the safety of LLMs can also
be strengthened in both training and inference stage. In the
training stage, previous works focus on designing alignment
training approaches such as reinforcement learning from hu-
man feedback (RLHF) to align LLMs with humans’ values [29,
30, 43, 121, 209, 242, 365, 441, 472, 492, 531, 591, 642, 643]. In
the inference stage, existing research has studied red team-
ing methods to find LLMs’ flaws [58, 129, 250, 332, 334, 385,
447, 465, 598, 604, 612, 652], prompt injection or jailbreak ap-
proaches to probe LLMs’ safety risks [100, 145, 199, 220, 247,
263, 300, 302, 303, 396, 404, 408, 443, 574, 590], and defense
methods for the evolving jailbreaks [170, 172, 236, 417, 541].
4.3.3
Detecting LLM-Generated Misinformation. Mis-
information detection is an important measure for platforms
to prevent its dissemination, which has been discussed in
related surveys [16, 41, 45, 54, 66, 148, 206, 239, 363, 375,
440, 496, 619, 631]. Previously, there are a large number of
works on detecting human-written misinformation includ-
ing fake news [23, 24, 164, 208, 280, 283, 459, 518, 553, 634],
rumor [133, 273, 319, 379], clickbait [74], cherry-picking [25],
and propaganda [94, 322, 327]. Recently, more research fo-
cuses on machine-generated misinformation or neural misin-
formation, suggesting that it is generated by neural models,
such as [3, 7, 39, 109, 128, 162, 249, 406, 450, 615] and its detec-
tion methods [40, 367, 432, 463, 467, 481]. In the age of LLMs,
there start to be some initial works exploring LLM-generated
misinformation detection [65, 81, 115, 142, 160, 213, 372, 532,
550, 644], but more research is strongly desired. It is worth
noting that detecting LLM-generated misinformation holds
a close connection with the techniques in detecting LLM-
generated texts, which can be directly adopted in detecting
LLM-generated misinformation or take effect via notifying
the readers of the potential inauthenticity. The problem of
AI-Generated Fake News
Figure 5. A real-world example of AI-Generated Multimodal
Misinformation.
detecting LLM-generated texts [60, 137, 150, 174, 175, 240,
241, 243, 278, 282, 307, 321, 341, 343, 344, 350, 422, 469, 483,
486, 495, 502, 504, 528, 534, 554, 555, 570, 584, 586, 606] as
well as the watermarking techniques [182, 230, 235, 254, 520,
575, 583, 599, 639] has attracted increasing attention.
4.3.4
Public Education. The goal of public education is
two-fold. First, the general public should be educated about
the capacities and limitations of LLMs, which can include the
understanding that while LLMs can produce coherent and
plausible-sounding texts, the LLM-generated content may
contain nonfactual information. Thus, the public education
can potentially reduce the risk of normal people abusing
LLMs and generating profound hallucinated information
unintentionally. Second, it is imperative to enhance the pub-
lic’s digital literacy and immunity against LLM-generated
misinformation. For example, the characteristics of LLM-
generated misinformation and the identification approaches
should be taught in different communities, especially the mi-
nority groups who have been found to be more susceptible
to misinformation [207, 227, 310, 366, 371].
4.4
Looking Ahead
In this subsection, we will discuss the potential risks of mis-
information generated by LLMs as well as other large gener-
ative AI models in the near future, which may not explicitly
be exhibited yet, including AI-generated multimodal mis-
information, autonomous misinformation agents, cognitive
security and AI-manipulation, as well as the needed inter-
disciplinary countering measures.
4.4.1
AI-Generated Multimodal Misinformation. With
the development of generative AI, we have witnessed an
exponential increase of various tools to create content in
8
multimodalities, which include not only texts but also au-
dio, images, and video. For example, users can create high-
resolution images with close-sourced (e.g., Midjourney [339])
or open-sourced (e.g., Stable Diffusion [419]) text-to-image
generation tools. Also, multimodal LLMs (e.g., GPT-4V [588])
have demonstrated surprisingly strong capacities for visual
understanding and image-to-text generation. In reality, ma-
licious actors can easily combine these tools to craft hyper-
realistic yet entirely fabricated multimodal misinformation,
which may bring more challenges for normal people and
even digital experts. A real-world example of AI-generated
multimodal fake news is shown in Figure 5, which contains
both a piece of misleading text “Reports of an explosion
near the Pentagon in Washington DC” and a synthetic
fake image. We can also see the extent of potential impact
from the number of “views” and “likes”.
4.4.2
Autonomous Misinformation Agents. Recent ad-
vances in LLM agents have shown that LLMs can finish a
wide range of complex tasks automatically which require
multiple human-level abilities including planning, reasoning,
executing, reflecting, and collaborating [71, 179, 309, 519, 533,
559, 563, 645]. In the future, we can envision a society where
humans and agents live together [262]. However, it is also
shown that the current safety guard of LLMs can be easily
broken via fine-tuning [397, 455, 587]. Thus, the bad actors
can possibly create malicious autonomous misinformation
agents and deploy them in online information ecosystems.
The potential danger is that these misinformation agents may
operate without the need for humans’ detailed instructions,
tirelessly generate vast amounts of misleading content, adapt
to conversational contexts in real-time, and adjust their mes-
sages to cater to specific targeted audiences, which will make
a devastating impact on public trust and online safety. It is
worth noting that some recent works also discuss the risks
of LLM-powered bots [123, 578] and agentic systems [61].
To ensure that humans and agents live in harmony in the
future, more multidisciplinary efforts are desired.
4.4.3
Cognitive Security and AI-Manipulation. The
ultimate goal of AI technologies including LLMs should be
to maximize the benefits for humans. However, in the fu-
ture, LLM-generated misinformation could be weaponized
to serve as an emerging type of AI-powered cognitive at-
tacks, which can be defined as the cyber-physical-human
processes that manipulate humans’ behaviors for malicious
purposes by exploiting their cognitive vulnerabilities [195],
which pose serious concerns to humans’ cognitive secu-
rity [149]. Recent evidence has shown that LLMs can be
leveraged to infer the cognitive properties (e.g., personali-
ties) of humans from social media posts [389]. It is possi-
ble that bad actors or LLM-powered autonomous misinfor-
mation agents may exploit humans’ cognitive vulnerabili-
ties to maximize the impact, which is especially concern-
ing for minority communities. Furthermore, LLM-generated
misinformation can also be regarded as a new kind of AI-
manipulation [55, 57, 173, 376] or social media manipu-
lation [11, 565]. It is under-explored how to protect humans
against the negative impact of LLM-generated misinforma-
tion from a cognitive perspective.
4.4.4
Interdisciplinary Countering Efforts. In the long
run, combating LLM-generated misinformation needs efforts
from different disciplines including technology, sociology,
psychology, education, and policymaking. From the tech-
nology perspective, first, the factuality and safety aspects of
LLMs should be further strengthened. Second, more effec-
tive detection methods for LLM-generated misinformation
or texts are strongly needed. From the sociology perspective,
understanding the patterns of the dissemination of LLM-
generated misinformation or the behavior of LLM-powered
misinformation agents can help prevent the spread. From
the psychology perspective, recognizing the cognitive weak-
nesses that make individuals susceptible to misinformation,
which may be exploited by bad actors or LLM agents, can
lead to more effective intervention measures. From the ed-
ucation perspective, courses on digital literacy and critical
thinking can enhance the public’ discernment skills on LLM-
generated misinformation. From the policymaking perspec-
tive, it is pressing to enact regulations to mandate trans-
parency and accountability in the development and deploy-
ment of both close-sourced (e.g., ChatGPT, GPT-4) or open-
sourced (e.g., Llama2 [488], Mistral [212]) LLMs. It is worth
noting that the regulation of LLMs is an important compo-
nent of the overall picture of AI regulation, which is also dis-
cussed in recent works [20, 35, 46, 47, 124, 139, 140, 146, 156–
158, 176, 312, 342, 418, 431, 442, 489, 491, 560]. In addition,
we also need to involve the general public in the fight
against LLM-generated misinformation and foster construc-
tive discussions on the implications of LLM-generated mis-
information on free speech, privacy, and other fundamental
rights. By harnessing the efforts of multiple disciplines and
different stakeholders, we can form a multi-pronged defense
framework to combat LLM-generated misinformation and
safeguard information ecosystems.
5
Conclusion
The advent of LLMs can potentially usher in a new era of
combating misinformation, indicating both emergent oppor-
tunities and challenges. This survey paper first provides a
systematic review of the history of combating misinforma-
tion before the rise of LLMs. Then, we dive into an in-depth
discussion on the existing efforts and future outlook around
two fundamental questions on combating misinformation
in the age of LLMs: can we utilize LLMs to combat misinfor-
mation and how to combat LLM-generated misinformation.
Overall, LLMs have great potential to be adopted in the fight
against misinformation, and more efforts are needed to min-
imize the risks of LLM-generated misinformation.
9
References
[1] Sahar Abdelnabi, Rakibul Hasan, and Mario Fritz. 2022. Open-Domain,
Content-based, Multi-modal Fact-checking of Out-of-Context Images
via Online Resources. In IEEE/CVF Conference on Computer Vision and
Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24,
2022. IEEE, 14920–14929. https://doi.org/10.1109/CVPR52688.2022.
01452
[2] A. Abilov, Yiqing Hua, Hana Matatov, Ofra Amir, and Mor Naa-
man. 2021. VoterFraud2020: a Multi-modal Dataset of Election Fraud
Claims on Twitter. International Conference on Web and Social Media
(2021). https://doi.org/10.1609/icwsm.v15i1.18113
[3] David Ifeoluwa Adelani, Haotian Mai, Fuming Fang, Huy H. Nguyen,
Junichi Yamagishi, and Isao Echizen. 2019. Generating Sentiment-
Preserving Fake Online Reviews Using Neural Language Models and
Their Human- and Machine-based Detection. arXiv preprint arXiv:
1907.09177 (2019).
[4] Zhila Aghajari, Eric P. S. Baumer, and Dominic DiFranzo. 2023. Re-
viewing Interventions to Address Misinformation: The Need to Ex-
pand Our Vision Beyond an Individualistic Focus. Proc. ACM Hum.-
Comput. Interact. 7, CSCW1, Article 87 (2023), 34 pages.
https:
//doi.org/10.1145/3579520
[5] Ayush Agrawal, Lester Mackey, and Adam Tauman Kalai. 2023. Do
Language Models Know When They’re Hallucinating References?
arXiv preprint arXiv: 2305.18248 (2023).
[6] Emil Ahlbäck and Max Dougly. 2023. Can Large Language Models
Enhance Fake News Detection?: Improving Fake News Detection
With Data Augmentation.
[7] Ankit Aich, Souvik Bhattacharya, and Natalie Parde. 2022.
De-
mystifying Neural Fake News via Linguistic Feature-Based Inter-
pretation. In Proceedings of the 29th International Conference on
Computational Linguistics. International Committee on Computa-
tional Linguistics, Gyeongju, Republic of Korea, 6586–6599. https:
//aclanthology.org/2022.coling-1.573
[8] Rachith Aiyappa, Matthew R. DeVerna, Manita Pote, Bao Tran Truong,
Wanying Zhao, David Axelrod, Aria Pessianzadeh, Zoher Kachwala,
Munjung Kim, Ozgur Can Seckin, Minsuk Kim, Sunny Gandhi, Am-
rutha Manikonda, Francesco Pierri, Filippo Menczer, and Kai-Cheng
Yang. 2023. A Multi-Platform Collection of Social Media Posts about
the 2022 U.S. Midterm Elections. arXiv preprint arXiv: 2301.06287
(2023).
[9] Oluwaseun Ajao, Deepayan Bhowmik, and Shahrzad Zargari. 2019.
Sentiment Aware Fake News Detection on Online Social Networks. In
IEEE International Conference on Acoustics, Speech and Signal Process-
ing, ICASSP 2019, Brighton, United Kingdom, May 12-17, 2019. IEEE,
2507–2511. https://doi.org/10.1109/ICASSP.2019.8683170
[10] Mubashara Akhtar, Oana Cocarascu, and Elena Simperl. 2022. Pub-
HealthTab: A Public Health Table-based Dataset for Evidence-based
Fact Checking. In Findings of the Association for Computational Lin-
guistics: NAACL 2022. Association for Computational Linguistics, Seat-
tle, United States, 1–16. https://doi.org/10.18653/v1/2022.findings-
naacl.1
[11] Mohammad Majid Akhtar, Rahat Masood, Muhammad Ikram, and
Salil S. Kanhere. 2023. False Information, Bots and Malicious Cam-
paigns: Demystifying Elements of Social Media Manipulations. arXiv
preprint arXiv: 2308.12497 (2023).
[12] Firoj Alam, Stefano Cresci, Tanmoy Chakraborty, Fabrizio Silvestri,
Dimiter Dimitrov, Giovanni Da San Martino, Shaden Shaar, Hamed
Firooz, and Preslav Nakov. 2022. A Survey on Multimodal Disin-
formation Detection. In Proceedings of the 29th International Con-
ference on Computational Linguistics. International Committee on
Computational Linguistics, Gyeongju, Republic of Korea, 6625–6643.
https://aclanthology.org/2022.coling-1.576
[13] Firoj Alam, Shaden Shaar, Fahim Dalvi, Hassan Sajjad, Alex Nikolov,
Hamdy Mubarak, Giovanni Da San Martino, Ahmed Abdelali, Nadir
Durrani, Kareem Darwish, Abdulaziz Al-Homaid, Wajdi Zaghouani,
Tommaso Caselli, Gijs Danoe, Friso Stolk, Britt Bruntink, and Preslav
Nakov. 2021.
Fighting the COVID-19 Infodemic: Modeling the
Perspective of Journalists, Fact-Checkers, Social Media Platforms,
Policy Makers, and the Society. In Findings of the Association for
Computational Linguistics: EMNLP 2021. Association for Computa-
tional Linguistics, Punta Cana, Dominican Republic, 611–649. https:
//doi.org/10.18653/v1/2021.findings-emnlp.56
[14] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain
Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Mal-
colm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda
Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob
Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sa-
hand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals,
Andrew Zisserman, and Karen Simonyan. 2022. Flamingo: a Visual
Language Model for Few-Shot Learning. DEEPMIND (2022).
[15] Joshua Albrecht, Ellie Kitanidis, and Abraham J. Fetterman. 2022.
Despite ""super-human"" performance, current LLMs are unsuited for
decisions about ethics and safety. arXiv preprint arXiv: 2212.06295
(2022).
[16] Ihsan Ali, Mohamad Nizam Bin Ayub, Palaiahnakote Shivakumara,
and Nurul Fazmidar Binti Mohd Noor. 2022. Fake News Detection
Techniques on Social Media: A Survey. Wireless Communications and
Mobile Computing 2022 (2022).
[17] Hunt Allcott and Matthew Gentzkow. 2017. Social media and fake
news in the 2016 election. Journal of economic perspectives 31, 2 (2017),
211–236.
[18] Saleema Amershi. 2011. Designing for effective end-user interac-
tion with machine learning. In Proceedings of the 24th annual ACM
symposium adjunct on User interface software and technology. 47–50.
[19] Saleema Amershi, Maya Cakmak, William Bradley Knox, and Todd
Kulesza. 2014. Power to the people: The role of humans in interactive
machine learning. Ai Magazine 35, 4 (2014), 105–120.
[20] Markus Anderljung, Joslyn Barnhart, Anton Korinek, Jade Leung,
Cullen O’Keefe, Jess Whittlestone, Shahar Avin, Miles Brundage,
Justin Bullock, Duncan Cass-Beggs, Ben Chang, Tantum Collins,
Tim Fist, Gillian Hadfield, Alan Hayes, Lewis Ho, Sara Hooker, Eric
Horvitz, Noam Kolt, Jonas Schuett, Yonadav Shavit, Divya Siddarth,
Robert Trager, and Kevin Wolf. 2023. Frontier AI Regulation: Manag-
ing Emerging Risks to Public Safety. arXiv preprint arXiv: 2307.03718
(2023).
[21] Jacob Andreas. 2022. Language Models as Agent Models. In Findings
of the Association for Computational Linguistics: EMNLP 2022. Associa-
tion for Computational Linguistics, Abu Dhabi, United Arab Emirates,
5769–5779. https://aclanthology.org/2022.findings-emnlp.423
[22] Dimosthenis Antypas, Jose Camacho-Collados, Alun Preece, and
David Rogers. 2021. COVID-19 and Misinformation: A Large-Scale
Lexical Analysis on Twitter. In Proceedings of the 59th Annual Meeting
of the Association for Computational Linguistics and the 11th Inter-
national Joint Conference on Natural Language Processing: Student
Research Workshop. Association for Computational Linguistics, On-
line, 119–126. https://doi.org/10.18653/v1/2021.acl-srw.13
[23] Antonio A Arechar, Jennifer Allen, Adam J Berinsky, Rocky Cole,
Ziv Epstein, Kiran Garimella, Andrew Gully, Jackson G Lu, Robert M
Ross, Michael N Stagnaro, et al. 2023. Understanding and combatting
misinformation across 16 countries on six continents. Nature human
behaviour (2023), 1–12.
[24] Arnav Arora, Preslav Nakov, Momchil Hardalov, Sheikh Muhammad
Sarwar, Vibha Nayak, Yoan Dinkov, Dimitrina Zlatkova, Kyle Dent,
Ameya Bhatawdekar, Guillaume Bouchard, and Isabelle Augenstein.
2021. Detecting Harmful Content On Online Platforms: What Plat-
forms Need Vs. Where Research Efforts Go. Comput. Surveys (2021).
https://doi.org/10.1145/3603399
10
[25] Abolfazl Asudeh, Hosagrahar Visvesvaraya Jagadish, You Wu, and
Cong Yu. 2020. On detecting cherry-picked trendlines. Proceedings
of the VLDB Endowment 13, 6 (2020), 939–952.
[26] Navid Ayoobi, Sadat Shahriar, and Arjun Mukherjee. 2023. The Loom-
ing Threat of Fake and LLM-generated LinkedIn Profiles: Challenges
and Opportunities for Detection and Prevention. In Proceedings of
the 34th ACM Conference on Hypertext and Social Media. 1–10.
[27] Lucas Azevedo, Mathieu d’Aquin, Brian Davis, and Manel Zarrouk.
2021. LUX (Linguistic aspects Under eXamination): Discourse Analy-
sis for Automatic Fake News Classification. In Findings of the Associa-
tion for Computational Linguistics: ACL-IJCNLP 2021. Association for
Computational Linguistics, Online, 41–56. https://doi.org/10.18653/
v1/2021.findings-acl.4
[28] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng,
Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei
Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu,
Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren,
Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei
Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian
Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan,
Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang,
Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023.
Qwen Technical Report. arXiv preprint arXiv: 2309.16609 (2023).
[29] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen,
Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom
Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom
Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny
Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane
Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown,
Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Ka-
plan. 2022. Training a Helpful and Harmless Assistant with Rein-
forcement Learning from Human Feedback. arXiv preprint arXiv:
Arxiv-2204.05862 (2022).
[30] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, John
Kernion, Andy Jones, A. Chen, Anna Goldie, Azalia Mirhoseini, C.
McKinnon, Carol Chen, Catherine Olsson, C. Olah, Danny Hernandez,
Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, E. Perez,
Jamie Kerr, J. Mueller, Jeff Ladish, J. Landau, Kamal Ndousse, Kamil˙
e
Lukoši¯
ut˙
e, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas
Schiefer, Noem’i Mercado, Nova DasSarma, R. Lasenby, Robin Larson,
Sam Ringer, Scott Johnston, S. Kravec, Sheer El Showk, Stanislav Fort,
Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, T. Henighan,
Tristan Hume, Sam Bowman, Zac Hatfield-Dodds, Benjamin Mann,
Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom B. Brown,
and Jared Kaplan. 2022. Constitutional AI: Harmlessness from AI
Feedback. ARXIV.ORG (2022). https://doi.org/10.48550/arXiv.2212.
08073
[31] Joseph B Bak-Coleman, Ian Kennedy, Morgan Wack, Andrew Beers,
Joseph S Schafer, Emma S Spiro, Kate Starbird, and Jevin D West. 2022.
Combining interventions to reduce the spread of viral misinformation.
Nature Human Behaviour 6, 10 (2022), 1372–1380.
[32] Vian Bakir and Andrew McStay. 2018. Fake news and the economy of
emotions: Problems, causes, solutions. Digital journalism 6, 2 (2018),
154–175.
[33] Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan
Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung,
Quyet V. Do, Yan Xu, and Pascale Fung. 2023. A Multitask, Multi-
lingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallu-
cination, and Interactivity. arXiv preprint arXiv: Arxiv-2302.04023
(2023).
[34] Giorgio Barnabò, Federico Siciliano, Carlos Castillo, Stefano Leonardi,
Preslav Nakov, Giovanni Da San Martino, and Fabrizio Silvestri. 2022.
FbMultiLingMisinfo: Challenging Large-Scale Multilingual Bench-
mark for Misinformation Detection. In 2022 International Joint Con-
ference on Neural Networks (IJCNN). 1–8.
https://doi.org/10.1109/
IJCNN55064.2022.9892739
[35] Anthony M. Barrett, Dan Hendrycks, Jessica Newman, and Brandie
Nonnecke. 2022. Actionable Guidance for High-Consequence AI Risk
Management: Towards Standards Addressing AI Catastrophic Risks.
arXiv preprint arXiv: 2206.08966 (2022).
[36] Clark Barrett, Brad Boyd, Ellie Burzstein, Nicholas Carlini, Brad
Chen, Jihye Choi, Amrita Roy Chowdhury, Mihai Christodorescu,
Anupam Datta, Soheil Feizi, Kathleen Fisher, Tatsunori Hashimoto,
Dan Hendrycks, Somesh Jha, Daniel Kang, Florian Kerschbaum, Eric
Mitchell, John Mitchell, Zulfikar Ramzan, Khawaja Shams, Dawn
Song, Ankur Taly, and Diyi Yang. 2023. Identifying and Mitigating
the Security Risks of Generative AI. arXiv preprint arXiv: 2308.14840
(2023).
[37] Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT: A Pretrained
Language Model for Scientific Text. In Proceedings of the 2019 Con-
ference on Empirical Methods in Natural Language Processing and the
9th International Joint Conference on Natural Language Processing
(EMNLP-IJCNLP). Association for Computational Linguistics, Hong
Kong, China, 3615–3620. https://doi.org/10.18653/v1/D19-1371
[38] Yoshua Bengio, Geoffrey Hinton, Andrew Yao, Dawn Song, Pieter
Abbeel, Yuval Noah Harari, Ya-Qin Zhang, Lan Xue, Shai Shalev-
Shwartz, Gillian Hadfield, Jeff Clune, Tegan Maharaj, Frank Hutter,
Atılım Güneş Baydin, Sheila McIlraith, Qiqi Gao, Ashwin Acharya,
David Krueger, Anca Dragan, Philip Torr, Stuart Russell, Daniel Kah-
neman, Jan Brauner, and Sören Mindermann. 2023. Managing AI
Risks in an Era of Rapid Progress. arXiv preprint arXiv: 2310.17688
(2023).
[39] Pranjal Bhardwaj, Krishna Yadav, Hind Alsharif, and Rania Anwar
Aboalela. 2021. GAN-Based Unsupervised Learning Approach to
Generate and Detect Fake News. In International Conference on Cyber
Security, Privacy and Networking. Springer, 384–396.
[40] Meghana Moorthy Bhat and Srinivasan Parthasarathy. 2020. How
Effectively Can Machines Defend Against Machine-Generated Fake
News? An Empirical Study. In Proceedings of the First Workshop on
Insights from Negative Results in NLP. Association for Computational
Linguistics, Online, 48–53. https://doi.org/10.18653/v1/2020.insights-
1.7
[41] Amrita Bhattacharjee, Kai Shu, Min Gao, and Huan Liu. 2020. Dis-
information in the online information ecosystem: detection, mitiga-
tion and challenges. ArXiv preprint abs/2010.09113 (2020).
https:
//arxiv.org/abs/2010.09113
[42] Tian Bian, Xi Xiao, Tingyang Xu, Peilin Zhao, Wenbing Huang, Yu
Rong, and Junzhou Huang. 2020. Rumor Detection on Social Media
with Bi-Directional Graph Convolutional Networks. In The Thirty-
Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The
Thirty-Second Innovative Applications of Artificial Intelligence Confer-
ence, IAAI 2020, The Tenth AAAI Symposium on Educational Advances
in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12,
2020. AAAI Press, 549–556.
https://aaai.org/ojs/index.php/AAAI/
article/view/5393
[43] Federico Bianchi, Mirac Suzgun, Giuseppe Attanasio, Paul Röttger,
Dan Jurafsky, Tatsunori Hashimoto, and James Zou. 2023. Safety-
Tuned LLaMAs: Lessons From Improving the Safety of Large Lan-
guage Models that Follow Instructions.
arXiv preprint arXiv:
2309.07875 (2023).
[44] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Sim-
ran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg,
Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch,
Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie Chen, Kath-
leen Creel, Jared Quincy Davis, Dora Demszky, Chris Donahue,
Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy,
11
Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gille-
spie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tat-
sunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny
Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky,
Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte
Khani, Omar Khattab, Pang Wei Koh, Mark Krass, Ranjay Krishna,
Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee,
Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma,
Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric Mitchell,
Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan,
Ben Newman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan, Ju-
lian Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, Joon Sung
Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghu-
nathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo
Ruiz, Jack Ryan, Christopher Ré, Dorsa Sadigh, Shiori Sagawa, Keshav
Santhanam, Andy Shih, Krishnan Srinivasan, Alex Tamkin, Rohan
Taori, Armin W. Thomas, Florian Tramèr, Rose E. Wang, William
Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michi-
hiro Yasunaga, Jiaxuan You, Matei Zaharia, Michael Zhang, Tianyi
Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and
Percy Liang. 2021. On the Opportunities and Risks of Foundation
Models. arXiv preprint arXiv: Arxiv-2108.07258 (2021).
[45] Alessandro Bondielli and Francesco Marcelloni. 2019. A survey on
fake news and rumour detection techniques. Information Sciences
497 (2019), 38–55.
[46] Sam Bowman, Jeeyoon Hyun, Ethan Perez, Edwin Chen, Craig Pet-
tit, Scott Heiner, Kamil˙
e Lukoši¯
ut˙
e, Amanda Askell, Andy Jones,
Anna Chen, Anna Goldie, Azalia Mirhoseini, C. McKinnon, C. Olah,
Daniela Amodei, Dario Amodei, Dawn Drain, Dustin Li, Eli Tran-
Johnson, John Kernion, Jamie Kerr, J. Mueller, Jeff Ladish, J. Lan-
dau, Kamal Ndousse, Liane Lovitt, Nelson Elhage, Nicholas Schiefer,
Nicholas Joseph, Noem’i Mercado, Nova DasSarma, Robin Larson,
Sam McCandlish, S. Kundu, Scott Johnston, S. Kravec, Sheer El
Showk, Stanislav Fort, Timothy Telleen-Lawton, Tom B. Brown,
T. Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Ben-
jamin Mann, and Jared Kaplan. 2022. Measuring Progress on Scal-
able Oversight for Large Language Models.
ARXIV.ORG (2022).
https://doi.org/10.48550/arXiv.2211.03540
[47] Danilo Brajovic, Niclas Renner, Vincent Philipp Goebels, Philipp
Wagner, Benjamin Fresz, Martin Biller, Mara Klaeb, Janika Kutz, Jens
Neuhuettler, and Marco F. Huber. 2023. Model Reporting for Certifi-
able AI: A Proposal from Merging EU Regulation into AI Development.
arXiv preprint arXiv: 2307.11525 (2023).
[48] Yuyan Bu, Qiang Sheng, Juan Cao, Peng Qi, Danding Wang, and
Jintao Li. 2023. Online Misinformation Video Detection: A Survey.
arXiv preprint arXiv: Arxiv-2302.03242 (2023).
[49] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes
Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li,
Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro,
and Yi Zhang. 2023. Sparks of Artificial General Intelligence: Early ex-
periments with GPT-4. arXiv preprint arXiv: Arxiv-2303.12712 (2023).
[50] Mars Gokturk Buchholz. 2023. Assessing the Effectiveness of GPT-3
in Detecting False Political Statements: A Case Study on the LIAR
Dataset. arXiv preprint arXiv: 2306.08190 (2023).
[51] Boxi Cao, Hongyu Lin, Xianpei Han, and Le Sun. 2023. The Life Cycle
of Knowledge in Big Language Models: A Survey. arXiv preprint
arXiv: 2303.07616 (2023).
[52] Yihan Cao, Siyu Li, Yixin Liu, Zhiling Yan, Yutong Dai, Philip S. Yu,
and Lichao Sun. 2023. A Comprehensive Survey of AI-Generated
Content (AIGC): A History of Generative AI from GAN to ChatGPT.
arXiv preprint arXiv: Arxiv-2303.04226 (2023).
[53] Kevin Matthe Caramancion. 2023. Harnessing the Power of ChatGPT
to Decimate Mis/Disinformation: Using ChatGPT for Fake News
Detection. In 2023 IEEE World AI IoT Congress (AIIoT). IEEE, 0042–
0046.
[54] Fernando Cardoso Durier da Silva, Rafael Vieira, and Ana Cristina
Garcia. 2019. Can machines learn to detect fake news? a survey
focused on social media. (2019).
[55] Joseph Carlsmith. 2022. Is Power-Seeking AI an Existential Risk?
arXiv preprint arXiv: 2206.13353 (2022).
[56] Richard M Carpiano, Timothy Callaghan, Renee DiResta, Noel T
Brewer, Chelsea Clinton, Alison P Galvani, Rekha Lakshmanan,
Wendy E Parmet, Saad B Omer, Alison M Buttenheim, et al. 2023.
Confronting the evolution and expansion of anti-vaccine activism in
the USA in the COVID-19 era. The Lancet 401, 10380 (2023), 967–970.
[57] Micah Carroll, Alan Chan, Hal Ashton, and David Krueger. 2023.
Characterizing Manipulation from AI Systems. ARXIV.ORG (2023).
https://doi.org/10.48550/arXiv.2303.09387
[58] Stephen Casper, Jason Lin, Joe Kwon, Gatlen Culp, and Dylan
Hadfield-Menell. 2023. Explore, Establish, Exploit: Red Teaming
Language Models from Scratch. arXiv preprint arXiv: 2306.09442
(2023).
[59] Megha Chakraborty, Khusbu Pahwa, Anku Rani, Adarsh Mahor,
Aditya Pakala, Arghya Sarkar, Harshit Dave, Ishan Paul, Janvita
Reddy, Preethi Gurumurthy, Ritvik G, Samahriti Mukherjee, Shreyas
Chatterjee, Kinjal Sensharma, Dwip Dalal, Suryavardan S, Shreyash
Mishra, Parth Patwa, Aman Chadha, Amit Sheth, and Amitava Das.
2023. FACTIFY3M: A Benchmark for Multimodal Fact Verification
with Explainability through 5W Question-Answering. arXiv preprint
arXiv: 2306.05523 (2023).
[60] Souradip Chakraborty, Amrit Singh Bedi, Sicheng Zhu, Bang An,
Dinesh Manocha, and Furong Huang. 2023. On the Possibilities of
AI-Generated Text Detection. arXiv preprint arXiv: Arxiv-2304.04736
(2023).
[61] Alan Chan, Rebecca Salganik, Alva Markelius, Chris Pang, Nitar-
shan Rajkumar, Dmitrii Krasheninnikov, L. Langosco, Zhonghao
He, Yawen Duan, Micah Carroll, Michelle Lin, A. Mayhew, Kather-
ine Collins, Maryam Molamohammadi, John Burden, Wanru Zhao,
Shalaleh Rismani, Konstantinos Voudouris, Umang Bhatt, Adrian
Weller, David Krueger, and Tegan Maharaj. 2023. Harms from In-
creasingly Agentic Algorithmic Systems. ARXIV.ORG (2023). https:
//doi.org/10.48550/arXiv.2302.10329
[62] Man-pui Sally Chan, Christopher R Jones, Kathleen Hall Jamieson,
and Dolores Albarracín. 2017. Debunking: A meta-analysis of the
psychological efficacy of messages countering misinformation. Psy-
chological science 28, 11 (2017), 1531–1546.
[63] Tyler A. Chang and Benjamin K. Bergen. 2023. Language Model
Behavior: A Comprehensive Survey. arXiv preprint arXiv: Arxiv-
2303.11504 (2023).
[64] Chen Chen, Jie Fu, and L. Lyu. 2023. A Pathway Towards Responsible
AI Generated Content. International Joint Conference on Artificial
Intelligence (2023). https://doi.org/10.48550/arXiv.2303.01325
[65] Canyu Chen and Kai Shu. 2023. Can LLM-Generated Misinformation
Be Detected? arXiv preprint arXiv: 2309.13788 (2023).
[66] Canyu Chen, Haoran Wang, Matthew A. Shapiro, Yunyu Xiao, Fei
Wang, and Kai Shu. 2022. Combating Health Misinformation in Social
Media: Characterization, Detection, Intervention, and Open Issues.
ARXIV.ORG (2022). https://doi.org/10.48550/arXiv.2211.05289
[67] Jifan Chen, Aniruddh Sriram, Eunsol Choi, and Greg Durrett. 2022.
Generating Literal and Implied Subquestions to Fact-check Com-
plex Claims. In Proceedings of the 2022 Conference on Empirical
Methods in Natural Language Processing. Association for Compu-
tational Linguistics, Abu Dhabi, United Arab Emirates, 3495–3516.
https://aclanthology.org/2022.emnlp-main.229
[68] Liang Chen, Yang Deng, Yatao Bian, Zeyu Qin, Bingzhe Wu, Tat-Seng
Chua, and Kam-Fai Wong. 2023. Beyond Factuality: A Comprehensive
Evaluation of Large Language Models as Knowledge Generators.
12
arXiv preprint arXiv: 2310.07289 (2023).
[69] Lei Chen, Guanying Li, Zhongyu Wei, Yang Yang, Baohua Zhou, Qi
Zhang, and Xuanjing Huang. 2022. A Progressive Framework for
Role-Aware Rumor Resolution. In Proceedings of the 29th International
Conference on Computational Linguistics. International Committee on
Computational Linguistics, Gyeongju, Republic of Korea, 2748–2758.
https://aclanthology.org/2022.coling-1.242
[70] Shiqi Chen, Yiran Zhao, Jinghan Zhang, I-Chun Chern, Siyang Gao,
Pengfei Liu, and Junxian He. 2023. FELM: Benchmarking Factu-
ality Evaluation of Large Language Models. arXiv preprint arXiv:
2310.00741 (2023).
[71] Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan,
Chen Qian, Chi-Min Chan, Yujia Qin, Yaxi Lu, Ruobing Xie, Zhiyuan
Liu, Maosong Sun, and Jie Zhou. 2023. AgentVerse: Facilitating Multi-
Agent Collaboration and Exploring Emergent Behaviors in Agents.
arXiv preprint arXiv: 2308.10848 (2023).
[72] Xi Chen, Xiao Wang, Lucas Beyer, Alexander Kolesnikov, Jialin Wu,
Paul Voigtlaender, Basil Mustafa, Sebastian Goodman, Ibrahim Al-
abdulmohsin, Piotr Padlewski, Daniel Salz, Xi Xiong, Daniel Vlasic,
Filip Pavetic, Keran Rong, Tianli Yu, Daniel Keysers, Xiaohua Zhai,
and Radu Soricut. 2023. PaLI-3 Vision Language Models: Smaller,
Faster, Stronger. arXiv preprint arXiv: 2310.09199 (2023).
[73] Xiang ’Anthony’ Chen, Jeff Burke, Ruofei Du, Matthew K. Hong,
Jennifer Jacobs, Philippe Laban, Dingzeyu Li, Nanyun Peng, Karl
D. D. Willis, Chien-Sheng Wu, and Bolei Zhou. 2023. Next Steps
for Human-Centered Generative AI: A Technical Perspective. arXiv
preprint arXiv: 2306.15774 (2023).
[74] Yimin Chen, Niall J Conroy, and Victoria L Rubin. 2015. Misleading
online content: recognizing clickbait as"" false news"". In Proceedings of
the 2015 ACM on workshop on multimodal deception detection. 15–19.
[75] Yixuan Chen, Dongsheng Li, Peng Zhang, Jie Sui, Qin Lv, Lu Tun, and
Li Shang. 2022. Cross-Modal Ambiguity Learning for Multimodal
Fake News Detection. In Proceedings of the ACM Web Conference 2022
(Virtual Event, Lyon, France) (WWW ’22). Association for Computing
Machinery, New York, NY, USA, 2897–2905. https://doi.org/10.1145/
3485447.3511968
[76] Yixuan Chen, Jie Sui, Liang Hu, and Wei Gong. 2019. Attention-
Residual Network with CNN for Rumor Detection. In Proceedings of
the 28th ACM International Conference on Information and Knowledge
Management, CIKM 2019, Beijing, China, November 3-7, 2019, Wenwu
Zhu, Dacheng Tao, Xueqi Cheng, Peng Cui, Elke A. Rundensteiner,
David Carmel, Qi He, and Jeffrey Xu Yu (Eds.). ACM, 1121–1130.
https://doi.org/10.1145/3357384.3357950
[77] Ziwei Chen, Linmei Hu, Weixin Li, Yingxia Shao, and Liqiang Nie.
2023. Causal Intervention and Counterfactual Reasoning for Multi-
modal Fake News Detection. In Proceedings of the 61st Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long Pa-
pers). Association for Computational Linguistics, Toronto, Canada,
627–638. https://doi.org/10.18653/v1/2023.acl-long.37
[78] Zhihong Chen, Feng Jiang, Junying Chen, Tiannan Wang, Fei Yu,
Guiming Chen, Hongbo Zhang, Juhao Liang, Chen Zhang, Zhiyi
Zhang, Jianquan Li, Xiang Wan, Benyou Wang, and Haizhou Li. 2023.
Phoenix: Democratizing ChatGPT across Languages. arXiv preprint
arXiv: 2304.10453 (2023).
[79] Lu Cheng, Kush R. Varshney, and Huan Liu. 2021. Socially Responsible
AI Algorithms: Issues, Purposes, and Challenges. J. Artif. Intell. Res.
71 (2021), 1137–1181. https://doi.org/10.1613/jair.1.12814
[80] Qinyuan Cheng, Tianxiang Sun, Wenwei Zhang, Siyin Wang, Xi-
angyang Liu, Mozhi Zhang, Junliang He, Mianqiu Huang, Zhangyue
Yin, Kai Chen, and Xipeng Qiu. 2023. Evaluating Hallucinations in
Chinese Large Language Models. arXiv preprint arXiv: 2310.03368
(2023).
[81] I-Chun Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng,
Chunting Zhou, Junxian He, Graham Neubig, and Pengfei Liu. 2023.
FacTool: Factuality Detection in Generative AI - A Tool Augmented
Framework for Multi-Task and Multi-Domain Scenarios.
arXiv
preprint arXiv: 2307.13528 (2023).
[82] Tsun-Hin Cheung and Kin-Man Lam. 2023. FactLLaMA: Optimizing
Instruction-Following Language Models with External Knowledge for
Automated Fact-Checking. arXiv preprint arXiv: 2309.00240 (2023).
[83] Hyewon Choi and Youngjoong Ko. 2021. Using Topic Modeling and
Adversarial Neural Networks for Fake News Video Detection. In
Proceedings of the 30th ACM International Conference on Information
& Knowledge Management (Virtual Event, Queensland, Australia)
(CIKM ’21). Association for Computing Machinery, New York, NY,
USA, 2950–2954. https://doi.org/10.1145/3459637.3482212
[84] Rajdipa Chowdhury, Sriram Srinivasan, and Lise Getoor. 2020. Joint
Estimation of User And Publisher Credibility for Fake News Detection.
In CIKM ’20: The 29th ACM International Conference on Information
and Knowledge Management, Virtual Event, Ireland, October 19-23,
2020, Mathieu d’Aquin, Stefan Dietze, Claudia Hauff, Edward Curry,
and Philippe Cudré-Mauroux (Eds.). ACM, 1993–1996. https://doi.
org/10.1145/3340531.3412066
[85] Christos Christodoulou, Nikos Salamanos, Pantelitsa Leonidou,
Michail Papadakis, and Michael Sirivianos. 2023. Identifying Misin-
formation on YouTube through Transcript Contextual Analysis with
Transformer Models. arXiv preprint arXiv: 2307.12155 (2023).
[86] Samuel Kai Wah Chu, Runbin Xie, and Yanshu Wang. 2021. Cross-
language fake news detection. Data and Information Management 5,
1 (2021), 100–109.
[87] Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Tao He,
Haotian Wang, Weihua Peng, Ming Liu, Bing Qin, and Ting Liu. 2023.
A Survey of Chain of Thought Reasoning: Advances, Frontiers and
Future. arXiv preprint arXiv: 2309.15402 (2023).
[88] Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James Glass,
and Pengcheng He. 2023. DoLa: Decoding by Contrasting Layers
Improves Factuality in Large Language Models. arXiv preprint arXiv:
2309.03883 (2023).
[89] Giovanni Luca Ciampaglia, Prashant Shiralkar, Luis M Rocha, Johan
Bollen, Filippo Menczer, and Alessandro Flammini. 2015. Computa-
tional fact checking from knowledge networks. PloS one 10, 6 (2015),
e0128193.
[90] Roi Cohen, May Hamri, Mor Geva, and Amir Globerson. 2023. LM vs
LM: Detecting Factual Errors via Cross Examination. arXiv preprint
arXiv: 2305.13281 (2023).
[91] Andrew Critch and Stuart Russell. 2023.
TASRA: a Taxonomy
and Analysis of Societal-Scale Risks from AI. arXiv preprint arXiv:
2306.06924 (2023).
[92] Jian Cui, Kwanwoo Kim, Seung Ho Na, and Seungwon Shin. 2022.
Meta-Path-Based Fake News Detection Leveraging Multi-Level Social
Context Information. In Proceedings of the 31st ACM International
Conference on Information & Knowledge Management (Atlanta, GA,
USA) (CIKM ’22). Association for Computing Machinery, New York,
NY, USA, 325–334. https://doi.org/10.1145/3511808.3557394
[93] Limeng Cui, Haeseung Seo, Maryam Tabar, Fenglong Ma, Suhang
Wang, and Dongwon Lee. 2020. DETERRENT: Knowledge Guided
Graph Attention Network for Detecting Healthcare Misinformation.
In KDD ’20: The 26th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining, Virtual Event, CA, USA, August 23-27, 2020, Rajesh
Gupta, Yan Liu, Jiliang Tang, and B. Aditya Prakash (Eds.). ACM,
492–502. https://dl.acm.org/doi/10.1145/3394486.3403092
[94] Giovanni Da San Martino, Seunghak Yu, Alberto Barrón-Cedeño,
Rostislav Petrov, and Preslav Nakov. 2019. Fine-Grained Analysis
of Propaganda in News Article. In Proceedings of the 2019 Confer-
ence on Empirical Methods in Natural Language Processing and the
9th International Joint Conference on Natural Language Processing
(EMNLP-IJCNLP). Association for Computational Linguistics, Hong
Kong, China, 5636–5646. https://doi.org/10.18653/v1/D19-1565
13
[95] Arkadipta De, Dibyanayan Bandyopadhyay, Baban Gain, and Asif
Ekbal. 2021. A Transformer-Based Approach to Multilingual Fake
News Detection in Low-Resource Languages. ACM Trans. Asian
Low-Resour. Lang. Inf. Process. 21, 1, Article 9 (2021), 20 pages. https:
//doi.org/10.1145/3472619
[96] Luigi De Angelis, Francesco Baglivo, Guglielmo Arzilli, Gaetano Pier-
paolo Privitera, Paolo Ferragina, Alberto Eugenio Tozzi, and Caterina
Rizzo. 2023. ChatGPT and the rise of large language models: the new
AI-driven infodemic threat in public health. Frontiers in Public Health
11 (2023), 1166120.
[97] Marco Del Tredici and Raquel Fernández. 2020. Words are the Win-
dow to the Soul: Language-based User Representations for Fake
News Detection. In Proceedings of the 28th International Conference
on Computational Linguistics. International Committee on Compu-
tational Linguistics, Barcelona, Spain (Online), 5467–5479.
https:
//doi.org/10.18653/v1/2020.coling-main.477
[98] D. Dementieva, Mikhail Kuimov, and A. Panchenko. 2022. Multiverse:
Multilingual Evidence for Fake News Detection. Journal of Imaging
(2022). https://doi.org/10.3390/jimaging9040077
[99] Daryna Dementieva and Alexander Panchenko. 2021. Cross-lingual
Evidence Improves Monolingual Fake News Detection. In Proceed-
ings of the 59th Annual Meeting of the Association for Computational
Linguistics and the 11th International Joint Conference on Natural Lan-
guage Processing: Student Research Workshop. Association for Com-
putational Linguistics, Online, 310–320. https://doi.org/10.18653/v1/
2021.acl-srw.32
[100] Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng
Li, Haoyu Wang, Tianwei Zhang, and Yang Liu. 2023. Jailbreaker:
Automated Jailbreak Across Multiple Large Language Model Chatbots.
arXiv preprint arXiv: 2307.08715 (2023).
[101] Leon Derczynski, Hannah Rose Kirk, Vidhisha Balachandran, Sachin
Kumar, Yulia Tsvetkov, M. R. Leiser, and Saif Mohammad. 2023. As-
sessing Language Model Deployment with Risk Cards. arXiv preprint
arXiv: Arxiv-2303.18190 (2023).
[102] Erik Derner and Kristina Batistič. 2023. Beyond the Safeguards:
Exploring the Security Risks of ChatGPT.
arXiv preprint arXiv:
2305.08005 (2023).
[103] Matthew R. DeVerna, Harry Yaojun Yan, Kai-Cheng Yang, and Filippo
Menczer. 2023. Artificial intelligence is ineffective and potentially
harmful for fact checking. arXiv preprint arXiv: 2308.10800 (2023).
[104] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
2019. BERT: Pre-training of Deep Bidirectional Transformers for
Language Understanding. In Proceedings of the 2019 Conference of the
North American Chapter of the Association for Computational Linguis-
tics: Human Language Technologies, Volume 1 (Long and Short Papers).
Association for Computational Linguistics, Minneapolis, Minnesota,
4171–4186. https://doi.org/10.18653/v1/N19-1423
[105] Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu,
Xian Li, Asli Celikyilmaz, and Jason Weston. 2023.
Chain-of-
Verification Reduces Hallucination in Large Language Models. arXiv
preprint arXiv: 2309.11495 (2023).
[106] Yingtong Dou, Kai Shu, Congying Xia, Philip S. Yu, and Lichao Sun.
2021. User Preference-Aware Fake News Detection. In Proceedings
of the 44th International ACM SIGIR Conference on Research and De-
velopment in Information Retrieval (Virtual Event, Canada) (SIGIR
’21). Association for Computing Machinery, New York, NY, USA,
2051–2055. https://doi.org/10.1145/3404835.3462990
[107] Jiangshu Du, Yingtong Dou, Congying Xia, Limeng Cui, Jing Ma, and
Philip S. Yu. 2021. Cross-lingual COVID-19 Fake News Detection.
International Conference on Data Mining Workshops (ICDMW) (2021).
https://doi.org/10.1109/ICDMW53433.2021.00110
[108] Li Du, Yequan Wang, Xingrun Xing, Yiqun Ya, Xiang Li, Xin Jiang, and
Xuezhi Fang. 2023. Quantifying and Attributing the Hallucination
of Large Language Models via Association Analysis. arXiv preprint
arXiv: 2309.05217 (2023).
[109] Yibing Du, Antoine Bosselut, and Christopher D. Manning. 2022.
Synthetic Disinformation Attacks on Automated Fact Verification
Systems. In Thirty-Sixth AAAI Conference on Artificial Intelligence,
AAAI 2022, Thirty-Fourth Conference on Innovative Applications of
Artificial Intelligence, IAAI 2022, The Twelveth Symposium on Educa-
tional Advances in Artificial Intelligence, EAAI 2022 Virtual Event,
February 22 - March 1, 2022. AAAI Press, 10581–10589.
https:
//ojs.aaai.org/index.php/AAAI/article/view/21302
[110] Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, and
Igor Mordatch. 2023. Improving Factuality and Reasoning in Lan-
guage Models through Multiagent Debate. arXiv preprint arXiv:
2305.14325 (2023).
[111] Yaqian Dun, Kefei Tu, Chen Chen, Chunyan Hou, and Xiaojie Yuan.
2021. KAN: Knowledge-aware Attention Network for Fake News
Detection. In Thirty-Fifth AAAI Conference on Artificial Intelligence,
AAAI 2021, Thirty-Third Conference on Innovative Applications of Arti-
ficial Intelligence, IAAI 2021, The Eleventh Symposium on Educational
Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February
2-9, 2021. AAAI Press, 81–89. https://ojs.aaai.org/index.php/AAAI/
article/view/16080
[112] Nouha Dziri, Sivan Milton, Mo Yu, Osmar Zaiane, and Siva Reddy.
2022. On the Origin of Hallucinations in Conversational Models:
Is it the Datasets or the Models?. In Proceedings of the 2022 Con-
ference of the North American Chapter of the Association for Com-
putational Linguistics: Human Language Technologies. Association
for Computational Linguistics, Seattle, United States, 5271–5285.
https://doi.org/10.18653/v1/2022.naacl-main.387
[113] El-Mahdi El-Mhamdi, Sadegh Farhadkhani, R. Guerraoui, Nirupam
Gupta, L. Hoang, Rafael Pinot, and John Stephan. 2022. On the
Impossible Safety of Large AI Models. ARXIV.ORG (2022).
https:
//doi.org/10.48550/arXiv.2209.15259
[114] Mohamed Elaraby, Mengyin Lu, Jacob Dunn, Xueying Zhang, Yu
Wang, and Shizhu Liu. 2023. Halo: Estimation and Reduction of
Hallucinations in Open-Source Weak Large Language Models. arXiv
preprint arXiv: 2308.11764 (2023).
[115] Ziv Epstein, Antonio Alonso Arechar, and David Rand. 2023. What
label should be applied to content produced by generative AI? (2023).
[116] Martin Fajcik, P. Motlícek, and P. Smrz. 2022. Claim-Dissector: An
Interpretable Fact-Checking System with Joint Re-ranking and Verac-
ity Prediction. Annual Meeting of the Association for Computational
Linguistics (2022). https://doi.org/10.48550/arXiv.2207.14116
[117] Mingyuan Fan, Cen Chen, Chengyu Wang, and Jun Huang. 2023.
On the Trustworthiness Landscape of State-of-the-art Generative
Models: A Comprehensive Survey. arXiv preprint arXiv: 2307.16680
(2023).
[118] Parsa Farinneya, Mohammad Mahdi Abdollah Pour, Sardar Hamidian,
and Mona Diab. 2021. Active Learning for Rumor Identification
on Social Media. In Findings of the Association for Computational
Linguistics: EMNLP 2021. Association for Computational Linguistics,
Punta Cana, Dominican Republic, 4556–4565.
https://doi.org/10.
18653/v1/2021.findings-emnlp.387
[119] Philip Feldman, James R. Foulds, and Shimei Pan. 2023. Trapping
LLM Hallucinations Using Tagged Context Prompts. arXiv preprint
arXiv: 2306.06085 (2023).
[120] Zhangyin Feng, Xiaocheng Feng, Dezhi Zhao, Maojin Yang, and Bing
Qin. 2023. Retrieval-Generation Synergy Augmented Large Language
Models. arXiv preprint arXiv: 2310.05149 (2023).
[121] Patrick Fernandes, Aman Madaan, Emmy Liu, António Farinhas, Pe-
dro Henrique Martins, Amanda Bertsch, José G. C. de Souza, Shuyan
Zhou, Tongshuang Sherry Wu, Graham Neubig, and André F. T.
Martins. 2023.
Bridging the Gap: A Survey on Integrating (Hu-
man) Feedback for Natural Language Generation. ARXIV.ORG (2023).
https://doi.org/10.48550/arXiv.2305.00955
14
[122] Emilio Ferrara. 2023. GenAI Against Humanity: Nefarious Applica-
tions of Generative Artificial Intelligence and Large Language Models.
arXiv preprint arXiv: 2310.00737 (2023).
[123] Emilio Ferrara. 2023. Social bot detection in the age of ChatGPT:
Challenges and opportunities. First Monday (2023).
[124] Anja Folberth, Jutta Jahnel, Jascha Bareis, Carsten Orwat, and Chris-
tian Wadephul. 2022. Tackling problems, harvesting benefits - A
systematic review of the regulatory debate around AI. arXiv preprint
arXiv: 2209.05468 (2022).
[125] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan
Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng,
Ke Li, Xing Sun, and Rongrong Ji. 2023. MME: A Comprehensive
Evaluation Benchmark for Multimodal Large Language Models. arXiv
preprint arXiv: 2306.13394 (2023).
[126] Dongqi Fu, Yikun Ban, Hanghang Tong, Ross Maciejewski, and Jingrui
He. 2022. DISCO: Comprehensive and Explainable Disinformation
Detection. In Proceedings of the 31st ACM International Conference
on Information & Knowledge Management (Atlanta, GA, USA) (CIKM
’22). Association for Computing Machinery, New York, NY, USA,
4848–4852. https://doi.org/10.1145/3511808.3557202
[127] Yi Fung, Kung-Hsiang Huang, Preslav Nakov, and Heng Ji. 2022. The
Battlefront of Combating Misinformation and Coping with Media
Bias. In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of
the Association for Computational Linguistics and the 12th International
Joint Conference on Natural Language Processing: Tutorial Abstracts.
Association for Computational Linguistics, Taipei, 28–34.
https:
//aclanthology.org/2022.aacl-tutorials.5
[128] Yi Fung, Christopher Thomas, Revanth Gangi Reddy, Sandeep
Polisetty, Heng Ji, Shih-Fu Chang, Kathleen McKeown, Mohit Bansal,
and Avi Sil. 2021. InfoSurgeon: Cross-Media Fine-grained Informa-
tion Consistency Checking for Fake News Detection. In Proceedings of
the 59th Annual Meeting of the Association for Computational Linguis-
tics and the 11th International Joint Conference on Natural Language
Processing (Volume 1: Long Papers). Association for Computational
Linguistics, Online, 1683–1698. https://doi.org/10.18653/v1/2021.acl-
long.133
[129] Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao
Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer,
Kamal Ndousse, Andy Jones, Sam Bowman, Anna Chen, Tom Con-
erly, Nova DasSarma, Dawn Drain, Nelson Elhage, Sheer El-Showk,
Stanislav Fort, Zac Hatfield-Dodds, Tom Henighan, Danny Hernan-
dez, Tristan Hume, Josh Jacobson, Scott Johnston, Shauna Kravec,
Catherine Olsson, Sam Ringer, Eli Tran-Johnson, Dario Amodei, Tom
Brown, Nicholas Joseph, Sam McCandlish, Chris Olah, Jared Kaplan,
and Jack Clark. 2022. Red Teaming Language Models to Reduce
Harms: Methods, Scaling Behaviors, and Lessons Learned. arXiv
preprint arXiv: 2209.07858 (2022).
[130] Li Gao, Lingyun Song, Jie Liu, Bolin Chen, and Xuequn Shang. 2022.
Topology Imbalance and Relation Inauthenticity Aware Hierarchical
Graph Attention Networks for Fake News Detection. In Proceedings
of the 29th International Conference on Computational Linguistics.
International Committee on Computational Linguistics, Gyeongju,
Republic of Korea, 4687–4696. https://aclanthology.org/2022.coling-
1.415
[131] Shen Gao, Zhengliang Shi, Minghang Zhu, Bowen Fang, Xin Xin,
Pengjie Ren, Zhumin Chen, and Jun Ma. 2023. Confucius: Iterative
Tool Learning from Introspection Feedback by Easy-to-Difficult Cur-
riculum. arXiv preprint arXiv: 2308.14034 (2023).
[132] Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. 2023. En-
abling Large Language Models to Generate Text with Citations. arXiv
preprint arXiv: 2305.14627 (2023).
[133] Yuan Gao, Xiang Wang, Xiangnan He, Huamin Feng, and Yongdong
Zhang. 2022. Rumor Detection with Self-supervised Learning on
Texts and Social Graph. Frontiers Comput. Sci. (2022). https://doi.org/
10.48550/arXiv.2204.08838
[134] Sonal Garg and Dilip Kumar Sharma. 2022. Linguistic features based
framework for automatic fake news detection. Computers & Industrial
Engineering 172 (2022), 108432.
[135] Valerio La Gatta, Chiyu Wei, Luca Luceri, Francesco Pierri, and Emilio
Ferrara. 2023. Retrieving false claims on Twitter during the Russia-
Ukraine conflict. arXiv preprint arXiv: 2303.10121 (2023).
[136] Gregor Geigle, Abhay Jain, Radu Timofte, and Goran Glavaš. 2023.
mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs. arXiv
preprint arXiv: 2307.06930 (2023).
[137] Soumya Suvra Ghosal, Souradip Chakraborty, Jonas Geiping, Furong
Huang, Dinesh Manocha, and Amrit Singh Bedi. 2023. Towards Pos-
sibilities & Impossibilities of AI-generated Text Detection: A Survey.
arXiv preprint arXiv:2310.15264 (2023).
[138] Max Glockner, Yufang Hou, and Iryna Gurevych. 2022. Missing
Counter-Evidence Renders NLP Fact-Checking Unrealistic for Mis-
information. In Proceedings of the 2022 Conference on Empirical
Methods in Natural Language Processing. Association for Compu-
tational Linguistics, Abu Dhabi, United Arab Emirates, 5916–5936.
https://aclanthology.org/2022.emnlp-main.397
[139] David Glukhov, Ilia Shumailov, Yarin Gal, Nicolas Papernot, and Var-
dan Papyan. 2023. LLM Censorship: A Machine Learning Challenge
or a Computer Security Problem? arXiv preprint arXiv: 2307.10719
(2023).
[140] Catalina Goanta, Nikolaos Aletras, Ilias Chalkidis, Sofia Ranchor-
das, and Gerasimos Spanakis. 2023. Regulation and NLP (RegNLP):
Taming Large Language Models. arXiv preprint arXiv: 2310.05553
(2023).
[141] Josh A Goldstein, Jason Chao, Shelby Grossman, Alex Stamos, and
Michael Tomz. 2023. Can AI Write Persuasive Propaganda? (2023).
[142] Josh A. Goldstein, Girish Sastry, Micah Musser, Renee DiResta,
Matthew Gentzel, and Katerina Sedova. 2023. Generative Language
Models and Automated Influence Operations: Emerging Threats and
Potential Mitigations. ARXIV.ORG (2023). https://doi.org/10.48550/
arXiv.2301.04246
[143] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang,
Minlie Huang, Nan Duan, and Weizhu Chen. 2023. ToRA: A Tool-
Integrated Reasoning Agent for Mathematical Problem Solving. arXiv
preprint arXiv: 2309.17452 (2023).
[144] Jon Green, William Hobbs, Stefan McCabe, and David Lazer. 2022.
Online engagement with 2020 election misinformation and turnout in
the 2021 Georgia runoff election. Proceedings of the National Academy
of Sciences 119, 34 (2022), e2115900119.
[145] Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, C. Endres, Thorsten
Holz, and Mario Fritz. 2023. More than you’ve asked for: A Compre-
hensive Analysis of Novel Prompt Injection Threats to Application-
Integrated Large Language Models.
ARXIV.ORG (2023).
https:
//doi.org/10.48550/arXiv.2302.12173
[146] Ross Gruetzemacher, Alan Chan, Kevin Frazier, Christy Manning,
Štěpán Los, James Fox, José Hernández-Orallo, John Burden, Matija
Franklin, Clíodhna Ní Ghuidhir, Mark Bailey, Daniel Eth, Toby
Pilditch, and Kyle Kilian. 2023. An International Consortium for
Evaluations of Societal-Scale Risks from Advanced AI. arXiv preprint
arXiv: 2310.14455 (2023).
[147] Nuno M. Guerreiro, Duarte Alves, Jonas Waldendorf, Barry Haddow,
Alexandra Birch, Pierre Colombo, and André F. T. Martins. 2023. Hal-
lucinations in Large Multilingual Translation Models. arXiv preprint
arXiv: Arxiv-2303.16104 (2023).
[148] Andrew M Guess and Benjamin A Lyons. 2020. Misinformation,
disinformation, and online propaganda. Social media and democracy:
The state of the field, prospects for reform 10 (2020).
[149] Bin Guo, Yasan Ding, Yueheng Sun, Shuai Ma, and Ke Li. 2019. The
Mass, Fake News, and Cognition Security. Frontiers of Computer
Science (2019). https://doi.org/10.1007/s11704-020-9256-0
15
[150] Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yux-
uan Ding, Jianwei Yue, and Yupeng Wu. 2023. How Close is ChatGPT
to Human Experts? Comparison Corpus, Evaluation, and Detection.
arXiv preprint arXiv: Arxiv-2301.07597 (2023).
[151] Danhuai Guo, Huixuan Chen, Ruoling Wu, and Yangang Wang. 2023.
AIGC Challenges and Opportunities Related to Public Safety: A Case
Study of ChatGPT. Journal of Safety Science and Resilience (2023).
[152] Zishan Guo, Renren Jin, Chuang Liu, Yufei Huang, Dan Shi, Supryadi,
Linhao Yu, Yan Liu, Jiaxuan Li, Bojian Xiong, and Deyi Xiong. 2023.
Evaluating Large Language Models: A Comprehensive Survey. arXiv
preprint arXiv: 2310.19736 (2023).
[153] Ashim Gupta and Vivek Srikumar. 2021. X-Fact: A New Benchmark
Dataset for Multilingual Fact Checking. In Proceedings of the 59th
Annual Meeting of the Association for Computational Linguistics and
the 11th International Joint Conference on Natural Language Processing
(Volume 2: Short Papers). Association for Computational Linguistics,
Online, 675–682. https://doi.org/10.18653/v1/2021.acl-short.86
[154] Prakhar Gupta, Chien-Sheng Wu, Wenhao Liu, and Caiming Xiong.
2022. DialFact: A Benchmark for Fact-Checking in Dialogue. In
Proceedings of the 60th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers). Association for Compu-
tational Linguistics, Dublin, Ireland, 3785–3801. https://doi.org/10.
18653/v1/2022.acl-long.263
[155] Vipin Gupta, Rina Kumari, Nischal Ashok, Tirthankar Ghosal, and
Asif Ekbal. 2022. MMM: An Emotion and Novelty-aware Approach
for Multilingual Multimodal Misinformation Detection. In Findings
of the Association for Computational Linguistics: AACL-IJCNLP 2022.
Association for Computational Linguistics, Online only, 464–477.
https://aclanthology.org/2022.findings-aacl.43
[156] P. Hacker. 2023. Sustainable AI Regulation. Social Science Research
Network (2023). https://doi.org/10.48550/arXiv.2306.00292
[157] Philipp Hacker, Andreas Engel, and Marco Mauer. 2023. Regulating
ChatGPT and other Large Generative AI Models. arXiv preprint arXiv:
Arxiv-2302.02337 (2023).
[158] Gillian K. Hadfield and Jack Clark. 2023. Regulatory Markets: The
Future of AI Governance. arXiv preprint arXiv: 2304.04914 (2023).
[159] Samar Haider, Luca Luceri, A. Deb, Adam Badawy, Nanyun Peng,
and Emilio Ferrara. 2020. Detecting Social Media Manipulation in
Low-Resource Languages. The Web Conference (2020). https://doi.
org/10.1145/3543873.3587615
[160] Ahmed Abdeen Hamed. 2023. Improving Detection of ChatGPT-
Generated Fake Science Using Real Publication Text: Introducing
xFakeBibs a Supervised Learning Network Algorithm. (2023).
[161] Hicham Hammouchi and Mounir Ghogho. 2022. Evidence-Aware
Multilingual Fake News Detection. IEEE Access 10 (2022), 116808–
116818. https://doi.org/10.1109/ACCESS.2022.3220690
[162] Hans W. A. Hanley and Zakir Durumeric. 2023. Machine-Made
Media: Monitoring the Mobilization of Machine-Generated Articles
on Misinformation and Mainstream News Websites. arXiv preprint
arXiv: 2305.09820 (2023).
[163] Fatima Haouari. 2022. Evidence-Based Early Rumor Verification
in Social Media. In European Conference on Information Retrieval.
Springer, 496–504.
[164] Momchil Hardalov, Arnav Arora, Preslav Nakov, and Isabelle Augen-
stein. 2022. A Survey on Stance Detection for Mis- and Disinformation
Identification. In Findings of the Association for Computational Linguis-
tics: NAACL 2022. Association for Computational Linguistics, Seattle,
United States, 1259–1277. https://doi.org/10.18653/v1/2022.findings-
naacl.94
[165] Katrin Hartwig, Frederic Doell, and Christian Reuter. 2023. The Land-
scape of User-centered Misinformation Interventions - A Systematic
Literature Review. arXiv preprint arXiv: Arxiv-2301.06517 (2023).
[166] Bing He, Mustaque Ahamad, and Srijan Kumar. 2023. Reinforcement
Learning-Based Counter-Misinformation Response Generation: A
Case Study of COVID-19 Vaccine Misinformation. In Proceedings of
the ACM Web Conference 2023 (Austin, TX, USA) (WWW ’23). Asso-
ciation for Computing Machinery, New York, NY, USA, 2698–2709.
https://doi.org/10.1145/3543507.3583388
[167] Kai He, Rui Mao, Qika Lin, Yucheng Ruan, Xiang Lan, Mengling Feng,
and Erik Cambria. 2023. A Survey of Large Language Models for
Healthcare: from Data, Technology, and Applications to Accountabil-
ity and Ethics. arXiv preprint arXiv: 2310.05694 (2023).
[168] Qianyu He, Jie Zeng, Wenhao Huang, Lina Chen, Jin Xiao, Qianxi He,
Xunzhe Zhou, Lida Chen, Xintao Wang, Yuncheng Huang, Haoning
Ye, Zihan Li, Shisong Chen, Yikai Zhang, Zhouhong Gu, Jiaqing Liang,
and Yanghua Xiao. 2023. Can Large Language Models Understand
Real-World Complex Instructions? arXiv preprint arXiv: 2309.09150
(2023).
[169] Zhenyu He, Ce Li, Fan Zhou, and Yi Yang. 2021. Rumor Detection on
Social Media with Event Augmentations. In Proceedings of the 44th
International ACM SIGIR Conference on Research and Development in
Information Retrieval (Virtual Event, Canada) (SIGIR ’21). Association
for Computing Machinery, New York, NY, USA, 2020–2024. https:
//doi.org/10.1145/3404835.3463001
[170] Alec Helbling, Mansi Phute, Matthew Hull, and Duen Horng Chau.
2023. LLM Self Defense: By Self Examination, LLMs Know They Are
Being Tricked. arXiv preprint arXiv: 2308.07308 (2023).
[171] Peter Henderson, Xuechen Li, Dan Jurafsky, Tatsunori Hashimoto,
Mark A. Lemley, and Percy Liang. 2023. Foundation Models and Fair
Use. arXiv preprint arXiv: Arxiv-2303.15715 (2023).
[172] Peter Henderson, E. Mitchell, Christopher D. Manning, Dan Jurafsky,
and Chelsea Finn. 2022. Self-Destructing Models: Increasing the
Costs of Harmful Dual Uses of Foundation Models. Proceedings
of the 2023 AAAI/ACM Conference on AI, Ethics, and Society (2022).
https://doi.org/10.1145/3600211.3604690
[173] Dan Hendrycks, Mantas Mazeika, and Thomas Woodside. 2023. An
Overview of Catastrophic AI Risks. arXiv preprint arXiv: 2306.12001
(2023).
[174] Da Silva Gameiro Henrique, Andrei Kucharavy, and Rachid Guerraoui.
2023. Stochastic Parrots Looking for Stochastic Parrots: LLMs are
Easy to Fine-Tune and Hard to Detect with other LLMs. arXiv preprint
arXiv: Arxiv-2304.08968 (2023).
[175] Steffen Herbold, Annette Hautli-Janisz, Ute Heuer, Zlata Kikteva, and
Alexander Trautsch. 2023. AI, write an essay for me: A large-scale
comparison of human-written versus ChatGPT-generated essays.
arXiv preprint arXiv: Arxiv-2304.14276 (2023).
[176] Lewis Ho, Joslyn Barnhart, Robert Trager, Yoshua Bengio, Miles
Brundage, Allison Carnegie, Rumman Chowdhury, Allan Dafoe,
Gillian Hadfield, Margaret Levi, and Duncan Snidal. 2023. Interna-
tional Institutions for Advanced AI. arXiv preprint arXiv: 2307.04699
(2023).
[177] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term
memory. Neural computation 9, 8 (1997), 1735–1780.
[178] Emma Hoes, Sacha Altay, and Juan Bermeo. 2023. Leveraging Chat-
GPT for Efficient Fact-Checking. (2023).
[179] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin
Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin,
Liyang Zhou, Chenyu Ran, Lingfeng Xiao, and Chenglin Wu. 2023.
MetaGPT: Meta Programming for Multi-Agent Collaborative Frame-
work. arXiv preprint arXiv: 2308.00352 (2023).
[180] Benjamin D. Horne and Sibel Adali. 2017. This Just In: Fake News
Packs a Lot in Title, Uses Simpler, Repetitive Content in Text Body,
More Similar to Satire than Real News. arXiv:1703.09398 [cs.SI]
[181] Eric Horvitz. 1999. Principles of mixed-initiative user interfaces. In
Proceedings of the SIGCHI conference on Human Factors in Computing
Systems. 159–166.
[182] Abe Bohan Hou, Jingyu Zhang, Tianxing He, Yichen Wang, Yung-
Sung Chuang, Hongwei Wang, Lingfeng Shen, Benjamin Van Durme,
16
Daniel Khashabi, and Yulia Tsvetkov. 2023. SemStamp: A Semantic
Watermark with Paraphrastic Robustness for Text Generation. arXiv
preprint arXiv: 2310.03991 (2023).
[183] Beizhe Hu, Qiang Sheng, Juan Cao, Yuhui Shi, Yang Li, Danding Wang,
and Peng Qi. 2023. Bad Actor, Good Advisor: Exploring the Role of
Large Language Models in Fake News Detection. arXiv preprint arXiv:
2309.12247 (2023).
[184] Beizhe Hu, Qiang Sheng, Juan Cao, Yongchun Zhu, Danding Wang,
Zhengjia Wang, and Zhiwei Jin. 2023.
Learn over Past, Evolve
for Future: Forecasting Temporal Trends for Fake News Detection.
In Proceedings of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 5: Industry Track). Association
for Computational Linguistics, Toronto, Canada, 116–125.
https:
//doi.org/10.18653/v1/2023.acl-industry.13
[185] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi
Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-Rank
Adaptation of Large Language Models. In The Tenth International
Conference on Learning Representations, ICLR 2022, Virtual Event,
April 25-29, 2022. OpenReview.net. https://openreview.net/forum?
id=nZeVKeeFYf9
[186] Linmei Hu, Zeyi Liu, Ziwang Zhao, Lei Hou, Liqiang Nie, and Juanzi
Li. 2023. A Survey of Knowledge Enhanced Pre-Trained Language
Models. IEEE Transactions on Knowledge and Data Engineering (2023).
[187] Linmei Hu, Tianchi Yang, Luhao Zhang, Wanjun Zhong, Duyu
Tang, Chuan Shi, Nan Duan, and Ming Zhou. 2021. Compare to
The Knowledge: Graph Neural Fake News Detection with Exter-
nal Knowledge. In Proceedings of the 59th Annual Meeting of the
Association for Computational Linguistics and the 11th International
Joint Conference on Natural Language Processing (Volume 1: Long Pa-
pers). Association for Computational Linguistics, Online, 754–763.
https://doi.org/10.18653/v1/2021.acl-long.62
[188] Xuming Hu, Junzhe Chen, Xiaochuan Li, Yufei Guo, Lijie Wen,
Philip S. Yu, and Zhijiang Guo. 2023. Do Large Language Models
Know about Facts? arXiv preprint arXiv: 2310.05177 (2023).
[189] Xuming Hu, Zhijiang Guo, Junzhe Chen, Lijie Wen, and Philip S.
Yu. 2023. MR2: A Benchmark for Multimodal Retrieval-Augmented
Rumor Detection in Social Media. In Proceedings of the 46th Inter-
national ACM SIGIR Conference on Research and Development in
Information Retrieval (Taipei, Taiwan) (SIGIR ’23). Association for
Computing Machinery, New York, NY, USA, 2901–2912.
https:
//doi.org/10.1145/3539618.3591896
[190] Xuming Hu, Zhijiang Guo, Guanyu Wu, Lijie Wen, and Philip S. Yu.
2023. Give Me More Details: Improving Fact-Checking with Latent
Retrieval. arXiv preprint arXiv: 2305.16128 (2023).
[191] Jie Huang and K. Chang. 2022. Towards Reasoning in Large Language
Models: A Survey. Annual Meeting of the Association for Computa-
tional Linguistics (2022). https://doi.org/10.48550/arXiv.2212.10403
[192] Jie Huang and Kevin Chen-Chuan Chang. 2023. Citation: A Key to
Building Responsible and Accountable Large Language Models. arXiv
preprint arXiv: 2307.02185 (2023).
[193] Kung-Hsiang Huang, Kathleen McKeown, Preslav Nakov, Yejin Choi,
and Heng Ji. 2022. Faking Fake News for Real Fake News Detection:
Propaganda-loaded Training Data Generation. arXiv preprint arXiv:
Arxiv-2203.05386 (2022).
[194] Kung-Hsiang Huang, ChengXiang Zhai, and Heng Ji. 2022. CON-
CRETE: Improving Cross-lingual Fact-checking with Cross-lingual
Retrieval. In Proceedings of the 29th International Conference on
Computational Linguistics. International Committee on Computa-
tional Linguistics, Gyeongju, Republic of Korea, 1024–1035. https:
//aclanthology.org/2022.coling-1.86
[195] Linan Huang and Quanyan Zhu. 2023. An Introduction of System-
Scientific Approaches to Cognitive Security. arXiv preprint arXiv:
2301.05920 (2023).
[196] Xiaowei Huang, Wenjie Ruan, Wei Huang, Gaojie Jin, Yi Dong, Chang-
shun Wu, Saddek Bensalem, Ronghui Mu, Yi Qi, Xingyu Zhao, Kaiwen
Cai, Yanghao Zhang, Sihao Wu, Peipei Xu, Dengyu Wu, Andre Freitas,
and Mustafa A. Mustafa. 2023. A Survey of Safety and Trustworthi-
ness of Large Language Models through the Lens of Verification and
Validation. arXiv preprint arXiv: 2305.11391 (2023).
[197] Yinqiu Huang, Min Gao, Jia Wang, and Kai Shu. 2021. Dafd: Domain
adaptation framework for fake news detection. In Neural Information
Processing: 28th International Conference, ICONIP 2021, Sanur, Bali,
Indonesia, December 8–12, 2021, Proceedings, Part I 28. Springer, 305–
316.
[198] Yinqiu Huang, Min Gao, Jia Wang, Junwei Yin, Kai Shu, Qilin Fan,
and Junhao Wen. 2023. Meta-prompt based learning for low-resource
false information detection. Information Processing & Management
60, 3 (2023), 103279.
[199] Yangsibo Huang, Samyak Gupta, Mengzhou Xia, Kai Li, and Danqi
Chen. 2023. Catastrophic Jailbreak of Open-source LLMs via Exploit-
ing Generation. arXiv:2310.06987 [cs.CL]
[200] Yue Huang, Jiawen Shi, Yuan Li, Chenrui Fan, Siyuan Wu, Qihui
Zhang, Yixin Liu, Pan Zhou, Yao Wan, Neil Zhenqiang Gong, and
Lichao Sun. 2023. MetaTool Benchmark for Large Language Models:
Deciding Whether to Use Tools and Which to Use. arXiv preprint
arXiv: 2310.03128 (2023).
[201] Yuheng Huang, Jiayang Song, Zhijie Wang, Huaming Chen, Felix
Juefei-Xu, and Lei Ma. 2023. Look Before You Leap: An Exploratory
Study of Uncertainty Measurement for Large Language Models. arXiv
preprint arXiv: 2307.10236 (2023).
[202] Yue Huang and Lichao Sun. 2023. Harnessing the Power of ChatGPT
in Fake News: An In-Depth Exploration in Generation, Detection and
Explanation. arXiv preprint arXiv: 2310.05046 (2023).
[203] Yue Huang, Qihui Zhang, Philip S. Y, and Lichao Sun. 2023. Trust-
GPT: A Benchmark for Trustworthy and Responsible Large Language
Models. arXiv preprint arXiv: 2306.11507 (2023).
[204] Zhen Huang, Zhilong Lv, Xiaoyun Han, Binyang Li, Menglong Lu,
and Dongsheng Li. 2022. Social Bot-Aware Graph Neural Network
for Early Rumor Detection. In Proceedings of the 29th International
Conference on Computational Linguistics. International Committee on
Computational Linguistics, Gyeongju, Republic of Korea, 6680–6690.
https://aclanthology.org/2022.coling-1.580
[205] Umar Iqbal, Tadayoshi Kohno, and Franziska Roesner. 2023. LLM
Platform Security: Applying a Systematic Evaluation Framework to
OpenAI’s ChatGPT Plugins. arXiv preprint arXiv: 2309.10254 (2023).
[206] Md Rafiqul Islam, Shaowu Liu, Xianzhi Wang, and Guandong Xu.
2020. Deep learning for misinformation detection on online social
networks: a survey and new perspectives. Social Network Analysis
and Mining 10 (2020), 1–20.
[207] J Jaiswal, C LoSchiavo, and DC Perlman. 2020. Disinformation, mis-
information and inequality-driven mistrust in the time of COVID-19:
lessons unlearned from AIDS denialism. AIDS and Behavior 24, 10
(2020), 2776–2780.
[208] Ujun Jeong, Kaize Ding, Lu Cheng, Ruocheng Guo, Kai Shu, and Huan
Liu. 2022. Nothing stands alone: Relational fake news detection with
hypergraph neural networks. In 2022 IEEE International Conference
on Big Data (Big Data). IEEE, 596–605.
[209] Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao Lou,
Kaile Wang, Yawen Duan, Zhonghao He, Jiayi Zhou, Zhaowei Zhang,
Fanzhi Zeng, Kwan Yee Ng, Juntao Dai, Xuehai Pan, Aidan O’Gara,
Yingshan Lei, Hua Xu, Brian Tse, Jie Fu, Stephen McAleer, Yaodong
Yang, Yizhou Wang, Song-Chun Zhu, Yike Guo, and Wen Gao. 2023.
AI Alignment: A Comprehensive Survey.
arXiv preprint arXiv:
2310.19852 (2023).
[210] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, D. Su, Yan Xu, Etsuko
Ishii, Yejin Bang, Wenliang Dai, Andrea Madotto, and Pascale Fung.
2022. Survey of Hallucination in Natural Language Generation. Acm
17
Computing Surveys (2022). https://doi.org/10.1145/3571730
[211] Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, and Pascale
Fung. 2023. Towards Mitigating Hallucination in Large Language
Models via Self-Reflection. arXiv preprint arXiv: 2310.06271 (2023).
[212] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bam-
ford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand,
Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard
Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut
Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023.
Mistral 7B. arXiv preprint arXiv: 2310.06825 (2023).
[213] Bohan Jiang, Zhen Tan, Ayushi Nirmal, and Huan Liu. 2023. Disinfor-
mation Detection: An Evolving Challenge in the Age of LLMs. arXiv
preprint arXiv: 2309.15847 (2023).
[214] Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu,
Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig.
2023. Active Retrieval Augmented Generation. arXiv preprint arXiv:
2305.06983 (2023).
[215] Yiqiao Jin, Xiting Wang, Ruichao Yang, Yizhou Sun, Wei Wang, Hao
Liao, and Xing Xie. 2022. Towards Fine-Grained Reasoning for Fake
News Detection. In Thirty-Sixth AAAI Conference on Artificial In-
telligence, AAAI 2022, Thirty-Fourth Conference on Innovative Ap-
plications of Artificial Intelligence, IAAI 2022, The Twelveth Sympo-
sium on Educational Advances in Artificial Intelligence, EAAI 2022
Virtual Event, February 22 - March 1, 2022. AAAI Press, 5746–5754.
https://ojs.aaai.org/index.php/AAAI/article/view/20517
[216] Pica Johansson, Florence Enock, Scott Hale, Bertie Vidgen, Cassidy
Bereskin, Helen Margetts, and Jonathan Bright. 2022. How can we
combat online misinformation? A systematic overview of current
interventions and their efficacy. arXiv preprint arXiv: 2212.11864
(2022).
[217] Prerna Juneja, M. Bhuiyan, and Tanushree Mitra. 2023. Assessing
enactment of content regulation policies: A post hoc crowd-sourced
audit of election misinformation on YouTube. International Conference
on Human Factors in Computing Systems (2023). https://doi.org/10.
1145/3544548.3580846
[218] Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. 2014. A
Convolutional Neural Network for Modelling Sentences. In Proceed-
ings of the 52nd Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers). Association for Computational
Linguistics, Baltimore, Maryland, 655–665. https://doi.org/10.3115/
v1/P14-1062
[219] Rohit Kumar Kaliyar, Anurag Goswami, and Pratik Narang. 2021.
FakeBERT: Fake News Detection in Social Media with a BERT-
Based Deep Learning Approach. Multimedia Tools Appl. 80, 8 (2021),
11765–11788. https://doi.org/10.1007/s11042-020-10183-2
[220] Daniel Kang, Xuechen Li, I. Stoica, Carlos Guestrin, M. Zaharia, and
Tatsunori Hashimoto. 2023. Exploiting Programmatic Behavior of
LLMs: Dual-Use Through Standard Security Attacks. ARXIV.ORG
(2023). https://doi.org/10.48550/arXiv.2302.05733
[221] Mert Karabacak and Konstantinos Margetis. 2023. Embracing Large
Language Models for Medical Applications: Opportunities and Chal-
lenges. Cureus 15, 5 (2023).
[222] Hamid Karimi and Jiliang Tang. 2019.
Learning Hierarchical
Discourse-level Structure for Fake News Detection. In Proceedings of
the 2019 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, Volume
1 (Long and Short Papers). Association for Computational Linguistics,
Minneapolis, Minnesota, 3432–3442. https://doi.org/10.18653/v1/N19-
1347
[223] Elise Karinshak, Sunny Xun Liu, Joon Sung Park, and Jeffrey T. Han-
cock. 2023. Working With AI to Persuade: Examining a Large Lan-
guage Model’s Ability to Generate Pro-Vaccination Messages. Proc.
ACM Hum.-Comput. Interact. 7, CSCW1, Article 116 (2023), 29 pages.
https://doi.org/10.1145/3579592
[224] Krishnaram Kenthapadi, Himabindu Lakkaraju, and Nazneen Rajani.
2023. Generative AI Meets Responsible AI: Practical Challenges and
Opportunities. In Proceedings of the 29th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining (Long Beach, CA, USA)
(KDD ’23). Association for Computing Machinery, New York, NY,
USA, 5805–5806. https://doi.org/10.1145/3580305.3599557
[225] Dhruv Khattar, Jaipal Singh Goud, Manish Gupta, and Vasudeva
Varma. 2019. MVAE: Multimodal Variational Autoencoder for Fake
News Detection. In The World Wide Web Conference, WWW 2019,
San Francisco, CA, USA, May 13-17, 2019, Ling Liu, Ryen W. White,
Amin Mantrach, Fabrizio Silvestri, Julian J. McAuley, Ricardo Baeza-
Yates, and Leila Zia (Eds.). ACM, 2915–2921. https://doi.org/10.1145/
3308558.3313552
[226] Ling Min Serena Khoo, Hai Leong Chieu, Zhong Qian, and Jing Jiang.
2020. Interpretable Rumor Detection in Microblogs by Attending
to User Interactions. In The Thirty-Fourth AAAI Conference on Arti-
ficial Intelligence, AAAI 2020, The Thirty-Second Innovative Applica-
tions of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI
Symposium on Educational Advances in Artificial Intelligence, EAAI
2020, New York, NY, USA, February 7-12, 2020. AAAI Press, 8783–8790.
https://aaai.org/ojs/index.php/AAAI/article/view/6405
[227] Jagdish Khubchandani and Yilda Macias. 2021. COVID-19 vaccina-
tion hesitancy in Hispanics and African-Americans: A review and
recommendations for practice. Brain, behavior, & immunity-health
15 (2021), 100277.
[228] Celeste Kidd and Abeba Birhane. 2023. How AI can distort human
beliefs. Science 380, 6651 (2023), 1222–1223.
[229] Jiho Kim, Sungjin Park, Yeonsu Kwon, Yohan Jo, James Thorne, and
E. Choi. 2023. FactKG: Fact Verification via Reasoning on Knowl-
edge Graphs. Annual Meeting of the Association for Computational
Linguistics (2023). https://doi.org/10.48550/arXiv.2305.06590
[230] John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian
Miers, and Tom Goldstein. 2023. A Watermark for Large Language
Models. arXiv preprint arXiv: Arxiv-2301.10226 (2023).
[231] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo,
and Yusuke Iwasawa. 2022. Large Language Models are Zero-Shot
Reasoners. In Advances in Neural Information Processing Systems,
Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho
(Eds.). https://openreview.net/forum?id=e2TBb5y0yFf
[232] Sai Koneru, Jian Wu, and Sarah Rajtmajer. 2023. Can Large Language
Models Discern Evidence for Scientific Hypotheses? Case Studies in
the Social Sciences. arXiv preprint arXiv: 2309.06578 (2023).
[233] Ziyi Kou, Lanyu Shang, Yang Zhang, Zhenrui Yue, Huimin Zeng, and
Dong Wang. 2022. Crowd, Expert & AI: A Human-AI Interactive
Approach Towards Natural Language Explanation Based COVID-19
Misinformation Detection. In Proc. Int. Joint Conf. Artif. Intell.(IJCAI).
5087–5093.
[234] Canasai Kruengkrai, Junichi Yamagishi, and Xin Wang. 2021. A
Multi-Level Attention Model for Evidence-Based Fact Checking. In
Findings of the Association for Computational Linguistics: ACL-IJCNLP
2021. Association for Computational Linguistics, Online, 2447–2460.
https://doi.org/10.18653/v1/2021.findings-acl.217
[235] Rohith Kuditipudi, John Thickstun, Tatsunori Hashimoto, and Percy
Liang. 2023. Robust Distortion-free Watermarks for Language Models.
arXiv preprint arXiv: 2307.15593 (2023).
[236] Aounon Kumar, Chirag Agarwal, Suraj Srinivas, Soheil Feizi, and
Hima Lakkaraju. 2023. Certifying LLM Safety against Adversarial
Prompting. arXiv preprint arXiv: 2309.02705 (2023).
[237] Sachin Kumar, Vidhisha Balachandran, Lucille Njoo, Antonios Anas-
tasopoulos, and Yulia Tsvetkov. 2023. Language Generation Mod-
els Can Cause Harm: So What Can We Do About It? An Action-
able Survey. In Proceedings of the 17th Conference of the European
Chapter of the Association for Computational Linguistics. Associa-
tion for Computational Linguistics, Dubrovnik, Croatia, 3299–3321.
18
https://aclanthology.org/2023.eacl-main.241
[238] Srijan Kumar and Neil Shah. 2018. False information on web and
social media: A survey. ArXiv preprint abs/1804.08559 (2018). https:
//arxiv.org/abs/1804.08559
[239] Srijan Kumar, Robert West, and Jure Leskovec. 2016. Disinforma-
tion on the Web: Impact, Characteristics, and Detection of Wikipedia
Hoaxes. In Proceedings of the 25th International Conference on World
Wide Web, WWW 2016, Montreal, Canada, April 11 - 15, 2016, Jacque-
line Bourdeau, Jim Hendler, Roger Nkambou, Ian Horrocks, and Ben Y.
Zhao (Eds.). ACM, 591–602. https://doi.org/10.1145/2872427.2883085
[240] Tharindu Kumarage, Amrita Bhattacharjee, Djordje Padejski, Kristy
Roschke, Dan Gillmor, Scott Ruston, Huan Liu, and Joshua Garland.
2023. J-Guard: Journalism Guided Adversarially Robust Detection of
AI-generated News. arXiv preprint arXiv: 2309.03164 (2023).
[241] Tharindu Kumarage, Joshua Garland, Amrita Bhattacharjee, Kirill
Trapeznikov, Scott Ruston, and Huan Liu. 2023. Stylometric Detection
of AI-Generated Text in Twitter Timelines. arXiv preprint arXiv:
Arxiv-2303.03697 (2023).
[242] Sandipan Kundu, Yuntao Bai, Saurav Kadavath, Amanda Askell, An-
drew Callahan, Anna Chen, Anna Goldie, Avital Balwit, Azalia Mirho-
seini, Brayden McLean, Catherine Olsson, Cassie Evraets, Eli Tran-
Johnson, Esin Durmus, Ethan Perez, Jackson Kernion, Jamie Kerr,
Kamal Ndousse, Karina Nguyen, Nelson Elhage, Newton Cheng,
Nicholas Schiefer, Nova DasSarma, Oliver Rausch, Robin Larson,
Shannon Yang, Shauna Kravec, Timothy Telleen-Lawton, Thomas I.
Liao, Tom Henighan, Tristan Hume, Zac Hatfield-Dodds, Sören Min-
dermann, Nicholas Joseph, Sam McCandlish, and Jared Kaplan. 2023.
Specific versus General Principles for Constitutional AI.
arXiv
preprint arXiv: 2310.13798 (2023).
[243] Laida Kushnareva, Daniil Cherniavskii, Vladislav Mikhailov, Eka-
terina Artemova, Serguei Barannikov, Alexander Bernstein, Irina
Piontkovskaya, Dmitri Piontkovski, and Evgeny Burnaev. 2021. Ar-
tificial Text Detection via Examining the Topology of Attention
Maps. In Proceedings of the 2021 Conference on Empirical Methods
in Natural Language Processing. Association for Computational Lin-
guistics, Online and Punta Cana, Dominican Republic, 635–649.
https://doi.org/10.18653/v1/2021.emnlp-main.50
[244] Viet Dac Lai, Nghia Trung Ngo, Amir Pouran Ben Veyseh, Hieu
Man, Franck Dernoncourt, Trung Bui, and Thien Huu Nguyen. 2023.
ChatGPT Beyond English: Towards a Comprehensive Evaluation of
Large Language Models in Multilingual Learning. arXiv preprint
arXiv: Arxiv-2304.05613 (2023).
[245] Hussain S Lalani, Renée DiResta, Richard J Baron, and David Scales.
2023. Addressing Viral Medical Rumors and False or Misleading
Information. Annals of Internal Medicine 176, 8 (2023), 1113–1120.
[246] An Lao, Chongyang Shi, and Yayi Yang. 2021. Rumor Detection with
Field of Linear and Non-Linear Propagation. In Proceedings of the
Web Conference 2021 (Ljubljana, Slovenia) (WWW ’21). Association
for Computing Machinery, New York, NY, USA, 3178–3187. https:
//doi.org/10.1145/3442381.3450016
[247] Raz Lapid, Ron Langberg, and Moshe Sipper. 2023. Open Sesame!
Universal Black Box Jailbreaking of Large Language Models. arXiv
preprint arXiv: 2309.01446 (2023).
[248] David MJ Lazer, Matthew A Baum, Yochai Benkler, Adam J Berinsky,
Kelly M Greenhill, Filippo Menczer, Miriam J Metzger, Brendan Ny-
han, Gordon Pennycook, David Rothschild, et al. 2018. The science
of fake news. Science 359, 6380 (2018), 1094–1096.
[249] Thai Le, Suhang Wang, and Dongwon Lee. 2020. MALCOM: Gener-
ating Malicious Comments to Attack Neural Fake News Detection
Models. In 20th IEEE International Conference on Data Mining, ICDM
2020, Sorrento, Italy, November 17-20, 2020, Claudia Plant, Haixun
Wang, Alfredo Cuzzocrea, Carlo Zaniolo, and Xindong Wu (Eds.).
IEEE, 282–291. https://doi.org/10.1109/ICDM50108.2020.00037
[250] Deokjae Lee, JunYeong Lee, Jung-Woo Ha, Jin-Hwa Kim, Sang-Woo
Lee, Hwaran Lee, and Hyun Oh Song. 2023. Query-Efficient Black-
Box Red Teaming via Bayesian Optimization. In Proceedings of the
61st Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers). Association for Computational Linguistics,
Toronto, Canada, 11551–11574. https://doi.org/10.18653/v1/2023.acl-
long.646
[251] Nayeon Lee, Yejin Bang, Andrea Madotto, and Pascale Fung. 2021.
Towards Few-shot Fact-Checking via Perplexity. In Proceedings of
the 2021 Conference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Language Technolo-
gies. Association for Computational Linguistics, Online, 1971–1981.
https://doi.org/10.18653/v1/2021.naacl-main.158
[252] Nayeon Lee, Belinda Z. Li, Sinong Wang, Pascale Fung, Hao Ma,
Wen-tau Yih, and Madian Khabsa. 2021. On Unifying Misinformation
Detection. In Proceedings of the 2021 Conference of the North Ameri-
can Chapter of the Association for Computational Linguistics: Human
Language Technologies. Association for Computational Linguistics,
Online, 5479–5485. https://doi.org/10.18653/v1/2021.naacl-main.432
[253] Nayeon Lee, Wei Ping, Peng Xu, M. Patwary, M. Shoeybi, and Bryan
Catanzaro. 2022. Factuality Enhanced Language Models for Open-
Ended Text Generation. ARXIV.ORG (2022). https://doi.org/10.48550/
arXiv.2206.04624
[254] Taehyun Lee, Seokhee Hong, Jaewoo Ahn, Ilgee Hong, Hwaran Lee,
Sangdoo Yun, Jamin Shin, and Gunhee Kim. 2023. Who Wrote this
Code? Watermarking for Code Generation. arXiv preprint arXiv:
2305.15060 (2023).
[255] Deren Lei, Yaxi Li, Mengya Hu, Mingyu Wang, Vincent Yun, Emily
Ching, and Eslam Kamal. 2023. Chain of Natural Language Inference
for Reducing Large Language Model Ungrounded Hallucinations.
arXiv preprint arXiv: 2310.03951 (2023).
[256] João A. Leite, Olesya Razuvayevskaya, Kalina Bontcheva, and Car-
olina Scarton. 2023. Detecting Misinformation with LLM-Predicted
Credibility Signals and Weak Supervision.
arXiv preprint arXiv:
2309.07601 (2023).
[257] Or Levi, Pedram Hosseini, Mona Diab, and David Broniatowski. 2019.
Identifying Nuances in Fake News vs. Satire: Using Semantic and
Linguistic Cues. In Proceedings of the Second Workshop on Natural
Language Processing for Internet Freedom: Censorship, Disinformation,
and Propaganda. Association for Computational Linguistics, Hong
Kong, China, 31–35. https://doi.org/10.18653/v1/D19-5004
[258] Stephan Lewandowsky, Ullrich KH Ecker, Colleen M Seifert, Norbert
Schwarz, and John Cook. 2012. Misinformation and its correction:
Continued influence and successful debiasing. Psychological science
in the public interest 13, 3 (2012), 106–131.
[259] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and
Ying Shan. 2023. SEED-Bench: Benchmarking Multimodal LLMs with
Generative Comprehension. arXiv preprint arXiv: 2307.16125 (2023).
[260] Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li,
Lijuan Wang, and Jianfeng Gao. 2023. Multimodal Foundation Models:
From Specialists to General-Purpose Assistants. arXiv preprint arXiv:
2309.10020 (2023).
[261] Chen Li, Hao Peng, Jianxin Li, Lichao Sun, Lingjuan Lyu, Lihong
Wang, Philip S. Yu, and Lifang He. 2022. Joint Stance and Rumor
Detection in Hierarchical Heterogeneous Graph. IEEE Transactions
on Neural Networks and Learning Systems 33, 6 (2022), 2530–2542.
https://doi.org/10.1109/TNNLS.2021.3114027
[262] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii
Khizbullin, and Bernard Ghanem. 2023. CAMEL: Communicative
Agents for ""Mind"" Exploration of Large Scale Language Model Society.
arXiv preprint arXiv: 2303.17760 (2023).
[263] Haoran Li, Dadi Guo, Wei Fan, Mingshi Xu, and Yangqiu Song. 2023.
Multi-step Jailbreaking Privacy Attacks on ChatGPT. ARXIV.ORG
(2023). https://doi.org/10.48550/arXiv.2304.05197
19
[264] Huayang Li, Yixuan Su, Deng Cai, Yan Wang, and Lemao Liu. 2022.
A Survey on Retrieval-Augmented Text Generation. arXiv preprint
arXiv: 2202.01110 (2022).
[265] Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong
Wen. 2023. HaluEval: A Large-Scale Hallucination Evaluation Bench-
mark for Large Language Models. arXiv preprint arXiv: 2305.11747
(2023).
[266] Jianning Li, Amin Dada, Jens Kleesiek, and Jan Egger. 2023. ChatGPT
in Healthcare: A Taxonomy and Systematic Review. medRxiv (2023),
2023–03.
[267] Jiawen Li, Shiwen Ni, and Hung-Yu Kao. 2021. Meet The Truth: Lever-
age Objective Facts and Subjective Views for Interpretable Rumor
Detection. In Findings of the Association for Computational Linguistics:
ACL-IJCNLP 2021. Association for Computational Linguistics, Online,
705–715. https://doi.org/10.18653/v1/2021.findings-acl.63
[268] Jiawen Li, Yudianto Sujana, and Hung-Yu Kao. 2020. Exploiting
Microblog Conversation Structures to Detect Rumors. In Proceedings
of the 28th International Conference on Computational Linguistics.
International Committee on Computational Linguistics, Barcelona,
Spain (Online), 5420–5429. https://doi.org/10.18653/v1/2020.coling-
main.473
[269] Ke Li, Bin Guo, Siyuan Ren, and Zhiwen Yu. 2022. AdaDebunk: An
Efficient and Reliable Deep State Space Model for Adaptive Fake
News Early Detection. In Proceedings of the 31st ACM International
Conference on Information & Knowledge Management (Atlanta, GA,
USA) (CIKM ’22). Association for Computing Machinery, New York,
NY, USA, 1156–1165. https://doi.org/10.1145/3511808.3557227
[270] Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, and Mar-
tin Wattenberg. 2023. Inference-Time Intervention: Eliciting Truthful
Answers from a Language Model. arXiv preprint arXiv: 2306.03341
(2023).
[271] Miaoran Li, Baolin Peng, and Zhu Zhang. 2023. Self-Checker: Plug-
and-Play Modules for Fact-Checking with Large Language Models.
arXiv preprint arXiv: 2305.14623 (2023).
[272] Quanzhi Li, Qiong Zhang, and Luo Si. 2019. Rumor Detection by
Exploiting User Credibility Information, Attention and Multi-task
Learning. In Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics. Association for Computational Linguis-
tics, Florence, Italy, 1173–1179. https://doi.org/10.18653/v1/P19-1113
[273] Quanzhi Li, Qiong Zhang, Luo Si, and Yingchi Liu. 2019. Rumor
Detection on Social Media: Datasets, Methods and Opportunities. In
Proceedings of the Second Workshop on Natural Language Processing
for Internet Freedom: Censorship, Disinformation, and Propaganda.
Association for Computational Linguistics, Hong Kong, China, 66–75.
https://doi.org/10.18653/v1/D19-5008
[274] Qifei Li and Wangchunshu Zhou. 2020. Connecting the Dots Between
Fact Verification and Fake News Detection. In Proceedings of the 28th
International Conference on Computational Linguistics. International
Committee on Computational Linguistics, Barcelona, Spain (Online),
1820–1825. https://doi.org/10.18653/v1/2020.coling-main.165
[275] Xiaopeng Li, Shasha Li, Shezheng Song, Jing Yang, Jun Ma, and Jie Yu.
2023. PMET: Precise Model Editing in a Transformer. arXiv preprint
arXiv: 2308.08742 (2023).
[276] Xinyi Li, Yongfeng Zhang, and Edward C. Malthouse. 2023. A Prelim-
inary Study of ChatGPT on News Recommendation: Personalization,
Provider Fairness, Fake News. arXiv preprint arXiv: 2306.10702 (2023).
[277] Yichuan Li, Kyumin Lee, Nima Kordzadeh, Brenton Faber, Cameron
Fiddes, Elaine Chen, and Kai Shu. 2021. Multi-source domain adap-
tation with weak supervision for early fake news detection. In 2021
IEEE International Conference on Big Data (Big Data). IEEE, 668–676.
[278] Yafu Li, Qintong Li, Leyang Cui, Wei Bi, Longyue Wang, Linyi Yang,
Shuming Shi, and Yue Zhang. 2023. Deepfake Text Detection in the
Wild. arXiv preprint arXiv: 2305.13242 (2023).
[279] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara
Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai
Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan,
Ce Zhang, Christian Alexander Cosgrove, Christopher D Manning,
Christopher Re, Diana Acosta-Navas, Drew Arad Hudson, Eric Ze-
likman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren,
Huaxiu Yao, Jue WANG, Keshav Santhanam, Laurel Orr, Lucia Zheng,
Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri S.
Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan An-
drew Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tat-
sunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary,
William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Ko-
reeda. 2023. Holistic Evaluation of Language Models. Transactions
on Machine Learning Research (2023). https://openreview.net/forum?
id=iO4LZibEqW Featured Certification, Expert Certification.
[280] Hao Liao, Jiahao Peng, Zhanyi Huang, Wei Zhang, Guanghua Li, Kai
Shu, and Xing Xie. 2023. MUSER: A MUlti-Step Evidence Retrieval
Enhancement Framework for Fake News Detection. In Proceedings of
the 29th ACM SIGKDD Conference on Knowledge Discovery and Data
Mining. 4461–4472.
[281] Q. Vera Liao and Jennifer Wortman Vaughan. 2023. AI Transparency
in the Age of LLMs: A Human-Centered Research Roadmap. arXiv
preprint arXiv: 2306.01941 (2023).
[282] Wenxiong Liao, Zhengliang Liu, Haixing Dai, Shaochen Xu, Zihao
Wu, Yiyang Zhang, Xiaoke Huang, Dajiang Zhu, Hongmin Cai, Tian-
ming Liu, and Xiang Li. 2023. Differentiate ChatGPT-generated and
Human-written Medical Texts. arXiv preprint arXiv: Arxiv-2304.11567
(2023).
[283] Anders Edelbo Lillie and Emil Refsgaard Middelboe. 2019. Fake
news detection using stance classification: A survey. ArXiv preprint
abs/1907.00181 (2019). https://arxiv.org/abs/1907.00181
[284] Hongzhan Lin, Jing Ma, Liangliang Chen, Zhiwei Yang, Mingfei
Cheng, and Chen Guang. 2022. Detect Rumors in Microblog Posts
for Low-Resource Domains via Adversarial Contrastive Learning. In
Findings of the Association for Computational Linguistics: NAACL 2022.
Association for Computational Linguistics, Seattle, United States,
2543–2556. https://doi.org/10.18653/v1/2022.findings-naacl.194
[285] Hongzhan Lin, Jing Ma, Mingfei Cheng, Zhiwei Yang, Liangliang
Chen, and Guang Chen. 2021. Rumor Detection on Twitter with
Claim-Guided Hierarchical Graph Attention Networks. In Proceedings
of the 2021 Conference on Empirical Methods in Natural Language
Processing. Association for Computational Linguistics, Online and
Punta Cana, Dominican Republic, 10035–10047. https://doi.org/10.
18653/v1/2021.emnlp-main.786
[286] Hongzhan Lin, Pengyao Yi, Jing Ma, Haiyun Jiang, Ziyang Luo, Shum-
ing Shi, and Ruifang Liu. 2023. Zero-shot rumor detection with prop-
agation structure via prompt learning. In Proceedings of the AAAI
Conference on Artificial Intelligence, Vol. 37. 5213–5221.
[287] Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. TruthfulQA:
Measuring How Models Mimic Human Falsehoods. In Proceedings of
the 60th Annual Meeting of the Association for Computational Linguis-
tics (Volume 1: Long Papers). Association for Computational Linguis-
tics, Dublin, Ireland, 3214–3252. https://doi.org/10.18653/v1/2022.acl-
long.229
[288] Chengyuan Liu, Fubang Zhao, Lizhi Qing, Yangyang Kang, Chang-
long Sun, Kun Kuang, and Fei Wu. 2023. A Chinese Prompt Attack
Dataset for LLMs with Evil Content. arXiv preprint arXiv: 2309.11830
(2023).
[289] Fuxiao Liu, Tianrui Guan, Zongxia Li, Lichang Chen, Yaser Yacoob,
Dinesh Manocha, and Tianyi Zhou. 2023. HallusionBench: You See
What You Think? Or You Think What You See? An Image-Context
Reasoning Benchmark Challenging for GPT-4V(ision), LLaVA-1.5,
and Other Multi-modality Models. arXiv:2310.14566 [cs.CV]
20
[290] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and
Lijuan Wang. 2023. Aligning Large Multi-Modal Model with Robust
Instruction Tuning. ArXiv preprint abs/2306.14565 (2023).
https:
//arxiv.org/abs/2306.14565
[291] Fuxiao Liu, Yaser Yacoob, and Abhinav Shrivastava. 2023. COVID-
VTS: Fact Extraction and Verification on Short Video Platforms. In
Proceedings of the 17th Conference of the European Chapter of the Asso-
ciation for Computational Linguistics. Association for Computational
Linguistics, Dubrovnik, Croatia, 178–188. https://aclanthology.org/
2023.eacl-main.14
[292] Haoyang Liu, Maheep Chaudhary, and Haohan Wang. 2023. Towards
Trustworthy and Aligned Machine Learning: A Data-centric Survey
with Causality Perspectives. arXiv preprint arXiv: 2307.16851 (2023).
[293] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023.
Visual Instruction Tuning. ARXIV.ORG (2023).
https://doi.org/10.
48550/arXiv.2304.08485
[294] Hui Liu, Wenya Wang, and Hao Li. 2023. Interpretable Multimodal
Misinformation Detection with Logic Reasoning. Annual Meeting of
the Association for Computational Linguistics (2023). https://doi.org/
10.48550/arXiv.2305.05964
[295] Haochen Liu, Yiqi Wang, Wenqi Fan, Xiaorui Liu, Yaxin Li, Shaili Jain,
Anil K. Jain, and Jiliang Tang. 2021. Trustworthy AI: A Computational
Perspective. Acm Transactions On Intelligent Systems And Technology
(2021). https://doi.org/10.1145/3546872
[296] Jiongnan Liu, Jiajie Jin, Zihan Wang, Jiehan Cheng, Zhicheng Dou,
and Ji-Rong Wen. 2023. RETA-LLM: A Retrieval-Augmented Large
Language Model Toolkit. arXiv preprint arXiv: 2306.05212 (2023).
[297] Q. Liu, Jun Wu, Shu Wu, and Liang Wang. 2023. Out-of-distribution
Evidence-aware Fake News Detection via Dual Adversarial Debiasing.
ARXIV.ORG (2023). https://doi.org/10.48550/arXiv.2304.12888
[298] Xiao Liu, Hanyu Lai, Hao Yu, Yifan Xu, Aohan Zeng, Zhengxiao Du,
Peng Zhang, Yuxiao Dong, and Jie Tang. 2023. WebGLM: Towards An
Efficient Web-Enhanced Question Answering System with Human
Preferences. arXiv preprint arXiv: 2306.07906 (2023).
[299] Xin Liu, Daniel McDuff, Geza Kovacs, Isaac Galatzer-Levy, Jacob
Sunshine, Jiening Zhan, Ming-Zher Poh, Shun Liao, Paolo Di Achille,
and Shwetak Patel. 2023. Large Language Models are Few-Shot Health
Learners. arXiv preprint arXiv: 2305.15525 (2023).
[300] Xiaogeng Liu, Nan Xu, Muhao Chen, and Chaowei Xiao. 2023. Au-
toDAN: Generating Stealthy Jailbreak Prompts on Aligned Large
Language Models. arXiv preprint arXiv: 2310.04451 (2023).
[301] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai,
Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang,
Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng
Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong,
and Jie Tang. 2023. AgentBench: Evaluating LLMs as Agents. arXiv
preprint arXiv: 2308.03688 (2023).
[302] Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Tianwei Zhang,
Yepang Liu, Haoyu Wang, Yan Zheng, and Yang Liu. 2023. Prompt
Injection attack against LLM-integrated Applications. arXiv preprint
arXiv: 2306.05499 (2023).
[303] Yi Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng, Ying
Zhang, Lida Zhao, Tianwei Zhang, and Yang Liu. 2023. Jailbreaking
ChatGPT via Prompt Engineering: An Empirical Study. arXiv preprint
arXiv: 2305.13860 (2023).
[304] Yang Liu and Yi-Fang Brook Wu. 2020. FNED: A Deep Network for
Fake News Early Detection on Social Media. ACM Trans. Inf. Syst. 38,
3, Article 25 (2020), 33 pages. https://doi.org/10.1145/3386253
[305] Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang,
Ruocheng Guo Hao Cheng, Yegor Klochkov, Muhammad Faaiz Taufiq,
and Hang Li. 2023. Trustworthy LLMs: a Survey and Guideline for
Evaluating Large Language Models’ Alignment. arXiv preprint arXiv:
2308.05374 (2023).
[306] Ye Liu, Semih Yavuz, Rui Meng, Meghana Moorthy, Shafiq Joty, Caim-
ing Xiong, and Yingbo Zhou. 2023. Exploring the Integration Strate-
gies of Retriever and Large Language Models. arXiv preprint arXiv:
2308.12574 (2023).
[307] Yikang Liu, Ziyin Zhang, Wanyang Zhang, Shisen Yue, Xiaojing Zhao,
Xinyuan Cheng, Yiwen Zhang, and Hai Hu. 2023. ArguGPT: evaluat-
ing, understanding and identifying argumentative essays generated
by GPT models. ARXIV.ORG (2023). https://doi.org/10.48550/arXiv.
2304.07666
[308] Zhaoyang Liu, Yinan He, Wenhai Wang, Weiyun Wang, Yi Wang,
Shoufa Chen, Qinglong Zhang, Zeqiang Lai, Yang Yang, Qingyun Li,
Jiashuo Yu, Kunchang Li, Zhe Chen, Xue Yang, Xizhou Zhu, Yali Wang,
Limin Wang, Ping Luo, Jifeng Dai, and Yu Qiao. 2023. InternGPT:
Solving Vision-Centric Tasks by Interacting with ChatGPT Beyond
Language. arXiv preprint arXiv: 2305.05662 (2023).
[309] Zhiwei Liu, Weiran Yao, Jianguo Zhang, Le Xue, Shelby Heinecke,
Rithesh Murthy, Yihao Feng, Zeyuan Chen, Juan Carlos Niebles, De-
vansh Arpit, Ran Xu, Phil Mui, Huan Wang, Caiming Xiong, and
Silvio Savarese. 2023. BOLAA: Benchmarking and Orchestrating LLM-
augmented Autonomous Agents. arXiv preprint arXiv: 2308.05960
(2023).
[310] Sahil Loomba, Alexandre de Figueiredo, Simon J Piatek, Kristen de
Graaf, and Heidi J Larson. 2021. Measuring the impact of COVID-19
vaccine misinformation on vaccination intent in the UK and USA.
Nature human behaviour 5, 3 (2021), 337–348.
[311] Menglong Lu, Zhen Huang, Binyang Li, Yunxiang Zhao, Zheng Qin,
and DongSheng Li. 2022. SIFTER: A Framework for Robust Rumor
Detection. IEEE/ACM Transactions on Audio, Speech, and Language
Processing 30 (2022), 429–442. https://doi.org/10.1109/TASLP.2022.
3140474
[312] Qinghua Lu, Liming Zhu, Xiwei Xu, Jon Whittle, Didar Zowghi, and
Aurelie Jacquet. 2022. Responsible AI Pattern Catalogue: A Collection
of Best Practices for AI Governance and Engineering. arXiv preprint
arXiv: 2209.04963 (2022).
[313] Yi-Ju Lu and Cheng-Te Li. 2020. GCAN: Graph-aware Co-Attention
Networks for Explainable Fake News Detection on Social Media. In
Proceedings of the 58th Annual Meeting of the Association for Computa-
tional Linguistics. Association for Computational Linguistics, Online,
505–514. https://doi.org/10.18653/v1/2020.acl-main.48
[314] Guoqing Luo, Yu Tong Han, Lili Mou, and Mauajama Firdaus. 2023.
Prompt-Based Editing for Text Style Transfer. arXiv preprint arXiv:
2301.11997 (2023).
[315] Junyu Luo, Cao Xiao, and Fenglong Ma. 2023. Zero-Resource Halluci-
nation Prevention for Large Language Models. arXiv preprint arXiv:
2309.02654 (2023).
[316] Yuefei Lyu, Xiaoyu Yang, Jiaxin Liu, Sihong Xie, Philip Yu, and Xi
Zhang. 2023. Interpretable and effective reinforcement learning for
attacking against graph-based rumor detection. In 2023 International
Joint Conference on Neural Networks (IJCNN). IEEE, 1–9.
[317] Jing Ma and Wei Gao. 2020. Debunking Rumors on Twitter with
Tree Transformer. In Proceedings of the 28th International Conference
on Computational Linguistics. International Committee on Compu-
tational Linguistics, Barcelona, Spain (Online), 5455–5466.
https:
//doi.org/10.18653/v1/2020.coling-main.476
[318] Jing Ma, Wei Gao, Prasenjit Mitra, Sejeong Kwon, Bernard J. Jansen,
Kam-Fai Wong, and Meeyoung Cha. 2016. Detecting Rumors from
Microblogs with Recurrent Neural Networks. In Proceedings of the
Twenty-Fifth International Joint Conference on Artificial Intelligence,
IJCAI 2016, New York, NY, USA, 9-15 July 2016, Subbarao Kambhampati
(Ed.). IJCAI/AAAI Press, 3818–3824. http://www.ijcai.org/Abstract/
16/537
[319] Jing Ma, Jun Li, Wei Gao, Yang Yang, and Kam-Fai Wong. 2021. Im-
proving rumor detection by promoting information campaigns with
transformer-based generative adversarial learning. IEEE Transactions
21
on Knowledge and Data Engineering (2021).
[320] Jiachen Ma, Yong Liu, Meng Liu, and Meng Han. 2022. Curricu-
lum Contrastive Learning for Fake News Detection. In Proceed-
ings of the 31st ACM International Conference on Information &
Knowledge Management (Atlanta, GA, USA) (CIKM ’22). Associa-
tion for Computing Machinery, New York, NY, USA, 4309–4313.
https://doi.org/10.1145/3511808.3557574
[321] Yongqiang Ma, Jiawei Liu, Fan Yi, Qikai Cheng, Yong Huang, Wei Lu,
and Xiaozhong Liu. 2023. AI vs. Human - Differentiation Analysis of
Scientific Content Generation. arXiv preprint arXiv: Arxiv-2301.10416
(2023).
[322] Abdurahman Maarouf, Dominik Bär, Dominique Geissler, and Stefan
Feuerriegel. 2023. HQP: A Human-Annotated Dataset for Detecting
Online Propaganda. arXiv preprint arXiv: 2304.14931 (2023).
[323] Syed Mahbub, Eric Pardede, and ASM Kayes. 2022. COVID-19 Rumor
Detection Using Psycho-Linguistic Features. IEEE Access 10 (2022),
117530–117543.
[324] Mohammad Mahyoob, Jeehaan Al-Garaady, and Musaad Alrahaili.
2020. Linguistic-based detection of fake news in social media. Forth-
coming, International Journal of English Linguistics 11, 1 (2020).
[325] Martin Májovsk`
y, Martin Čern`
y, Matěj Kasal, Martin Komarc, and
David Netuka. 2023. Artificial Intelligence Can Generate Fraudulent
but Authentic-Looking Scientific Medical Articles: Pandora’s Box Has
Been Opened. Journal of Medical Internet Research 25 (2023), e46924.
[326] Potsawee Manakul, Adian Liusie, and Mark J. F. Gales. 2023. Self-
CheckGPT: Zero-Resource Black-Box Hallucination Detection for
Generative Large Language Models. arXiv preprint arXiv: Arxiv-
2303.08896 (2023).
[327] Giovanni Da San Martino, Stefano Cresci, Alberto Barrón-Cedeño,
Seunghak Yu, Roberto Di Pietro, and Preslav Nakov. 2020. A Survey on
Computational Propaganda Detection. In Proceedings of the Twenty-
Ninth International Joint Conference on Artificial Intelligence, IJCAI
2020, Christian Bessiere (Ed.). ijcai.org, 4826–4832. https://doi.org/
10.24963/ijcai.2020/672
[328] Hana Matatov, Mor Naaman, and Ofra Amir. 2022. Stop the [Image]
Steal: The Role and Dynamics of Visual Content in the 2020 U.S.
Election Misinformation Campaign. arXiv preprint arXiv: 2209.02007
(2022).
[329] Mohit Mayank, Shakshi Sharma, and Rajesh Sharma. 2021. DEAP-
FAKED: Knowledge Graph based Approach for Fake News Detection.
International Conference on Advances in Social Networks Analysis and
Mining (2021). https://doi.org/10.1109/ASONAM55673.2022.10068653
[330] Philip M McCarthy. 2005. An assessment of the range and usefulness
of lexical diversity measures and the potential of the measure of tex-
tual, lexical diversity (MTLD). Ph. D. Dissertation. The University of
Memphis.
[331] Priyanka Meel and Dinesh Kumar Vishwakarma. 2020. Fake news,
rumor, information pollution in social media and web: A contempo-
rary survey of state-of-the-arts, challenges and opportunities. Expert
Systems with Applications 153 (2020), 112986.
[332] Ninareh Mehrabi, Palash Goyal, Christophe Dupuy, Qian Hu, Shalini
Ghosh, Richard Zemel, Kai-Wei Chang, Aram Galstyan, and Rahul
Gupta. 2023. FLIRT: Feedback Loop In-context Red Teaming. arXiv
preprint arXiv: 2308.04265 (2023).
[333] Nikhil Mehta, Maria Leonor Pacheco, and Dan Goldwasser. 2022.
Tackling Fake News Detection by Continually Improving Social Con-
text Representations using Graph Neural Networks. In Proceedings of
the 60th Annual Meeting of the Association for Computational Linguis-
tics (Volume 1: Long Papers). Association for Computational Linguis-
tics, Dublin, Ireland, 1363–1380. https://doi.org/10.18653/v1/2022.acl-
long.97
[334] Alex Mei, Sharon Levy, and William Yang Wang. 2023. ASSERT: Au-
tomated Safety Scenario Red Teaming for Evaluating the Robustness
of Large Language Models. arXiv preprint arXiv: 2310.09624 (2023).
[335] Filippo Menczer, David Crandall, Yong-Yeol Ahn, and Apu Kapadia.
2023. Addressing the harms of AI-generated inauthentic content.
Nature Machine Intelligence 5, 7 (2023), 679–680.
[336] Ethan Mendes, Yang Chen, Wei Xu, and Alan Ritter. 2023. Human-
in-the-loop Evaluation for Early Misinformation Detection: A Case
Study of COVID-19 Treatments. In Proceedings of the 61st Annual
Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers). Association for Computational Linguistics, Toronto,
Canada, 15817–15835. https://doi.org/10.18653/v1/2023.acl-long.881
[337] Kevin Meng, David Bau, A. Andonian, and Yonatan Belinkov. 2022.
Locating and Editing Factual Associations in GPT. Neural Information
Processing Systems (2022).
[338] Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpan-
tis, Ram Pasunuru, Roberta Raileanu, Baptiste Rozière, Timo Schick,
Jane Dwivedi-Yu, Asli Celikyilmaz, Edouard Grave, Yann LeCun, and
Thomas Scialom. 2023. Augmented Language Models: a Survey. arXiv
preprint arXiv: 2302.07842 (2023).
[339] midjourney. 2023. midjourney.com. https://www.midjourney.com/.
Accessed: 2023-10-3.
[340] Erxue Min, Yu Rong, Yatao Bian, Tingyang Xu, Peilin Zhao, Junzhou
Huang, and Sophia Ananiadou. 2022. Divide-and-Conquer: Post-User
Interaction Network for Fake News Detection on Social Media. In
Proceedings of the ACM Web Conference 2022 (Virtual Event, Lyon,
France) (WWW ’22). Association for Computing Machinery, New
York, NY, USA, 1148–1158. https://doi.org/10.1145/3485447.3512163
[341] Fatemehsadat Mireshghallah, Justus Mattern, Sicun Gao, Reza Shokri,
and Taylor Berg-Kirkpatrick. 2023. Smaller Language Models are
Better Black-box Machine-Generated Text Detectors. arXiv preprint
arXiv: 2305.09859 (2023).
[342] Saurabh Mishra, Jack Clark, and C. Raymond Perrault. 2020. Mea-
surement in AI Policy: Opportunities and Challenges. arXiv preprint
arXiv: 2009.09071 (2020).
[343] Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D.
Manning, and Chelsea Finn. 2023. DetectGPT: Zero-Shot Machine-
Generated Text Detection using Probability Curvature. arXiv preprint
arXiv: Arxiv-2301.11305 (2023).
[344] Sandra Mitrovi’c, Davide Andreoletti, and Omran Ayoub. 2023. Chat-
GPT or Human? Detect and Explain. Explaining Decisions of Ma-
chine Learning Model for Detecting Short ChatGPT-generated Text.
ARXIV.ORG (2023). https://doi.org/10.48550/arXiv.2301.13852
[345] Ryan C Moore, Ross Dahlke, and Jeffrey T Hancock. 2023. Exposure
to untrustworthy websites in the 2020 US election. Nature Human
Behaviour (2023), 1–10.
[346] Ahmadreza Mosallanezhad, Mansooreh Karami, Kai Shu, Michelle V
Mancenido, and Huan Liu. 2022. Domain adaptive fake news detection
via reinforcement learning. In Proceedings of the ACM Web Conference
2022. 3632–3640.
[347] Maximilian Mozes, Xuanli He, Bennett Kleinberg, and Lewis D. Griffin.
2023. Use of LLMs for Illicit Purposes: Threats, Prevention Measures,
and Vulnerabilities. arXiv preprint arXiv: 2308.12833 (2023).
[348] Akhtar Mubashara, Schlichtkrull Michael, Guo Zhijiang, Cocarascu
Oana, Simperl Elena, and Vlachos Andreas. 2023. Multimodal Au-
tomated Fact-Checking: A Survey. arXiv preprint arXiv: 2305.13507
(2023).
[349] Dor Muhlgay, Ori Ram, Inbal Magar, Yoav Levine, Nir Ratner, Yonatan
Belinkov, Omri Abend, Kevin Leyton-Brown, Amnon Shashua, and
Yoav Shoham. 2023. Generating Benchmarks for Factuality Evaluation
of Language Models. arXiv preprint arXiv: 2307.06908 (2023).
[350] Alberto Muñoz-Ortiz, Carlos Gómez-Rodríguez, and David Vilares.
2023. Contrasting Linguistic Patterns in Human and LLM-Generated
Text. arXiv preprint arXiv: 2308.09067 (2023).
[351] Niels Mündler, Jingxuan He, Slobodan Jenko, and Martin Vechev.
2023. Self-contradictory Hallucinations of Large Language Models:
Evaluation, Detection and Mitigation. arXiv preprint arXiv: 2305.15852
22
(2023).
[352] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang,
Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju,
William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen
Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John
Schulman. 2021. WebGPT: Browser-assisted question-answering
with human feedback. arXiv preprint arXiv: 2112.09332 (2021).
[353] Preslav Nakov, Alberto Barrón-Cedeño, Giovanni Da San Martino,
Firoj Alam, Julia Maria Struß, Thomas Mandl, Rubén Míguez, Tom-
maso Caselli, Mucahid Kutlu, Wajdi Zaghouani, Chengkai Li, Shaden
Shaar, Gautam Kishore Shahi, Hamdy Mubarak, Alex Nikolov, Niko-
lay Babulkov, Yavuz Selim Kartal, and Javier Beltrán. 2022. The
CLEF-2022 CheckThat! Lab on Fighting the COVID-19 Infodemic and
Fake News Detection. In Advances in Information Retrieval, Matthias
Hagen, Suzan Verberne, Craig Macdonald, Christin Seifert, Krisztian
Balog, Kjetil Nørvåg, and Vinay Setty (Eds.). Springer International
Publishing, Cham, 416–428.
[354] Preslav Nakov, David Corney, Maram Hasanain, Firoj Alam, Tamer
Elsayed, Alberto Barrón-Cedeño, Paolo Papotti, Shaden Shaar, and
Giovanni Da San Martino. 2021. Automated Fact-Checking for Assist-
ing Human Fact-Checkers. arXiv preprint arXiv: 2103.07769 (2021).
[355] Qiong Nan, Juan Cao, Yongchun Zhu, Yanyan Wang, and Jintao Li.
2021. MDFEND: Multi-Domain Fake News Detection. In Proceed-
ings of the 30th ACM International Conference on Information &amp;
Knowledge Management (Virtual Event, Queensland, Australia) (CIKM
’21). Association for Computing Machinery, New York, NY, USA,
3343–3347. https://doi.org/10.1145/3459637.3482139
[356] Christof Naumzik and Stefan Feuerriegel. 2022. Detecting False
Rumors from Retweet Dynamics on Social Media. In Proceedings of
the ACM Web Conference 2022 (Virtual Event, Lyon, France) (WWW
’22). Association for Computing Machinery, New York, NY, USA,
2798–2809. https://doi.org/10.1145/3485447.3512000
[357] Subhash Nerella, Sabyasachi Bandyopadhyay, Jiaqing Zhang, Miguel
Contreras, Scott Siegel, Aysegul Bumin, Brandon Silva, Jessica Sena,
Benjamin Shickel, Azra Bihorac, Kia Khezeli, and Parisa Rashidi.
2023. Transformers in Healthcare: A Survey. arXiv preprint arXiv:
2307.00067 (2023).
[358] newsguardtech.com.
2023.
Rise
of
the
Newsbots:
AI-
Generated
News
Websites
Proliferating
Online.
https:
//www.newsguardtech.com/special-reports/newsbots-ai-
generated-news-websites-proliferating/.
Accessed: 2023-10-
3.
[359] An T Nguyen, Aditya Kharosekar, Saumyaa Krishnan, Siddhesh Krish-
nan, Elizabeth Tate, Byron C Wallace, and Matthew Lease. 2018. Be-
lieve it or not: Designing a human-ai partnership for mixed-initiative
fact-checking. In Proceedings of the 31st Annual ACM Symposium on
User Interface Software and Technology. 189–199.
[360] Duc Minh Nguyen, Tien Huu Do, Robert Calderbank, and Nikos Deli-
giannis. 2019. Fake News Detection using Deep Markov Random
Fields. In Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Lan-
guage Technologies, Volume 1 (Long and Short Papers). Association
for Computational Linguistics, Minneapolis, Minnesota, 1391–1400.
https://doi.org/10.18653/v1/N19-1141
[361] Van-Hoang Nguyen, Kazunari Sugiyama, Preslav Nakov, and Min-
Yen Kan. 2020. FANG: Leveraging Social Context for Fake News
Detection Using Graph Representation. In CIKM ’20: The 29th ACM
International Conference on Information and Knowledge Management,
Virtual Event, Ireland, October 19-23, 2020, Mathieu d’Aquin, Stefan
Dietze, Claudia Hauff, Edward Curry, and Philippe Cudré-Mauroux
(Eds.). ACM, 1165–1174. https://doi.org/10.1145/3340531.3412046
[362] Dan Saattrup Nielsen and Ryan McConville. 2022. MuMiN: A Large-
Scale Multilingual Multimodal Fact-Checked Misinformation Social
Network Dataset. Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval (2022).
https:
//doi.org/10.1145/3477495.3531744
[363] Ray Oshikawa, Jing Qian, and William Yang Wang. 2020. A Survey on
Natural Language Processing for Fake News Detection. In Proceedings
of the Twelfth Language Resources and Evaluation Conference. Euro-
pean Language Resources Association, Marseille, France, 6086–6093.
https://aclanthology.org/2020.lrec-1.747
[364] Nedjma Ousidhoum, Zhangdie Yuan, and Andreas Vlachos. 2022.
Varifocal Question Generation for Fact-checking. In Proceedings of the
2022 Conference on Empirical Methods in Natural Language Processing.
Association for Computational Linguistics, Abu Dhabi, United Arab
Emirates, 2532–2544. https://aclanthology.org/2022.emnlp-main.163
[365] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wain-
wright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Kata-
rina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kel-
ton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder,
Paul Christiano, Jan Leike, and Ryan Lowe. 2022.
Training lan-
guage models to follow instructions with human feedback. In Ad-
vances in Neural Information Processing Systems, Alice H. Oh, Alekh
Agarwal, Danielle Belgrave, and Kyunghyun Cho (Eds.).
https:
//openreview.net/forum?id=TG8KACxEON
[366] Leena Paakkari and Orkan Okan. 2020. COVID-19: health literacy
is an underestimated problem. The Lancet Public Health 5, 5 (2020),
e249–e250.
[367] Artidoro Pagnoni, Martin Graciarena, and Yulia Tsvetkov. 2022.
Threat Scenarios and Best Practices to Detect Neural Fake News.
In Proceedings of the 29th International Conference on Computational
Linguistics. International Committee on Computational Linguistics,
Gyeongju, Republic of Korea, 1233–1249. https://aclanthology.org/
2022.coling-1.106
[368] Jeff Z. Pan, Simon Razniewski, Jan-Christoph Kalo, Sneha Singhania,
Jiaoyan Chen, Stefan Dietze, Hajira Jabeen, Janna Omeliyanenko, Wen
Zhang, Matteo Lissandrini, Russa Biswas, Gerard de Melo, Angela
Bonifati, Edlira Vakaj, Mauro Dragoni, and Damien Graux. 2023.
Large Language Models and Knowledge Graphs: Opportunities and
Challenges. arXiv preprint arXiv: 2308.06374 (2023).
[369] Liangming Pan, Xiaobao Wu, Xinyuan Lu, Anh Tuan Luu,
William Yang Wang, Min-Yen Kan, and Preslav Nakov. 2023. Fact-
Checking Complex Claims with Program-Guided Reasoning. In Pro-
ceedings of the 61st Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers). Association for Computa-
tional Linguistics, Toronto, Canada, 6981–7004. https://doi.org/10.
18653/v1/2023.acl-long.386
[370] Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, and
Xindong Wu. 2023. Unifying Large Language Models and Knowledge
Graphs: A Roadmap. arXiv preprint arXiv: 2306.08302 (2023).
[371] Wenjing Pan, Diyi Liu, and Jie Fang. 2021. An examination of fac-
tors contributing to the acceptance of online health misinformation.
Frontiers in psychology 12 (2021), 630268.
[372] Yikang Pan, Liangming Pan, Wenhu Chen, Preslav Nakov, Min-Yen
Kan, and William Yang Wang. 2023. On the Risk of Misinforma-
tion Pollution with Large Language Models. arXiv preprint arXiv:
2305.13661 (2023).
[373] Subhadarshi Panda and Sarah Ita Levitan. 2021. Detecting Multi-
lingual COVID-19 Misinformation on Social Media via Contextual-
ized Embeddings. In Proceedings of the Fourth Workshop on NLP for
Internet Freedom: Censorship, Disinformation, and Propaganda. As-
sociation for Computational Linguistics, Online, 125–129.
https:
//doi.org/10.18653/v1/2021.nlp4if-1.19
[374] E. Papadogiannakis, P. Papadopoulos, Evangelos P. Markatos, and
N. Kourtellis. 2022. Who Funds Misinformation? A Systematic Anal-
ysis of the Ad-related Profit Routines of Fake News sites. The Web
Conference (2022). https://doi.org/10.1145/3543507.3583443
23
[375] Shivam B Parikh and Pradeep K Atrey. 2018. Media-rich fake news
detection: A survey. In 2018 IEEE conference on multimedia information
processing and retrieval (MIPR). IEEE, 436–441.
[376] Peter S. Park, Simon Goldstein, Aidan O’Gara, Michael Chen, and
Dan Hendrycks. 2023. AI Deception: A Survey of Examples, Risks,
and Potential Solutions. arXiv preprint arXiv: 2308.14752 (2023).
[377] Ajay Patel, Nicholas Andrews, and Chris Callison-Burch. 2022. Low-
Resource Authorship Style Transfer: Can Non-Famous Authors Be
Imitated? arXiv preprint arXiv: 2212.08986 (2022).
[378] Ajay Patel, Delip Rao, and Chris Callison-Burch. 2023. Learning
Interpretable Style Embeddings via Prompting LLMs. arXiv preprint
arXiv: 2305.12696 (2023).
[379] Ajeet Ram Pathak, Aditee Mahajan, Keshav Singh, Aishwarya Patil,
and Anusha Nair. 2020. Analysis of techniques for rumor detection
in social media. Procedia Computer Science 167 (2020), 2286–2296.
[380] Parth Patwa, Shivam Sharma, Srinivas PYKL, Vineeth Guptha, Gi-
tanjali Kumari, Md. Shad Akhtar, Asif Ekbal, Amitava Das, and Tan-
moy Chakraborty. 2021. Fighting an Infodemic: COVID-19 Fake
News Dataset. In Combating Online Hostile Posts in Regional Lan-
guages during Emergency Situation - First International Workshop,
CONSTRAINT 2021, Collocated with AAAI 2021, Virtual Event, Feb-
ruary 8, 2021, Revised Selected Papers (Communications in Computer
and Information Science, Vol. 1402), Tanmoy Chakraborty, Kai Shu,
H. Russell Bernard, Huan Liu, and Md. Shad Akhtar (Eds.). Springer,
21–29. https://doi.org/10.1007/978-3-030-73696-5_3
[381] Bohdan M. Pavlyshenko. 2023. Analysis of Disinformation and Fake
News Detection Using Fine-Tuned Large Language Model. arXiv
preprint arXiv: 2309.04704 (2023).
[382] Jessica Paynter, Sarah Luskin-Saxby, Deb Keen, Kathryn Fordyce,
Grace Frost, Christine Imms, Scott Miller, David Trembath, Madonna
Tucker, and Ullrich Ecker. 2019. Evaluation of a template for counter-
ing misinformation—Real-world Autism treatment myth debunking.
PloS one 14, 1 (2019), e0210746.
[383] Kellin Pelrine, Jacob Danovitch, and Reihaneh Rabbany. 2021. The
Surprising Performance of Simple Baselines for Misinformation De-
tection. In Proceedings of the Web Conference 2021 (Ljubljana, Slovenia)
(WWW ’21). Association for Computing Machinery, New York, NY,
USA, 3432–3441. https://doi.org/10.1145/3442381.3450111
[384] Kellin Pelrine, Meilina Reksoprodjo, Caleb Gupta, Joel Christoph, and
Reihaneh Rabbany. 2023. Towards Reliable Misinformation Mitiga-
tion: Generalization, Uncertainty, and GPT-4. arXiv preprint arXiv:
2305.14928 (2023).
[385] Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring,
John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving.
2022. Red Teaming Language Models with Language Models. In
Proceedings of the 2022 Conference on Empirical Methods in Natural
Language Processing. Association for Computational Linguistics, Abu
Dhabi, United Arab Emirates, 3419–3448. https://aclanthology.org/
2022.emnlp-main.225
[386] Ethan Perez, Sam Ringer, Kamil˙
e Lukoši¯
ut˙
e, Karina Nguyen, Edwin
Chen, Scott Heiner, Craig Pettit, Catherine Olsson, Sandipan Kundu,
Saurav Kadavath, Andy Jones, Anna Chen, Benjamin Mann, Brian
Israel, Bryan Seethor, C. McKinnon, C. Olah, Daisong Yan, Daniela
Amodei, Dario Amodei, Dawn Drain, Dustin Li, Eli Tran-Johnson,
G. Khundadze, John Kernion, J. Landis, Jamie Kerr, J. Mueller, Jeey-
oon Hyun, J. Landau, Kamal Ndousse, L. Goldberg, Liane Lovitt,
Martin Lucas, Michael Sellitto, Miranda Zhang, Neerav Kingsland,
Nelson Elhage, Nicholas Joseph, Noem’i Mercado, Nova DasSarma,
Oliver Rausch, Robin Larson, Sam McCandlish, Scott Johnston, S.
Kravec, S. E. Showk, Tamera Lanham, Timothy Telleen-Lawton,
Tom B. Brown, T. Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-
Dodds, Jack Clark, Sam Bowman, Amanda Askell, Roger C. Grosse,
Danny Hernandez, Deep Ganguli, Evan Hubinger, Nicholas Schiefer,
and Jared Kaplan. 2022. Discovering Language Model Behaviors with
Model-Written Evaluations. Annual Meeting of the Association for
Computational Linguistics (2022). https://doi.org/10.48550/arXiv.2212.
09251
[387] Verónica Pérez-Rosas, Bennett Kleinberg, Alexandra Lefevre, and
Rada Mihalcea. 2018. Automatic Detection of Fake News. In Proceed-
ings of the 27th International Conference on Computational Linguistics.
Association for Computational Linguistics, Santa Fe, New Mexico,
USA, 3391–3401. https://aclanthology.org/C18-1287
[388] Roy H Perlis, Kristin Lunz Trujillo, Jon Green, Alauna Safarpour,
James N Druckman, Mauricio Santillana, Katherine Ognyanova, and
David Lazer. 2023. Misinformation, Trust, and Use of Ivermectin and
Hydroxychloroquine for COVID-19. In JAMA Health Forum, Vol. 4.
American Medical Association, e233257–e233257.
[389] Heinrich Peters and Sandra Matz. 2023. Large Language Models Can
Infer Psychological Dispositions of Social Media Users. arXiv preprint
arXiv: 2309.08631 (2023).
[390] Francesco Pierri, Geng Liu, and Stefano Ceri. 2023. ITA-ELECTION-
2022: A multi-platform dataset of social media conversations around
the 2022 Italian general election. arXiv preprint arXiv: 2301.05119
(2023).
[391] Francesco Pierri, Luca Luceri, Nikhil Jindal, and Emilio Ferrara. 2022.
Propaganda and Misinformation on Facebook and Twitter during
the Russian Invasion of Ukraine. Web Science Conference (2022).
https://doi.org/10.1145/3578503.3583597
[392] Kashyap Popat, Subhabrata Mukherjee, Andrew Yates, and Gerhard
Weikum. 2018. DeClarE: Debunking Fake News and False Claims
using Evidence-Aware Deep Learning. In Proceedings of the 2018
Conference on Empirical Methods in Natural Language Processing.
Association for Computational Linguistics, Brussels, Belgium, 22–32.
https://doi.org/10.18653/v1/D18-1003
[393] Piotr Przybyla. 2020.
Capturing the Style of Fake News. In The
Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020,
The Thirty-Second Innovative Applications of Artificial Intelligence
Conference, IAAI 2020, The Tenth AAAI Symposium on Educational
Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA,
February 7-12, 2020. AAAI Press, 490–497. https://aaai.org/ojs/index.
php/AAAI/article/view/5386
[394] Peng Qi, Juan Cao, Xirong Li, Huan Liu, Qiang Sheng, Xiaoyue Mi,
Qin He, Yongbiao Lv, Chenyang Guo, and Yingchao Yu. 2021. Improv-
ing Fake News Detection by Using an Entity-Enhanced Framework
to Fuse Diverse Multimodal Clues. In Proceedings of the 29th ACM
International Conference on Multimedia (Virtual Event, China) (MM
’21). Association for Computing Machinery, New York, NY, USA,
1212–1220. https://doi.org/10.1145/3474085.3481548
[395] Peng Qi, Yuyang Zhao, Yufeng Shen, Wei Ji, Juan Cao, and Tat-Seng
Chua. 2023. Two Heads Are Better Than One: Improving Fake News
Video Detection by Correlating with Neighbors. In Findings of the
Association for Computational Linguistics: ACL 2023. Association for
Computational Linguistics, Toronto, Canada, 11947–11959. https:
//doi.org/10.18653/v1/2023.findings-acl.756
[396] Xiangyu Qi, Kaixuan Huang, Ashwinee Panda, Mengdi Wang, and
Prateek Mittal. 2023. Visual Adversarial Examples Jailbreak Large
Language Models. arXiv preprint arXiv: 2306.13213 (2023).
[397] Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek
Mittal, and Peter Henderson. 2023. Fine-tuning Aligned Language
Models Compromises Safety, Even When Users Do Not Intend To!
arXiv preprint arXiv: 2310.03693 (2023).
[398] Cheng Qian, Chi Han, Yi R. Fung, Yujia Qin, Zhiyuan Liu, and Heng
Ji. 2023. CREATOR: Disentangling Abstract and Concrete Reasonings
of Large Language Models through Tool Creation. arXiv preprint
arXiv: 2305.14318 (2023).
[399] Shengsheng Qian, Jinguang Wang, Jun Hu, Quan Fang, and Chang-
sheng Xu. 2021. Hierarchical Multi-Modal Contextual Attention
24
Network for Fake News Detection. In Proceedings of the 44th In-
ternational ACM SIGIR Conference on Research and Development
in Information Retrieval (Virtual Event, Canada) (SIGIR ’21). Asso-
ciation for Computing Machinery, New York, NY, USA, 153–162.
https://doi.org/10.1145/3404835.3462871
[400] Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao,
Shumin Deng, Chuanqi Tan, Fei Huang, and Huajun Chen. 2023.
Reasoning with Language Model Prompting: A Survey. In Proceed-
ings of the 61st Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers). Association for Computational
Linguistics, Toronto, Canada, 5368–5393. https://doi.org/10.18653/
v1/2023.acl-long.294
[401] Yu Qiao, Daniel Wiechmann, and Elma Kerz. 2020. A Language-
Based Approach to Fake News Detection Through Interpretable
Features and BRNN. In Proceedings of the 3rd International Work-
shop on Rumours and Deception in Social Media (RDSM). Associa-
tion for Computational Linguistics, Barcelona, Spain (Online), 14–31.
https://aclanthology.org/2020.rdsm-1.2
[402] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu
Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Y. Fung,
Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu,
Shi Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bo Li,
Ziwei Tang, Jing Yi, Yu Zhu, Zhenning Dai, Lan Yan, Xin Cong, Ya-
Ting Lu, Weilin Zhao, Yuxiang Huang, Jun-Han Yan, Xu Han, Xian
Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji,
Zhiyuan Liu, and Maosong Sun. 2023. Tool Learning with Foundation
Models. ARXIV.ORG (2023). https://doi.org/10.48550/arXiv.2304.08354
[403] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu,
Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Runchu
Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu,
and Maosong Sun. 2023. ToolLLM: Facilitating Large Language Mod-
els to Master 16000+ Real-world APIs. arXiv preprint arXiv: 2307.16789
(2023).
[404] Huachuan Qiu, Shuai Zhang, Anqi Li, Hongliang He, and Zhenzhong
Lan. 2023. Latent Jailbreak: A Benchmark for Evaluating Text Safety
and Output Robustness of Large Language Models. arXiv preprint
arXiv: 2307.08487 (2023).
[405] Dorian Quelle and Alexandre Bovet. 2023. The Perils & Promises
of Fact-checking with Large Language Models.
arXiv preprint
arXiv:2310.13549 (2023).
[406] P. Ranade, Aritran Piplai, Sudip Mittal, A. Joshi, and Tim Finin. 2021.
Generating Fake Cyber Threat Intelligence Using Transformer-Based
Models. IEEE International Joint Conference on Neural Network (2021).
https://doi.org/10.1109/IJCNN52387.2021.9534192
[407] Aman Rangapur, Haoran Wang, and Kai Shu. 2023. Investigating
Online Financial Misinformation and Its Consequences: A Computa-
tional Perspective. arXiv preprint arXiv: 2309.12363 (2023).
[408] Abhinav Rao, Sachin Vashistha, Atharva Naik, Somak Aditya, and
Monojit Choudhury. 2023. Tricking LLMs into Disobedience: Under-
standing, Analyzing, and Preventing Jailbreaks. arXiv preprint arXiv:
2305.14965 (2023).
[409] Dongning Rao, Xin Miao, Zhihua Jiang, and Ran Li. 2021. STANKER:
Stacking Network based on Level-grained Attention-masked BERT
for Rumor Detection on Social Media. In Proceedings of the 2021
Conference on Empirical Methods in Natural Language Processing.
Association for Computational Linguistics, Online and Punta Cana,
Dominican Republic, 3347–3363. https://doi.org/10.18653/v1/2021.
emnlp-main.269
[410] Hannah Rashkin, Eunsol Choi, Jin Yea Jang, Svitlana Volkova, and
Yejin Choi. 2017. Truth of Varying Shades: Analyzing Language in
Fake News and Political Fact-Checking. In Proceedings of the 2017
Conference on Empirical Methods in Natural Language Processing.
Association for Computational Linguistics, Copenhagen, Denmark,
2931–2937. https://doi.org/10.18653/v1/D17-1317
[411] Alexander Ratner, Stephen H Bach, Henry Ehrenberg, Jason Fries,
Sen Wu, and Christopher Ré. 2017. Snorkel: Rapid training data cre-
ation with weak supervision. In Proceedings of the VLDB Endowment.
International Conference on Very Large Data Bases, Vol. 11. NIH Public
Access, 269.
[412] Maribeth Rauh, John Mellor, Jonathan Uesato, Po-Sen Huang,
Johannes Welbl, Laura Weidinger, Sumanth Dathathri, Amelia
Glaese, Geoffrey Irving, Iason Gabriel, William Isaac, and Lisa Anne
Hendricks. 2022. Characteristics of Harmful Text: Towards Rigorous
Benchmarking of Language Models. In NeurIPS. http://papers.nips.cc/
paper_files/paper/2022/hash/9ca22870ae0ba55ee50ce3e2d269e5de-
Abstract-Datasets_and_Benchmarks.html
[413] Vipula Rawte, Swagata Chakraborty, Agnibh Pathak, Anubhav Sarkar,
S. M Towhidul Islam Tonmoy, Aman Chadha, Amit P. Sheth, and Ami-
tava Das. 2023. The Troubling Emergence of Hallucination in Large
Language Models - An Extensive Definition, Quantification, and Pre-
scriptive Remediations. arXiv preprint arXiv: 2310.04988 (2023).
[414] Vipula Rawte, Amit Sheth, and Amitava Das. 2023. A Survey of
Hallucination in Large Foundation Models. arXiv preprint arXiv:
2309.05922 (2023).
[415] Emily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen, Chris Callison-
Burch, and Jason Wei. 2022. A Recipe for Arbitrary Text Style Transfer
with Large Language Models. In Proceedings of the 60th Annual Meet-
ing of the Association for Computational Linguistics (Volume 2: Short
Papers). Association for Computational Linguistics, Dublin, Ireland,
837–848. https://doi.org/10.18653/v1/2022.acl-short.94
[416] Yuxiang Ren, Bo Wang, Jiawei Zhang, and Yi Chang. 2020. Adversarial
Active Learning Based Heterogeneous Graph Neural Network for
Fake News Detection. In 2020 IEEE International Conference on Data
Mining (ICDM). 452–461. https://doi.org/10.1109/ICDM50108.2020.
00054
[417] Alexander Robey, Eric Wong, Hamed Hassani, and George J. Pap-
pas. 2023. SmoothLLM: Defending Large Language Models Against
Jailbreaking Attacks. arXiv preprint arXiv: 2310.03684 (2023).
[418] Natalia Díaz Rodríguez, J. Ser, M. Coeckelbergh, Marcos L’opez de
Prado, E. Herrera-Viedma, and Francisco Herrera. 2023. Connecting
the Dots in Trustworthy Artificial Intelligence: From AI Principles,
Ethics, and Key Requirements to Responsible AI Systems and Regula-
tion. ARXIV.ORG (2023). https://doi.org/10.48550/arXiv.2305.02231
[419] Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick Esser, and B.
Ommer. 2021. High-Resolution Image Synthesis with Latent Diffusion
Models. Computer Vision and Pattern Recognition (2021).
https:
//doi.org/10.1109/CVPR52688.2022.01042
[420] Victoria Rubin, Niall Conroy, Yimin Chen, and Sarah Cornwell.
2016.
Fake News or Truth? Using Satirical Cues to Detect Po-
tentially Misleading News. In Proceedings of the Second Workshop
on Computational Approaches to Deception Detection. Association
for Computational Linguistics, San Diego, California, 7–17. https:
//doi.org/10.18653/v1/W16-0802
[421] Arkadiy Saakyan and Smaranda Muresan. 2023. ICLEF: In-Context
Learning with Expert Feedback for Explainable Style Transfer. arXiv
preprint arXiv: 2309.08583 (2023).
[422] Vinu Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian,
Wenxiao Wang, and Soheil Feizi. 2023. Can AI-Generated Text be
Reliably Detected? arXiv preprint arXiv: Arxiv-2303.11156 (2023).
[423] Ujala Sajid and Faheem ul Hassan. 2022. ChatGPT and its effect on
Shaping the Future of Medical Writing. Pakistan Journal of Ethics 2,
2 (2022), 38–43.
[424] Mansi Sakarvadia, Aswathy Ajith, Arham Khan, Daniel Grzenda,
Nathaniel Hudson, André Bauer, Kyle Chard, and Ian Foster. 2023.
Memory Injections: Correcting Multi-Hop Reasoning Failures during
Inference in Transformer-Based Language Models. arXiv preprint
arXiv: 2309.05605 (2023).
25
[425] Nikos Salamanos, Pantelitsa Leonidou, Nikolaos Laoutaris, Michael
Sirivianos, Maria Aspri, and Marius Paraschiv. 2023. HyperGraphDis:
Leveraging Hypergraphs for Contextual and Social-Based Disinfor-
mation Detection. arXiv preprint arXiv: 2310.01113 (2023).
[426] Malik Sallam. 2023. The Utility of ChatGPT as an Example of Large
Language Models in Healthcare Education, Research and Practice:
Systematic Review on the Future Perspectives and Potential Limita-
tions. medRxiv (2023), 2023–02.
[427] Emily Saltz, Soubhik Barari, Claire Leibowicz, and Claire Wardle.
2021. Misinformation interventions are common, divisive, and poorly
understood. Harvard Kennedy School (HKS) Misinformation Review 2,
5 (2021).
[428] Emily Saltz, Claire R Leibowicz, and Claire Wardle. 2021. Encounters
with visual misinformation and labels across platforms: An interview
and diary study to inform ecosystem approaches to misinformation
interventions. In Extended Abstracts of the 2021 CHI Conference on
Human Factors in Computing Systems. 1–6.
[429] Mourad Sarrouti, Asma Ben Abacha, Yassine Mrabet, and Dina
Demner-Fushman. 2021. Evidence-based Fact-Checking of Health-
related Claims. In Findings of the Association for Computational Lin-
guistics: EMNLP 2021. Association for Computational Linguistics,
Punta Cana, Dominican Republic, 3499–3512.
https://doi.org/10.
18653/v1/2021.findings-emnlp.297
[430] Dietram A Scheufele, Andrew J Hoffman, Liz Neeley, and Czerne M
Reid. 2021.
Misinformation about science in the public sphere.
Proceedings of the National Academy of Sciences 118, 15 (2021),
e2104068118.
[431] Jonas Schuett. 2019. Defining the scope of AI regulations. arXiv
preprint arXiv: 1909.01095 (2019).
[432] Tal Schuster, Roei Schuster, Darsh J. Shah, and Regina Barzilay. 2020.
The Limitations of Stylometry for Detecting Machine-Generated
Fake News. Computational Linguistics 46, 2 (2020), 499–510. https:
//doi.org/10.1162/coli_a_00380
[433] Stephane Schwarz, Antônio Theóphilo, and Anderson Rocha. 2020.
EMET: Embeddings from Multilingual-Encoder Transformer for Fake
News Detection. In 2020 IEEE International Conference on Acoustics,
Speech and Signal Processing, ICASSP 2020, Barcelona, Spain, May 4-8,
2020. IEEE, 2777–2781. https://doi.org/10.1109/ICASSP40776.2020.
9054673
[434] scmp.com. 2023.
Chinese artificial intelligence firm iFly-
tek blames chatbot-generated article for sudden share price
swing on Shenzhen bourse.
https://www.scmp.com/tech/big-
tech/article/3221953/chinese-artificial-intelligence-firm-iflytek-
blames-chatbot-generated-article-sudden-share-price. Accessed:
2023-09-30.
[435] Elizabeth Seger, Noemi Dreksler, Richard Moulange, Emily Dardaman,
Jonas Schuett, K Wei, Christoph Winter, Mackenzie Arnold, Seán
Ó hÉigeartaigh, Anton Korinek, et al. 2023. Open-Sourcing Highly
Capable Foundation Models: An Evaluation of Risks, Benefits, and
Alternative Methods for Pursuing Open-Source Objectives. (2023).
[436] Vinay Setty and Erlend Rekve. 2020. Truth be Told: Fake News
Detection Using User Reactions on Reddit. In CIKM ’20: The 29th ACM
International Conference on Information and Knowledge Management,
Virtual Event, Ireland, October 19-23, 2020, Mathieu d’Aquin, Stefan
Dietze, Claudia Hauff, Edward Curry, and Philippe Cudré-Mauroux
(Eds.). ACM, 3325–3328. https://doi.org/10.1145/3340531.3417463
[437] Shaden Shaar, Firoj Alam, Giovanni Da San Martino, and Preslav
Nakov. 2022. The Role of Context in Detecting Previously Fact-
Checked Claims. In Findings of the Association for Computational
Linguistics: NAACL 2022. Association for Computational Linguistics,
Seattle, United States, 1619–1631. https://doi.org/10.18653/v1/2022.
findings-naacl.122
[438] Lanyu Shang, Ziyi Kou, Yang Zhang, and Dong Wang. 2022.
A
Duo-Generative Approach to Explainable Multimodal COVID-19
Misinformation Detection. In Proceedings of the ACM Web Con-
ference 2022 (Virtual Event, Lyon, France) (WWW ’22). Associa-
tion for Computing Machinery, New York, NY, USA, 3623–3631.
https://doi.org/10.1145/3485447.3512257
[439] Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan,
and Weizhu Chen. 2023. Enhancing Retrieval-Augmented Large
Language Models with Iterative Retrieval-Generation Synergy. arXiv
preprint arXiv: 2305.15294 (2023).
[440] Karishma Sharma, Feng Qian, He Jiang, Natali Ruchansky, Ming
Zhang, and Yan Liu. 2019. Combating fake news: A survey on identi-
fication and mitigation techniques. ACM Transactions on Intelligent
Systems and Technology (TIST) 10, 3 (2019), 1–42.
[441] Tianhao Shen, Renren Jin, Yufei Huang, Chuang Liu, Weilong Dong,
Zishan Guo, Xinwei Wu, Yan Liu, and Deyi Xiong. 2023. Large Lan-
guage Model Alignment: A Survey. arXiv preprint arXiv: 2309.15025
(2023).
[442] Xudong Shen, Hannah Brown, Jiashu Tao, Martin Strobel, Yao Tong,
Akshay Narayan, Harold Soh, and Finale Doshi-Velez. 2023. Towards
Regulatable AI Systems: Technical Gaps and Policy Opportunities.
arXiv preprint arXiv: 2306.12609 (2023).
[443] Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, and Yang
Zhang. 2023. ""Do Anything Now"": Characterizing and Evaluating
In-The-Wild Jailbreak Prompts on Large Language Models. arXiv
preprint arXiv: 2308.03825 (2023).
[444] Qiang Sheng, Juan Cao, Xueyao Zhang, Rundong Li, Danding Wang,
and Yongchun Zhu. 2022. Zoom Out and Observe: News Environment
Perception for Fake News Detection. In Proceedings of the 60th An-
nual Meeting of the Association for Computational Linguistics (Volume
1: Long Papers). Association for Computational Linguistics, Dublin,
Ireland, 4543–4556. https://doi.org/10.18653/v1/2022.acl-long.311
[445] Qiang Sheng, Xueyao Zhang, Juan Cao, and Lei Zhong. 2021. Integrat-
ing Pattern- and Fact-Based Fake News Detection via Model Prefer-
ence Learning. In Proceedings of the 30th ACM International Conference
on Information & Knowledge Management (Virtual Event, Queensland,
Australia) (CIKM ’21). Association for Computing Machinery, New
York, NY, USA, 1640–1650. https://doi.org/10.1145/3459637.3482440
[446] Toby Shevlane, Sebastian Farquhar, Ben Garfinkel, Mary Phuong,
Jess Whittlestone, Jade Leung, Daniel Kokotajlo, Nahema Marchal,
Markus Anderljung, Noam Kolt, Lewis Ho, Divya Siddarth, Shahar
Avin, Will Hawkins, Been Kim, Iason Gabriel, Vijay Bolina, Jack
Clark, Yoshua Bengio, Paul Christiano, and Allan Dafoe. 2023. Model
evaluation for extreme risks. arXiv preprint arXiv: 2305.15324 (2023).
[447] Zhouxing Shi, Yihan Wang, Fan Yin, Xiangning Chen, Kai-Wei Chang,
and Cho-Jui Hsieh. 2023. Red Teaming Language Model Detectors
with Language Models. arXiv preprint arXiv: 2305.19713 (2023).
[448] Kai Shu, Limeng Cui, Suhang Wang, Dongwon Lee, and Huan Liu.
2019. dEFEND: Explainable Fake News Detection. In Proceedings of the
25th ACM SIGKDD International Conference on Knowledge Discovery
& Data Mining, KDD 2019, Anchorage, AK, USA, August 4-8, 2019,
Ankur Teredesai, Vipin Kumar, Ying Li, Rómer Rosales, Evimaria
Terzi, and George Karypis (Eds.). ACM, 395–405. https://doi.org/10.
1145/3292500.3330935
[449] Kai Shu, Susan Dumais, Ahmed Hassan Awadallah, and Huan Liu.
2020. Detecting fake news with weak social supervision. IEEE Intelli-
gent Systems 36, 4 (2020), 96–103.
[450] Kai Shu, Yichuan Li, Kaize Ding, and Huan Liu. 2021. Fact-Enhanced
Synthetic News Generation. In Thirty-Fifth AAAI Conference on Ar-
tificial Intelligence, AAAI 2021, Thirty-Third Conference on Innova-
tive Applications of Artificial Intelligence, IAAI 2021, The Eleventh
Symposium on Educational Advances in Artificial Intelligence, EAAI
2021, Virtual Event, February 2-9, 2021. AAAI Press, 13825–13833.
https://ojs.aaai.org/index.php/AAAI/article/view/17629
[451] Kai Shu, Amy Sliva, Suhang Wang, Jiliang Tang, and Huan Liu. 2017.
Fake news detection on social media: A data mining perspective.
26
ACM SIGKDD explorations newsletter 19, 1 (2017), 22–36.
[452] Kai Shu, Suhang Wang, Thai Le, Dongwon Lee, and Huan Liu. 2018.
Deep headline generation for clickbait detection. In 2018 IEEE Inter-
national Conference on Data Mining (ICDM). IEEE, 467–476.
[453] Kai Shu, Suhang Wang, and Huan Liu. 2019. Beyond News Contents:
The Role of Social Context for Fake News Detection. In Proceedings
of the Twelfth ACM International Conference on Web Search and Data
Mining, WSDM 2019, Melbourne, VIC, Australia, February 11-15, 2019,
J. Shane Culpepper, Alistair Moffat, Paul N. Bennett, and Kristina Ler-
man (Eds.). ACM, 312–320. https://doi.org/10.1145/3289600.3290994
[454] Kai Shu, Guoqing Zheng, Yichuan Li, Subhabrata Mukherjee,
Ahmed Hassan Awadallah, Scott Ruston, and Huan Liu. 2021. Early
detection of fake news with multi-source weak social supervision. In
Machine Learning and Knowledge Discovery in Databases: European
Conference, ECML PKDD 2020, Ghent, Belgium, September 14–18, 2020,
Proceedings, Part III. Springer, 650–666.
[455] Manli Shu, Jiongxiao Wang, Chen Zhu, Jonas Geiping, Chaowei Xiao,
and Tom Goldstein. 2023. On the Exploitability of Instruction Tuning.
arXiv preprint arXiv: 2306.17194 (2023).
[456] Elena Shushkevich, Mikhail Alexandrov, and John Cardiff. 2023. Im-
proving Multiclass Classification of Fake News Using BERT-Based
Models and ChatGPT-Augmented Data. Inventions 8, 5 (2023), 112.
[457] Amila Silva, Ling Luo, Shanika Karunasekera, and Christopher Leckie.
2021. Embracing Domain Differences in Fake News: Cross-domain
Fake News Detection using Multi-modal Data. In Thirty-Fifth AAAI
Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Confer-
ence on Innovative Applications of Artificial Intelligence, IAAI 2021, The
Eleventh Symposium on Educational Advances in Artificial Intelligence,
EAAI 2021, Virtual Event, February 2-9, 2021. AAAI Press, 557–565.
https://ojs.aaai.org/index.php/AAAI/article/view/16134
[458] Irene Solaiman, Zeerak Talat, William Agnew, Lama Ahmad, Dylan
Baker, Su Lin Blodgett, Hal Daumé III, Jesse Dodge, Ellie Evans, Sara
Hooker, Yacine Jernite, Alexandra Sasha Luccioni, Alberto Lusoli,
Margaret Mitchell, Jessica Newman, Marie-Therese Png, Andrew
Strait, and Apostol Vassilev. 2023. Evaluating the Social Impact of
Generative AI Systems in Systems and Society. arXiv preprint arXiv:
2306.05949 (2023).
[459] Chenguang Song, Kai Shu, and Bin Wu. 2021. Temporally evolving
graph neural network for fake news detection. Information Processing
& Management 58, 6 (2021), 102712.
[460] Changhe Song, Cheng Yang, Huimin Chen, Cunchao Tu, Zhiyuan
Liu, and Maosong Sun. 2021. CED: Credible Early Detection of Social
Media Rumors. IEEE Transactions on Knowledge and Data Engineering
33, 8 (2021), 3035–3047. https://doi.org/10.1109/TKDE.2019.2961675
[461] Yun-Zhu Song, Yi-Syuan Chen, Yi-Ting Chang, Shao-Yu Weng, and
Hong-Han Shuai. 2021. Adversary-Aware Rumor Detection. In Find-
ings of the Association for Computational Linguistics: ACL-IJCNLP
2021. Association for Computational Linguistics, Online, 1371–1382.
https://doi.org/10.18653/v1/2021.findings-acl.118
[462] Damiano Spina, Mark Sanderson, Daniel Angus, Gianluca Demartini,
Dana Mckay, Lauren L Saling, and Ryen W White. 2023. Human-AI
Cooperation to Tackle Misinformation and Polarization. Commun.
ACM 66, 7 (2023), 40–45.
[463] Giovanni Spitale, Nikola Biller-Andorno, and Federico Germani. 2023.
AI model GPT-3 (dis)informs us better than humans. Science Ad-
vances 9, 26 (2023), eadh1850. https://doi.org/10.1126/sciadv.adh1850
arXiv:https://www.science.org/doi/pdf/10.1126/sciadv.adh1850
[464] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md
Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam San-
toro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor
Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex
Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang,
Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda
Dsouza, Ambrose Slone, Ameet Rahane, Anantharaman S. Iyer, An-
ders Johan Andreassen, Andrea Madotto, Andrea Santilli, Andreas
Stuhlmüller, Andrew M. Dai, Andrew La, Andrew Lampinen, Andy
Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna
Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi,
Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov,
Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla
Karakaş, B. Ryan Roberts, Bao Sheng Loe, Barret Zoph, Bartłomiej Bo-
janowski, Batuhan Özyurt, Behnam Hedayatnia, Behnam Neyshabur,
Benjamin Inden, Benno Stein, Berk Ekmekci, Bill Yuchen Lin, Blake
Howald, Bryan Orinion, Cameron Diao, Cameron Dour, Cather-
ine Stinson, Cedrick Argueta, Cesar Ferri, Chandan Singh, Charles
Rathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu, Chris Callison-
Burch, Christopher Waites, Christian Voigt, Christopher D Manning,
Christopher Potts, Cindy Ramirez, Clara E. Rivera, Clemencia Siro,
Colin Raffel, Courtney Ashcraft, Cristina Garbacea, Damien Sileo,
Dan Garrette, Dan Hendrycks, Dan Kilman, Dan Roth, C. Daniel
Freeman, Daniel Khashabi, Daniel Levy, Daniel Moseguí González,
Danielle Perszyk, Danny Hernandez, Danqi Chen, Daphne Ippolito,
Dar Gilboa, David Dohan, David Drakard, David Jurgens, Debajyoti
Datta, Deep Ganguli, Denis Emelin, Denis Kleyko, Deniz Yuret, Derek
Chen, Derek Tam, Dieuwke Hupkes, Diganta Misra, Dilyar Buzan,
Dimitri Coelho Mollo, Diyi Yang, Dong-Ho Lee, Dylan Schrader, Eka-
terina Shutova, Ekin Dogus Cubuk, Elad Segal, Eleanor Hagerman,
Elizabeth Barnes, Elizabeth Donoway, Ellie Pavlick, Emanuele Rodolà,
Emma Lam, Eric Chu, Eric Tang, Erkut Erdem, Ernie Chang, Ethan A
Chi, Ethan Dyer, Ethan Jerzak, Ethan Kim, Eunice Engefu Manyasi, Ev-
genii Zheltonozhskii, Fanyue Xia, Fatemeh Siar, Fernando Martínez-
Plumed, Francesca Happé, Francois Chollet, Frieda Rong, Gaurav
Mishra, Genta Indra Winata, Gerard de Melo, Germán Kruszewski, Gi-
ambattista Parascandolo, Giorgio Mariani, Gloria Xinyue Wang, Gon-
zalo Jaimovitch-Lopez, Gregor Betz, Guy Gur-Ari, Hana Galijasevic,
Hannah Kim, Hannah Rashkin, Hannaneh Hajishirzi, Harsh Mehta,
Hayden Bogar, Henry Francis Anthony Shevlin, Hinrich Schuetze,
Hiromu Yakura, Hongming Zhang, Hugh Mee Wong, Ian Ng, Isaac No-
ble, Jaap Jumelet, Jack Geissinger, Jackson Kernion, Jacob Hilton, Jae-
hoon Lee, Jaime Fernández Fisac, James B Simon, James Koppel, James
Zheng, James Zou, Jan Kocon, Jana Thompson, Janelle Wingfield,
Jared Kaplan, Jarema Radom, Jascha Sohl-Dickstein, Jason Phang, Ja-
son Wei, Jason Yosinski, Jekaterina Novikova, Jelle Bosscher, Jennifer
Marsh, Jeremy Kim, Jeroen Taal, Jesse Engel, Jesujoba Alabi, Jiacheng
Xu, Jiaming Song, Jillian Tang, Joan Waweru, John Burden, John
Miller, John U. Balis, Jonathan Batchelder, Jonathan Berant, Jörg Fro-
hberg, Jos Rozen, Jose Hernandez-Orallo, Joseph Boudeman, Joseph
Guerr, Joseph Jones, Joshua B. Tenenbaum, Joshua S. Rule, Joyce
Chua, Kamil Kanclerz, Karen Livescu, Karl Krauth, Karthik Gopalakr-
ishnan, Katerina Ignatyeva, Katja Markert, Kaustubh Dhole, Kevin
Gimpel, Kevin Omondi, Kory Wallace Mathewson, Kristen Chiafullo,
Ksenia Shkaruta, Kumar Shridhar, Kyle McDonell, Kyle Richardson,
Laria Reynolds, Leo Gao, Li Zhang, Liam Dugan, Lianhui Qin, Lidia
Contreras-Ochando, Louis-Philippe Morency, Luca Moschella, Lu-
cas Lam, Lucy Noble, Ludwig Schmidt, Luheng He, Luis Oliveros-
Colón, Luke Metz, Lütfi Kerem Senel, Maarten Bosma, Maarten
Sap, Maartje Ter Hoeve, Maheen Farooqi, Manaal Faruqui, Mantas
Mazeika, Marco Baturan, Marco Marelli, Marco Maru, Maria Jose
Ramirez-Quintana, Marie Tolkiehn, Mario Giulianelli, Martha Lewis,
Martin Potthast, Matthew L Leavitt, Matthias Hagen, Mátyás Schu-
bert, Medina Orduna Baitemirova, Melody Arnaud, Melvin McElrath,
Michael Andrew Yee, Michael Cohen, Michael Gu, Michael Ivanit-
skiy, Michael Starritt, Michael Strube, Michał Swędrowski, Michele
Bevilacqua, Michihiro Yasunaga, Mihir Kale, Mike Cain, Mimee Xu,
Mirac Suzgun, Mitch Walker, Mo Tiwari, Mohit Bansal, Moin Amin-
naseri, Mor Geva, Mozhdeh Gheini, Mukund Varma T, Nanyun Peng,
Nathan Andrew Chi, Nayeon Lee, Neta Gur-Ari Krakover, Nicholas
27
Cameron, Nicholas Roberts, Nick Doiron, Nicole Martinez, Nikita
Nangia, Niklas Deckers, Niklas Muennighoff, Nitish Shirish Keskar,
Niveditha S. Iyer, Noah Constant, Noah Fiedel, Nuan Wen, Oliver
Zhang, Omar Agha, Omar Elbaghdadi, Omer Levy, Owain Evans,
Pablo Antonio Moreno Casares, Parth Doshi, Pascale Fung, Paul Pu
Liang, Paul Vicol, Pegah Alipoormolabashi, Peiyuan Liao, Percy Liang,
Peter W Chang, Peter Eckersley, Phu Mon Htut, Pinyu Hwang, Piotr
Miłkowski, Piyush Patil, Pouya Pezeshkpour, Priti Oli, Qiaozhu Mei,
Qing Lyu, Qinlang Chen, Rabin Banjade, Rachel Etta Rudolph, Raefer
Gabriel, Rahel Habacker, Ramon Risco, Raphaël Millière, Rhythm
Garg, Richard Barnes, Rif A. Saurous, Riku Arakawa, Robbe Ray-
maekers, Robert Frank, Rohan Sikand, Roman Novak, Roman Sitelew,
Ronan Le Bras, Rosanne Liu, Rowan Jacobs, Rui Zhang, Russ Salakhut-
dinov, Ryan Andrew Chi, Seungjae Ryan Lee, Ryan Stovall, Ryan Tee-
han, Rylan Yang, Sahib Singh, Saif M. Mohammad, Sajant Anand, Sam
Dillavou, Sam Shleifer, Sam Wiseman, Samuel Gruetter, Samuel R.
Bowman, Samuel Stern Schoenholz, Sanghyun Han, Sanjeev Kwa-
tra, Sarah A. Rous, Sarik Ghazarian, Sayan Ghosh, Sean Casey, Se-
bastian Bischoff, Sebastian Gehrmann, Sebastian Schuster, Sepideh
Sadeghi, Shadi Hamdan, Sharon Zhou, Shashank Srivastava, Sherry
Shi, Shikhar Singh, Shima Asaadi, Shixiang Shane Gu, Shubh Pachchi-
gar, Shubham Toshniwal, Shyam Upadhyay, Shyamolima Shammie
Debnath, Siamak Shakeri, Simon Thormeyer, Simone Melzi, Siva
Reddy, Sneha Priscilla Makini, Soo-Hwan Lee, Spencer Torene, Sri-
harsha Hatwar, Stanislas Dehaene, Stefan Divic, Stefano Ermon,
Stella Biderman, Stephanie Lin, Stephen Prasad, Steven Piantadosi,
Stuart Shieber, Summer Misherghi, Svetlana Kiritchenko, Swaroop
Mishra, Tal Linzen, Tal Schuster, Tao Li, Tao Yu, Tariq Ali, Tat-
sunori Hashimoto, Te-Lin Wu, Théo Desbordes, Theodore Rothschild,
Thomas Phan, Tianle Wang, Tiberius Nkinyili, Timo Schick, Timo-
fei Kornev, Titus Tunduny, Tobias Gerstenberg, Trenton Chang, Tr-
ishala Neeraj, Tushar Khot, Tyler Shultz, Uri Shaham, Vedant Misra,
Vera Demberg, Victoria Nyamai, Vikas Raunak, Vinay Venkatesh
Ramasesh, vinay uday prabhu, Vishakh Padmakumar, Vivek Sriku-
mar, William Fedus, William Saunders, William Zhang, Wout Vossen,
Xiang Ren, Xiaoyu Tong, Xinran Zhao, Xinyi Wu, Xudong Shen,
Yadollah Yaghoobzadeh, Yair Lakretz, Yangqiu Song, Yasaman Bahri,
Yejin Choi, Yichi Yang, Yiding Hao, Yifu Chen, Yonatan Belinkov, Yu
Hou, Yufang Hou, Yuntao Bai, Zachary Seid, Zhuoye Zhao, Zijian
Wang, Zijie J. Wang, Zirui Wang, and Ziyi Wu. 2023. Beyond the
Imitation Game: Quantifying and extrapolating the capabilities of
language models. Transactions on Machine Learning Research (2023).
https://openreview.net/forum?id=uyTL5Bvosj
[465] Logan Stapleton, Jordan Taylor, Sarah Fox, Tongshuang Wu, and
Haiyi Zhu. 2023. Seeing Seeds Beyond Weeds: Green Teaming Gener-
ative AI for Beneficial Uses. arXiv preprint arXiv: 2306.03097 (2023).
[466] Kate Starbird, Renée DiResta, and Matt DeButts. 2023. Influence
and improvisation: Participatory disinformation during the 2020 US
election. Social Media+ Society 9, 2 (2023), 20563051231177943.
[467] Harald Stiff and Fredrik Johansson. 2022.
Detecting computer-
generated disinformation. International Journal of Data Science and
Analytics 13, 4 (2022), 363–383.
[468] Jinyan Su, Terry Yue Zhuo, Jonibek Mansurov, Di Wang, and Preslav
Nakov. 2023. Fake News Detectors are Biased against Texts Generated
by Large Language Models. arXiv preprint arXiv: 2309.08674 (2023).
[469] Jinyan Su, Terry Yue Zhuo, Di Wang, and Preslav Nakov. 2023. De-
tectLLM: Leveraging Log Rank Information for Zero-Shot Detection
of Machine-Generated Text. arXiv preprint arXiv: 2306.05540 (2023).
[470] Xing Su, Jian Yang, Jia Wu, and Zitai Qiu. 2023. Hy-DeFake: Hyper-
graph Neural Networks for Detecting Fake News in Online Social
Networks. arXiv preprint arXiv: 2309.02692 (2023).
[471] Xing Su, Jian Yang, Jia Wu, and Yuchen Zhang. 2023. Mining User-
aware Multi-Relations for Fake News Detection in Large Scale Online
Social Networks. In Proceedings of the Sixteenth ACM International
Conference on Web Search and Data Mining. 51–59.
[472] Ilia Sucholutsky, Lukas Muttenthaler, Adrian Weller, Andi Peng, An-
dreea Bobu, Been Kim, Bradley C. Love, Erin Grant, Iris Groen, Jascha
Achterberg, Joshua B. Tenenbaum, Katherine M. Collins, Kather-
ine L. Hermann, Kerem Oktar, Klaus Greff, Martin N. Hebart, Nori
Jacoby, Qiuyi Zhang, Raja Marjieh, Robert Geirhos, Sherol Chen, Si-
mon Kornblith, Sunayana Rane, Talia Konkle, Thomas P. O’Connell,
Thomas Unterthiner, Andrew K. Lampinen, Klaus-Robert Müller,
Mariya Toneva, and Thomas L. Griffiths. 2023. Getting aligned on
representational alignment. arXiv preprint arXiv: 2310.13018 (2023).
[473] Kai Sun, Yifan Ethan Xu, Hanwen Zha, Yue Liu, and Xin Luna Dong.
2023. Head-to-Tail: How Knowledgeable are Large Language Models
(LLM)? A.K.A. Will LLMs Replace Knowledge Graphs? arXiv preprint
arXiv: 2308.10168 (2023).
[474] Mengzhu Sun, Xi Zhang, Jianqiang Ma, and Yazheng Liu. 2021. In-
consistency Matters: A Knowledge-guided Dual-inconsistency Net-
work for Multi-modal Rumor Detection. In Findings of the Association
for Computational Linguistics: EMNLP 2021. Association for Compu-
tational Linguistics, Punta Cana, Dominican Republic, 1412–1423.
https://doi.org/10.18653/v1/2021.findings-emnlp.122
[475] Mengzhu Sun, Xi Zhang, Jiaqi Zheng, and Guixiang Ma. 2022.
DDGCN: Dual Dynamic Graph Convolutional Networks for Rumor
Detection on Social Media. In Thirty-Sixth AAAI Conference on Artifi-
cial Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative
Applications of Artificial Intelligence, IAAI 2022, The Twelveth Sym-
posium on Educational Advances in Artificial Intelligence, EAAI 2022
Virtual Event, February 22 - March 1, 2022. AAAI Press, 4611–4619.
https://ojs.aaai.org/index.php/AAAI/article/view/20385
[476] Tiening Sun, Zhong Qian, Sujun Dong, Peifeng Li, and Qiaoming
Zhu. 2022. Rumor Detection on Social Media with Graph Adversarial
Contrastive Learning. In Proceedings of the ACM Web Conference 2022
(Virtual Event, Lyon, France) (WWW ’22). Association for Computing
Machinery, New York, NY, USA, 2789–2797. https://doi.org/10.1145/
3485447.3511999
[477] Yanshen Sun, Jianfeng He, Shuo Lei, Limeng Cui, and Chang-Tien
Lu. 2023. Med-MMHL: A Multi-Modal Dataset for Detecting Human-
and LLM-Generated Misinformation in the Medical Domain. arXiv
preprint arXiv: 2306.08871 (2023).
[478] S Suryavardan, Shreyash Mishra, Megha Chakraborty, Parth Patwa,
Anku Rani, Aman Chadha, Aishwarya Reganti, Amitava Das, Amit
Sheth, Manoj Chinnakotla, Asif Ekbal, and Srijan Kumar. 2023. Find-
ings of Factify 2: Multimodal Fake News Detection. arXiv preprint
arXiv: 2307.10475 (2023).
[479] S Suryavardan, Shreyash Mishra, Parth Patwa, Megha Chakraborty,
Anku Rani, Aishwarya Reganti, Aman Chadha, Amitava Das, Amit
Sheth, Manoj Chinnakotla, Asif Ekbal, and Srijan Kumar. 2023. Factify
2: A Multimodal Fake News and Satire News Dataset. arXiv preprint
arXiv: 2304.03897 (2023).
[480] Briony Swire-Thompson, Joseph DeGutis, and David Lazer. 2020.
Searching for the backfire effect: Measurement and design considera-
tions. Journal of applied research in memory and cognition 9, 3 (2020),
286–299.
[481] Reuben Tan, Bryan Plummer, and Kate Saenko. 2020. Detecting Cross-
Modal Inconsistency to Defend Against Neural Fake News. In Proceed-
ings of the 2020 Conference on Empirical Methods in Natural Language
Processing (EMNLP). Association for Computational Linguistics, On-
line, 2081–2106. https://doi.org/10.18653/v1/2020.emnlp-main.163
[482] Yiming Tan, Dehai Min, Yu Li, Wenbo Li, Nan Hu, Yongrui Chen, and
Guilin Qi. 2023. Evaluation of ChatGPT as a Question Answering
System for Answering Complex Questions. arXiv preprint arXiv:
Arxiv-2303.07992 (2023).
[483] Ruixiang Tang, Yu-Neng Chuang, and Xia Hu. 2023. The Science of
Detecting LLM-Generated Texts. (2023). anonymous preprint under
review.
28
[484] Eshaan Tanwar, Subhabrata Dutta, Manish Borthakur, and Tanmoy
Chakraborty. 2023. Multilingual LLMs are Better Cross-lingual In-
context Learners with Alignment. In Proceedings of the 61st Annual
Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, Anna Rogers,
Jordan L. Boyd-Graber, and Naoaki Okazaki (Eds.). Association for
Computational Linguistics, 6292–6307.
https://aclanthology.org/
2023.acl-long.346
[485] Yla R Tausczik and James W Pennebaker. 2010. The psychological
meaning of words: LIWC and computerized text analysis methods.
Journal of language and social psychology 29, 1 (2010), 24–54.
[486] Yuchuan Tian, Hanting Chen, Xutao Wang, Zheyuan Bai, Qinghua
Zhang, Ruifeng Li, Chao Xu, and Yunhe Wang. 2023. Multiscale
Positive-Unlabeled Detection of AI-Generated Texts. arXiv preprint
arXiv: 2305.18149 (2023).
[487] Ehsan Toreini, Mhairi Aitken, Kovila P. L. Coopamootoo, Karen El-
liott, Vladimiro Gonzalez Zelaya, Paolo Missier, Magdalene Ng, and
Aad van Moorsel. 2020. Technologies for Trustworthy Machine Learn-
ing: A Survey in a Socio-Technical Context. arXiv preprint arXiv:
2007.08911 (2020).
[488] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Alma-
hairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal
Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes,
Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami,
Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann,
Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut
Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier
Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie,
Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,
Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian,
Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xi-
ang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, An-
gela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez,
Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2:
Open Foundation and Fine-Tuned Chat Models. arXiv preprint arXiv:
2307.09288 (2023).
[489] Robert Trager, Ben Harack, Anka Reuel, Allison Carnegie, Lennart
Heim, Lewis Ho, Sarah Kreps, Ranjit Lall, Owen Larter, Seán Ó
hÉigeartaigh, Simon Staffell, and José Jaime Villalobos. 2023. In-
ternational Governance of Civilian AI: A Jurisdictional Certification
Approach. arXiv preprint arXiv: 2308.15514 (2023).
[490] Milena Trajanoska, Riste Stojanov, and Dimitar Trajanov. 2023. En-
hancing Knowledge Graph Construction Using Large Language Mod-
els. arXiv preprint arXiv: 2305.04676 (2023).
[491] Isaac Triguero, Daniel Molina, Javier Poyatos, Javier Del Ser, and
Francisco Herrera. 2023. General Purpose Artificial Intelligence Sys-
tems (GPAIS): Properties, Definition, Taxonomy, Open Challenges
and Implications. arXiv preprint arXiv: 2307.14283 (2023).
[492] Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani,
Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra,
Clémentine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanse-
viero, Alexander M. Rush, and Thomas Wolf. 2023. Zephyr: Direct
Distillation of LM Alignment. arXiv preprint arXiv: 2310.16944 (2023).
[493] Jacob Tyo, Bhuwan Dhingra, and Z. Lipton. 2022.
On the State
of the Art in Authorship Attribution and Authorship Verification.
ARXIV.ORG (2022). https://doi.org/10.48550/arXiv.2209.06869
[494] Adaku Uchendu, Thai Le, and Dongwon Lee. 2023. Attribution and
Obfuscation of Neural Text Authorship: A Data Mining Perspective.
ACM SIGKDD Explorations Newsletter 25, 1 (2023), 1–18.
[495] Adaku Uchendu, Jooyoung Lee, Hua Shen, Thai Le, Ting-Hao ’Ken-
neth’ Huang, and Dongwon Lee. 2023. Understanding Individual and
Team-based Human Factors in Detecting Deepfake Texts. ARXIV.ORG
(2023). https://doi.org/10.48550/arXiv.2304.01002
[496] AR Ullah, Anupam Das, Anik Das, Muhammad Ashad Kabir, and Kai
Shu. 2021. A survey of covid-19 misinformation: Datasets, detection
techniques and open issues. ArXiv preprint abs/2110.00737 (2021).
https://arxiv.org/abs/2110.00737
[497] Rishabh Upadhyay, Gabriella Pasi, and Marco Viviani. 2023. Lever-
aging Socio-Contextual Information in BERT for Fake Health News
Detection in Social Media. In Proceedings of the 3rd International
Workshop on Open Challenges in Online Social Networks (Rome, Italy)
(OASIS ’23). Association for Computing Machinery, New York, NY,
USA, 38–46. https://doi.org/10.1145/3599696.3612902
[498] Vaibhav Vaibhav, Raghuram Mandyam, and Eduard Hovy. 2019. Do
Sentence Interactions Matter? Leveraging Sentence Level Represen-
tations for Fake News Classification. In Proceedings of the Thirteenth
Workshop on Graph-Based Methods for Natural Language Processing
(TextGraphs-13). Association for Computational Linguistics, Hong
Kong, 134–139. https://doi.org/10.18653/v1/D19-5316
[499] Sander van der Linden. 2022. Misinformation: susceptibility, spread,
and interventions to immunize the public. Nature Medicine 28, 3
(2022), 460–467.
[500] Francielle Vargas, Fabrício Benevenuto, and Thiago Pardo. 2021.
Toward Discourse-Aware Models for Multilingual Fake News De-
tection. In Proceedings of the Student Research Workshop Associ-
ated with RANLP 2021. INCOMA Ltd., Online, 210–218.
https:
//aclanthology.org/2021.ranlp-srw.29
[501] Neeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, and
Dong Yu. 2023. A Stitch in Time Saves Nine: Detecting and Mitigating
Hallucinations of LLMs by Validating Low-Confidence Generation.
arXiv preprint arXiv: 2307.03987 (2023).
[502] Christoforos Vasilatos, Manaar Alam, Talal Rahwan, Yasir Zaki, and
Michail Maniatakos. 2023. HowkGPT: Investigating the Detection of
ChatGPT-generated University Student Homework through Context-
Aware Perplexity Analysis. arXiv preprint arXiv: 2305.18226 (2023).
[503] Nikhita Vedula and Srinivasan Parthasarathy. 2021. FACE-KEG: Fact
Checking Explained Using KnowledgE Graphs. In Proceedings of the
14th ACM International Conference on Web Search and Data Min-
ing (Virtual Event, Israel) (WSDM ’21). Association for Computing
Machinery, New York, NY, USA, 526–534. https://doi.org/10.1145/
3437963.3441828
[504] Vivek Verma, Eve Fleisig, Nicholas Tomlin, and Dan Klein. 2023.
Ghostbuster: Detecting Text Ghostwritten by Large Language Models.
arXiv preprint arXiv: 2305.15047 (2023).
[505] Prashanth Vijayaraghavan and Soroush Vosoughi. 2022. TWEET-
SPIN: Fine-grained Propaganda Detection in Social Media Using
Multi-View Representations. In Proceedings of the 2022 Conference of
the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies. Association for Com-
putational Linguistics, Seattle, United States, 3433–3448.
https:
//doi.org/10.18653/v1/2022.naacl-main.251
[506] Juraj Vladika and F. Matthes. 2023. Scientific Fact-Checking: A Survey
of Resources and Approaches. Annual Meeting of the Association for
Computational Linguistics (2023). https://doi.org/10.48550/arXiv.2305.
16859
[507] Nguyen Vo and Kyumin Lee. 2020. Where Are the Facts? Searching
for Fact-checked Information to Alleviate the Spread of Fake News.
In Proceedings of the 2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP). Association for Computational Lin-
guistics, Online, 7717–7731. https://doi.org/10.18653/v1/2020.emnlp-
main.621
[508] Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason
Wei, Chris Tar, Yun-Hsuan Sung, Denny Zhou, Quoc Le, and Thang
Luong. 2023. FreshLLMs: Refreshing Large Language Models with
Search Engine Augmentation. arXiv preprint arXiv: 2310.03214 (2023).
29
[509] Nathan Walter, John J Brooks, Camille J Saucier, and Sapna Suresh.
2021. Evaluating the impact of attempts to correct health misinfor-
mation on social media: A meta-analysis. Health Communication 36,
13 (2021), 1776–1784.
[510] Nathan Walter and Sheila T Murphy. 2018. How to unring the bell:
A meta-analytic approach to correction of misinformation. Commu-
nication Monographs 85, 3 (2018), 423–441.
[511] Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang,
Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer,
Sang T. Truong, Simran Arora, Mantas Mazeika, Dan Hendrycks,
Zinan Lin, Yu Cheng, Sanmi Koyejo, Dawn Song, and Bo Li. 2023.
DecodingTrust: A Comprehensive Assessment of Trustworthiness in
GPT Models. arXiv preprint arXiv: 2306.11698 (2023).
[512] Cunxiang Wang, Xiaoze Liu, Yuanhao Yue, Xiangru Tang, Tianhang
Zhang, Cheng Jiayang, Yunzhi Yao, Wenyang Gao, Xuming Hu, Zehan
Qi, Yidong Wang, Linyi Yang, Jindong Wang, Xing Xie, Zheng Zhang,
and Yue Zhang. 2023. Survey on Factuality in Large Language Models:
Knowledge, Retrieval and Domain-Specificity. arXiv preprint arXiv:
2310.07521 (2023).
[513] Gengyu Wang, Kate Harwood, Lawrence Chillrud, Amith Ananthram,
Melanie Subbiah, and Kathleen McKeown. 2023. Check-COVID: Fact-
Checking COVID-19 News Claims with Scientific Evidence. arXiv
preprint arXiv: 2305.18265 (2023).
[514] Gengyu Wang, Kate Harwood, Lawrence Chillrud, Amith Ananthram,
Melanie Subbiah, and Kathleen McKeown. 2023. Check-COVID:
Fact-Checking COVID-19 News Claims with Scientific Evidence. In
Findings of the Association for Computational Linguistics: ACL 2023.
Association for Computational Linguistics, Toronto, Canada, 14114–
14127. https://doi.org/10.18653/v1/2023.findings-acl.888
[515] Haoran Wang, Yingtong Dou, Canyu Chen, Lichao Sun, Philip S. Yu,
and Kai Shu. 2023. Attacking Fake News Detectors via Manipulating
News Social Engagement. The Web Conference (2023). https://doi.
org/10.1145/3543507.3583868
[516] Haoran Wang and Kai Shu. 2023. Explainable Claim Verification
via Knowledge-Grounded Reasoning with Large Language Models.
arXiv preprint arXiv: 2310.05253 (2023).
[517] Jiaming Wang, Zhihao Du, Qian Chen, Yunfei Chu, Zhifu Gao, Zerui
Li, Kai Hu, Xiaohuan Zhou, Jin Xu, Ziyang Ma, Wen Wang, Siqi Zheng,
Chang Zhou, Zhijie Yan, and Shiliang Zhang. 2023. LauraGPT: Listen,
Attend, Understand, and Regenerate Audio with GPT. arXiv preprint
arXiv: 2310.04673 (2023).
[518] Jia Wang, Min Gao, Yinqiu Huang, Kai Shu, and Hualing Yi. 2023.
FinD: Fine-grained discrepancy-based fake news detection enhanced
by event abstract generation. Computer Speech & Language 78 (2023),
101461.
[519] Lei Wang, Chengbang Ma, Xueyang Feng, Zeyu Zhang, Hao ran Yang,
Jingsen Zhang, Zhi-Yang Chen, Jiakai Tang, Xu Chen, Yankai Lin,
Wayne Xin Zhao, Zhewei Wei, and Ji rong Wen. 2023. A Survey on
Large Language Model based Autonomous Agents. ArXiv preprint
abs/2308.11432 (2023). https://arxiv.org/abs/2308.11432
[520] Lean Wang, Wenkai Yang, Deli Chen, Hao Zhou, Yankai Lin, Fandong
Meng, Jie Zhou, and Xu Sun. 2023. Towards Codable Text Water-
marking for Large Language Models. arXiv preprint arXiv: 2307.15992
(2023).
[521] Ruize Wang, Duyu Tang, Nan Duan, Wanjun Zhong, Zhongyu Wei,
Xuanjing Huang, Daxin Jiang, and Ming Zhou. 2020. Leveraging
Declarative Knowledge in Text and First-Order Logic for Fine-Grained
Propaganda Detection. In Proceedings of the 2020 Conference on Em-
pirical Methods in Natural Language Processing (EMNLP). Associ-
ation for Computational Linguistics, Online, 3895–3903.
https:
//doi.org/10.18653/v1/2020.emnlp-main.320
[522] Tao Wang, Yushu Zhang, Shuren Qi, Ruoyu Zhao, Zhihua Xia, and
Jian Weng. 2023. Security and Privacy on Generative Data in AIGC:
A Survey. arXiv preprint arXiv: 2309.09435 (2023).
[523] Wenxuan Wang, Zhaopeng Tu, Chang Chen, Youliang Yuan, Jen tse
Huang, Wenxiang Jiao, and Michael R. Lyu. 2023. All Languages
Matter: On the Multilingual Safety of Large Language Models. arXiv
preprint arXiv: 2310.00905 (2023).
[524] Yuxia Wang, Haonan Li, Xudong Han, Preslav Nakov, and Timothy
Baldwin. 2023. Do-Not-Answer: A Dataset for Evaluating Safeguards
in LLMs. arXiv preprint arXiv: 2308.13387 (2023).
[525] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu
Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, Sen Xing, Guo
Chen, Junting Pan, Jiashuo Yu, Yali Wang, Limin Wang, and Yu Qiao.
2022. InternVideo: General Video Foundation Models via Generative
and Discriminative Learning. arXiv preprint arXiv: 2212.03191 (2022).
[526] Yile Wang, Peng Li, Maosong Sun, and Yang Liu. 2023. Self-Knowledge
Guided Retrieval Augmentation for Large Language Models. arXiv
preprint arXiv: 2310.05002 (2023).
[527] Yaqing Wang, Fenglong Ma, Haoyu Wang, Kishlay Jha, and Jing Gao.
2021. Multimodal Emergent Fake News Detection via Meta Neural
Process Networks. In Proceedings of the 27th ACM SIGKDD Conference
on Knowledge Discovery & Data Mining (Virtual Event, Singapore)
(KDD ’21). Association for Computing Machinery, New York, NY,
USA, 3708–3716. https://doi.org/10.1145/3447548.3467153
[528] Yuxia Wang, Jonibek Mansurov, Petar Ivanov, Jinyan Su, Artem
Shelmanov, Akim Tsvigun, Chenxi Whitehouse, Osama Mohammed
Afzal, Tarek Mahmoud, Alham Fikri Aji, and Preslav Nakov. 2023.
M4: Multi-generator, Multi-domain, and Multi-lingual Black-Box
Machine-Generated Text Detection. arXiv preprint arXiv: 2305.14902
(2023).
[529] Yuntao Wang, Yanghe Pan, Miao Yan, Zhou Su, and Tom H. Luan.
2023. A Survey on ChatGPT: AI-Generated Contents, Challenges,
and Solutions. IEEE Open Journal of the Computer Society (2023), 1–20.
https://doi.org/10.1109/OJCS.2023.3300321
[530] Yaqing Wang, Weifeng Yang, Fenglong Ma, Jin Xu, Bin Zhong, Qiang
Deng, and Jing Gao. 2020. Weak Supervision for Fake News Detection
via Reinforcement Learning. In The Thirty-Fourth AAAI Conference
on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative
Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth
AAAI Symposium on Educational Advances in Artificial Intelligence,
EAAI 2020, New York, NY, USA, February 7-12, 2020. AAAI Press, 516–
523. https://aaai.org/ojs/index.php/AAAI/article/view/5389
[531] Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng,
Wenyong Huang, Lifeng Shang, Xin Jiang, and Qun Liu. 2023. Align-
ing Large Language Models with Human: A Survey. arXiv preprint
arXiv: 2307.12966 (2023).
[532] Zecong Wang, Jiaxi Cheng, Chen Cui, and Chenhao Yu. 2023. Imple-
menting BERT and fine-tuned RobertA to detect AI generated news
by ChatGPT. arXiv preprint arXiv: 2306.07401 (2023).
[533] Zekun Moore Wang, Zhongyuan Peng, Haoran Que, Jiaheng Liu,
Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Ze-
hao Ni, Man Zhang, Zhaoxiang Zhang, Wanli Ouyang, Ke Xu, Wenhu
Chen, Jie Fu, and Junran Peng. 2023. RoleLLM: Benchmarking, Elicit-
ing, and Enhancing Role-Playing Abilities of Large Language Models.
arXiv preprint arXiv: 2310.00746 (2023).
[534] Debora Weber-Wulff, Alla Anohina-Naumeca, Sonja Bjelobaba,
Tomáš Foltýnek, Jean Guerrero-Dib, Olumide Popoola, Petr Šigut, and
Lorna Waddington. 2023. Testing of Detection Tools for AI-Generated
Text. arXiv preprint arXiv: 2306.15666 (2023).
[535] Krzysztof Węcel, Marcin Sawiński, Milena Stróżyna, Włodzimierz
Lewoniewski, Piotr Stolarski, Ewelina Księżniak, and Witold
Abramowicz. 2023. Artifcial intelligence-friend or foe in fake news
campaigns. Economics and Business Review 9, 2 (2023), 41–70.
[536] Lingwei Wei, Dou Hu, Yantong Lai, Wei Zhou, and Songlin Hu.
2022. A Unified Propagation Forest-based Framework for Fake News
Detection. In Proceedings of the 29th International Conference on
30
Computational Linguistics. International Committee on Computa-
tional Linguistics, Gyeongju, Republic of Korea, 2769–2779. https:
//aclanthology.org/2022.coling-1.244
[537] Lingwei Wei, Dou Hu, Wei Zhou, Zhaojuan Yue, and Songlin Hu. 2021.
Towards Propagation Uncertainty: Edge-enhanced Bayesian Graph
Convolutional Networks for Rumor Detection. In Proceedings of the
59th Annual Meeting of the Association for Computational Linguis-
tics and the 11th International Joint Conference on Natural Language
Processing (Volume 1: Long Papers). Association for Computational
Linguistics, Online, 3845–3854. https://doi.org/10.18653/v1/2021.acl-
long.297
[538] Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang, Hai-
hua Yang, Biye Li, Cheng Cheng, Weiwei Lü, Rui Hu, Chenxia Li, Liu
Yang, Xilin Luo, Xuejie Wu, Lunan Liu, Wenjun Cheng, Peng Cheng,
Jianhao Zhang, Xiaoyu Zhang, Lei Lin, Xiaokun Wang, Yutuan Ma,
Chuanhai Dong, Yanqi Sun, Yifu Chen, Yongyi Peng, Xiaojuan Liang,
Shuicheng Yan, Han Fang, and Yahui Zhou. 2023. Skywork: A More
Open Bilingual Foundation Model. arXiv preprint arXiv: 2310.19341
(2023).
[539] Xiangpeng Wei, Haoran Wei, Huan Lin, Tianhao Li, Pei Zhang,
Xingzhang Ren, Mei Li, Yu Wan, Zhiwei Cao, Binbin Xie, Tianxi-
ang Hu, Shangjie Li, Binyuan Hui, Bowen Yu, Dayiheng Liu, Baosong
Yang, Fei Huang, and Jun Xie. 2023. PolyLM: An Open Source Polyglot
Large Language Model. arXiv preprint arXiv: 2307.06018 (2023).
[540] Zimian Wei, Hengyue Pan, Linbo Qiao, Xin Niu, Peijie Dong, and
Dongsheng Li. 2022. Cross-Modal Knowledge Distillation in Multi-
Modal Fake News Detection. In ICASSP 2022 - 2022 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP). 4733–
4737. https://doi.org/10.1109/ICASSP43922.2022.9747280
[541] Zeming Wei, Yifei Wang, and Yisen Wang. 2023. Jailbreak and Guard
Aligned Language Models with Only Few In-Context Demonstrations.
arXiv preprint arXiv: 2310.06387 (2023).
[542] Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin,
Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja
Balle, Atoosa Kasirzadeh, Zac Kenton, Sasha Brown, Will Hawkins,
Tom Stepleton, Courtney Biles, Abeba Birhane, Julia Haas, Laura
Rimell, Lisa Anne Hendricks, William Isaac, Sean Legassick, Geoffrey
Irving, and Iason Gabriel. 2021. Ethical and social risks of harm from
Language Models. arXiv preprint arXiv: Arxiv-2112.04359 (2021).
[543] Laura Weidinger, Maribeth Rauh, Nahema Marchal, Arianna Manzini,
Lisa Anne Hendricks, Juan Mateos-Garcia, Stevie Bergman, Jackie
Kay, Conor Griffin, Ben Bariach, Iason Gabriel, Verena Rieser, and
William Isaac. 2023. Sociotechnical Safety Evaluation of Generative
AI Systems. arXiv preprint arXiv: 2310.11986 (2023).
[544] Orion Weller, Marc Marone, Nathaniel Weir, Dawn Lawrie, Daniel
Khashabi, and Benjamin Van Durme. 2023. ""According to ..."" Prompt-
ing Language Models Improves Quoting from Pre-Training Data.
arXiv preprint arXiv: 2305.13252 (2023).
[545] BigScience Workshop, :, Teven Le Scao, Angela Fan, Christo-
pher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman
Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé,
Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson,
Pawan Sasanka Ammanamanchi, Thomas Wang, Benoît Sagot, Niklas
Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel
Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu
Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh,
Hugo Laurençon, Yacine Jernite, Julien Launay, Margaret Mitchell,
Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri
Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu,
Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong,
Daniel van Strien, David Ifeoluwa Adelani, Dragomir Radev, Ed-
uardo González Ponferrada, Efrat Levkovizh, Ethan Kim, Eyal Bar
Natan, Francesco De Toni, Gérard Dupont, Germán Kruszewski, Gi-
ada Pistilli, Hady Elsahar, Hamza Benyamina, Hieu Tran, Ian Yu,
Idris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios, Javier de la
Rosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan Chang, Jörg Fro-
hberg, Joseph Tobing, Joydeep Bhattacharjee, Khalid Almubarak,
Kimbo Chen, Kyle Lo, Leandro Von Werra, Leon Weber, Long Phan,
Loubna Ben allal, Ludovic Tanguy, Manan Dey, Manuel Romero
Muñoz, Maraim Masoud, María Grandury, Mario Šaško, Max Huang,
Maximin Coavoux, Mayank Singh, Mike Tian-Jian Jiang, Minh Chien
Vu, Mohammad A. Jauhar, Mustafa Ghaleb, Nishant Subramani,
Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen, Omar Espejel,
Ona de Gibert, Paulo Villegas, Peter Henderson, Pierre Colombo,
Priscilla Amuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani,
Roberto Luis López, Rui Ribeiro, Salomey Osei, Sampo Pyysalo,
Sebastian Nagel, Shamik Bose, Shamsuddeen Hassan Muhammad,
Shanya Sharma, Shayne Longpre, Somaieh Nikpoor, Stanislav Silber-
berg, Suhas Pai, Sydney Zink, Tiago Timponi Torrent, Timo Schick,
Tristan Thrush, Valentin Danchev, Vassilina Nikoulina, Veronika
Laippala, Violette Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak
Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si, Davut Emre
Taşar, Elizabeth Salesky, Sabrina J. Mielke, Wilson Y. Lee, Abheesht
Sharma, Andrea Santilli, Antoine Chaffin, Arnaud Stiegler, Debajyoti
Datta, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit Pandey,
Hendrik Strobelt, Jason Alan Fries, Jos Rozen, Leo Gao, Lintang
Sutawika, M Saiful Bari, Maged S. Al-shaibani, Matteo Manica, Nihal
Nayak, Ryan Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David,
Stephen H. Bach, Taewoon Kim, Tali Bers, Thibault Fevry, Trishala
Neeraj, Urmish Thakker, Vikas Raunak, Xiangru Tang, Zheng-Xin
Yong, Zhiqing Sun, Shaked Brody, Yallow Uri, Hadar Tojarieh, Adam
Roberts, Hyung Won Chung, Jaesung Tae, Jason Phang, Ofir Press,
Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff
Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad
Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi, Omar
Sanseviero, Patrick von Platen, Pierre Cornette, Pierre François Laval-
lée, Rémi Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden
Smith, Stéphane Requena, Suraj Patil, Tim Dettmers, Ahmed Baruwa,
Amanpreet Singh, Anastasia Cheveleva, Anne-Laure Ligozat, Ar-
jun Subramonian, Aurélie Névéol, Charles Lovering, Dan Garrette,
Deepak Tunuguntla, Ehud Reiter, Ekaterina Taktasheva, Ekaterina
Voloshina, Eli Bogdanov, Genta Indra Winata, Hailey Schoelkopf, Jan-
Christoph Kalo, Jekaterina Novikova, Jessica Zosa Forde, Jordan Clive,
Jungo Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat, Miruna
Clinciu, Najoung Kim, Newton Cheng, Oleg Serikov, Omer Antverg,
Oskar van der Wal, Rui Zhang, Ruochen Zhang, Sebastian Gehrmann,
Shachar Mirkin, Shani Pais, Tatiana Shavrina, Thomas Scialom, Tian
Yun, Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov, Vladislav
Mikhailov, Yada Pruksachatkun, Yonatan Belinkov, Zachary Bam-
berger, Zdeněk Kasner, Alice Rueda, Amanda Pestana, Amir Feizpour,
Ammar Khan, Amy Faranak, Ana Santos, Anthony Hevia, Antigona
Unldreaj, Arash Aghagol, Arezoo Abdollahi, Aycha Tammour, Azadeh
HajiHosseini, Bahareh Behroozi, Benjamin Ajibade, Bharat Saxena,
Carlos Muñoz Ferrandis, Daniel McDuff, Danish Contractor, David
Lansky, Davis David, Douwe Kiela, Duong A. Nguyen, Edward Tan,
Emi Baylor, Ezinwanne Ozoani, Fatima Mirza, Frankline Ononiwu,
Habib Rezanejad, Hessie Jones, Indrani Bhattacharya, Irene Solaiman,
Irina Sedenko, Isar Nejadgholi, Jesse Passmore, Josh Seltzer, Julio Bo-
nis Sanz, Livia Dutra, Mairon Samagaio, Maraim Elbadri, Margot
Mieskes, Marissa Gerchick, Martha Akinlolu, Michael McKenna, Mike
Qiu, Muhammed Ghauri, Mykola Burynok, Nafis Abrar, Nazneen
Rajani, Nour Elkott, Nour Fahmy, Olanrewaju Samuel, Ran An, Ras-
mus Kromann, Ryan Hao, Samira Alizadeh, Sarmad Shubber, Silas
Wang, Sourav Roy, Sylvain Viguier, Thanh Le, Tobi Oyebade, Trieu Le,
Yoyo Yang, Zach Nguyen, Abhinav Ramesh Kashyap, Alfredo Palas-
ciano, Alison Callahan, Anima Shukla, Antonio Miranda-Escalada,
Ayush Singh, Benjamin Beilharz, Bo Wang, Caio Brito, Chenxi Zhou,
Chirag Jain, Chuxin Xu, Clémentine Fourrier, Daniel León Periñán,
31
Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio Barth, Florian
Fuhrimann, Gabriel Altay, Giyaseddin Bayrak, Gully Burns, He-
lena U. Vrabec, Imane Bello, Ishani Dash, Jihyun Kang, John Giorgi,
Jonas Golde, Jose David Posada, Karthik Rangasai Sivaraman, Lokesh
Bulchandani, Lu Liu, Luisa Shinzato, Madeleine Hahn de Bykhovetz,
Maiko Takeuchi, Marc Pàmies, Maria A Castillo, Marianna Nezhurina,
Mario Sänger, Matthias Samwald, Michael Cullan, Michael Wein-
berg, Michiel De Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank,
Myungsun Kang, Natasha Seelam, Nathan Dahlberg, Nicholas Mi-
chio Broad, Nikolaus Muellner, Pascale Fung, Patrick Haller, Ramya
Chandrasekhar, Renata Eisenberg, Robert Martin, Rodrigo Canalli,
Rosaline Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok S
Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-
aroonsiri, Srishti Kumar, Stefan Schweter, Sushil Bharati, Tanmay
Laud, Théo Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis Labrak,
Yash Shailesh Bajaj, Yash Venkatraman, Yifan Xu, Yingxin Xu, Yu
Xu, Zhe Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada,
and Thomas Wolf. 2022. BLOOM: A 176B-Parameter Open-Access
Multilingual Language Model. arXiv preprint arXiv: 2211.05100 (2022).
[546] Guangyang Wu, Weijie Wu, Xiaohong Liu, Kele Xu, Tianjiao Wan, and
Wenyi Wang. 2023. Cheap-fake Detection with LLM using Prompt
Engineering. IEEE International Conference on Multimedia and Expo
Workshops (ICMEW) (2023). https://doi.org/10.1109/ICMEW59549.
2023.00025
[547] Hanqian Wu, Xinwei Li, Lu Li, and Qipeng Wang. 2022. Propaganda
Techniques Detection in Low-Resource Memes with Multi-Modal
Prompt Tuning. In 2022 IEEE International Conference on Multimedia
and Expo (ICME). 01–06. https://doi.org/10.1109/ICME52920.2022.
9859642
[548] Jiayang Wu, Wensheng Gan, Zefeng Chen, Shicheng Wan, and Hong
Lin. 2023. AI-Generated Content (AIGC): A Survey. arXiv preprint
arXiv: Arxiv-2304.06632 (2023).
[549] Jiaying Wu and Bryan Hooi. 2023. DECOR: Degree-Corrected Social
Graph Refinement for Fake News Detection. In Proceedings of the
29th ACM SIGKDD Conference on Knowledge Discovery and Data
Mining (Long Beach, CA, USA) (KDD ’23). Association for Computing
Machinery, New York, NY, USA, 2582–2593. https://doi.org/10.1145/
3580305.3599298
[550] Jiaying Wu and Bryan Hooi. 2023. Fake News in Sheep’s Clothing:
Robust Fake News Detection Against LLM-Empowered Style Attacks.
arXiv preprint arXiv: 2310.10830 (2023).
[551] Jiaying Wu, Shen Li, Ailin Deng, Miao Xiong, and Bryan Hooi. 2023.
Prompt-and-Align: Prompt-Based Social Alignment for Few-Shot
Fake News Detection. arXiv preprint arXiv: 2309.16424 (2023).
[552] Junfei Wu, Qiang Liu, Weizhi Xu, and Shu Wu. 2022. Bias Mitigation
for Evidence-Aware Fake News Detection by Causal Intervention.
In Proceedings of the 45th International ACM SIGIR Conference on
Research and Development in Information Retrieval (Madrid, Spain)
(SIGIR ’22). Association for Computing Machinery, New York, NY,
USA, 2308–2313. https://doi.org/10.1145/3477495.3531850
[553] Junfei Wu, Weizhi Xu, Qiang Liu, Shu Wu, and Liang Wang. 2022.
Adversarial Contrastive Learning for Evidence-aware Fake News De-
tection with Graph Neural Networks. arXiv preprint arXiv: 2210.05498
(2022).
[554] Junchao Wu, Shu Yang, Runzhe Zhan, Yulin Yuan, Derek F. Wong,
and Lidia S. Chao. 2023. A Survey on LLM-generated Text Detection:
Necessity, Methods, and Future Directions. arXiv preprint arXiv:
2310.14724 (2023).
[555] Kangxi Wu, Liang Pang, Huawei Shen, Xueqi Cheng, and Tat-Seng
Chua. 2023. LLMDet: A Large Language Models Detection Tool. arXiv
preprint arXiv: 2305.15004 (2023).
[556] Kun Wu, Xu Yuan, and Yue Ning. 2021. Incorporating relational
knowledge in explainable fake news detection. In Pacific-Asia Con-
ference on Knowledge Discovery and Data Mining. Springer, 403–415.
[557] Liang Wu, Fred Morstatter, Kathleen M Carley, and Huan Liu. 2019.
Misinformation in social media: definition, manipulation, and detec-
tion. ACM SIGKDD explorations newsletter 21, 2 (2019), 80–90.
[558] Lianwei Wu, Yuan Rao, Haolin Jin, Ambreen Nazir, and Ling Sun.
2019. Different Absorption from the Same Sharing: Sifted Multi-
task Learning for Fake News Detection. In Proceedings of the 2019
Conference on Empirical Methods in Natural Language Processing and
the 9th International Joint Conference on Natural Language Processing
(EMNLP-IJCNLP). Association for Computational Linguistics, Hong
Kong, China, 4644–4653. https://doi.org/10.18653/v1/D19-1471
[559] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang,
Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. 2023.
AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent
Conversation Framework. arXiv preprint arXiv: 2308.08155 (2023).
[560] Weiyue Wu and Shaoshan Liu. 2023. A Comprehensive Review
and Systematic Analysis of Artificial Intelligence Regulation Policies.
arXiv preprint arXiv: 2307.12218 (2023).
[561] Xueqing Wu, Kung-Hsiang Huang, Yi Fung, and Heng Ji. 2022. Cross-
document Misinformation Detection based on Event Graph Rea-
soning. In Proceedings of the 2022 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Lan-
guage Technologies. Association for Computational Linguistics, Seat-
tle, United States, 543–558. https://doi.org/10.18653/v1/2022.naacl-
main.40
[562] Yang Wu, Pengwei Zhan, Yunjian Zhang, Liming Wang, and Zhen Xu.
2021. Multimodal Fusion with Co-Attention Networks for Fake News
Detection. In Findings of the Association for Computational Linguistics:
ACL-IJCNLP 2021. Association for Computational Linguistics, Online,
2560–2569. https://doi.org/10.18653/v1/2021.findings-acl.226
[563] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang
Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng,
Xiaoran Fan, Xiao Wang, Limao Xiong, Qin Liu, Yuhao Zhou, Weiran
Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin,
Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan
Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huan, and Tao Gui. 2023.
The Rise and Potential of Large Language Model Based Agents: A
Survey. arXiv preprint arXiv: 2309.07864 (2023).
[564] Rui Xia, Kaizhou Xuan, and Jianfei Yu. 2020. A State-independent
and Time-evolving Network for Early Rumor Detection in Social
Media. In Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP). Association for Computational
Linguistics, Online, 9042–9051.
https://doi.org/10.18653/v1/2020.
emnlp-main.727
[565] Madelyne Xiao and Jonathan Mayer. 2023. The Challenges of Machine
Learning for Trust and Safety: A Case Study on Misinformation
Detection. arXiv preprint arXiv: 2308.12215 (2023).
[566] Jianhui Xie, Song Liu, Ruixin Liu, Yinghong Zhang, and Yuesheng
Zhu. 2021. SERN: Stance Extraction and Reasoning Network for Fake
News Detection. In ICASSP 2021 - 2021 IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP). 2520–2524. https:
//doi.org/10.1109/ICASSP39728.2021.9414787
[567] Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He,
and Bryan Hooi. 2023. Can LLMs Express Their Uncertainty? An Em-
pirical Evaluation of Confidence Elicitation in LLMs. arXiv preprint
arXiv: 2306.13063 (2023).
[568] Fangzhi Xu, Qika Lin, Jiawei Han, Tianzhe Zhao, Jun Liu, and Erik
Cambria. 2023. Are Large Language Models Really Good Logical
Reasoners? A Comprehensive Evaluation and Beyond. arXiv preprint
arXiv: 2306.09841 (2023).
[569] Guohai Xu, Jiayi Liu, Ming Yan, Haotian Xu, Jinghui Si, Zhuoran Zhou,
Peng Yi, Xing Gao, Jitao Sang, Rong Zhang, Ji Zhang, Chao Peng, Fei
Huang, and Jingren Zhou. 2023. CValues: Measuring the Values of
Chinese Large Language Models from Safety to Responsibility. arXiv
preprint arXiv: 2307.09705 (2023).
32
[570] Han Xu, Jie Ren, Pengfei He, Shenglai Zeng, Yingqian Cui, Amy Liu,
Hui Liu, and Jiliang Tang. 2023. On the Generalization of Training-
based ChatGPT Detection Methods. arXiv preprint arXiv: 2310.01307
(2023).
[571] Weizhi Xu, Qiang Liu, Shu Wu, and Liang Wang. 2023. Counterfactual
Debiasing for Fact Verification. In Proceedings of the 61st Annual
Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers). Association for Computational Linguistics, Toronto,
Canada, 6777–6789. https://doi.org/10.18653/v1/2023.acl-long.374
[572] Weizhi Xu, Junfei Wu, Qiang Liu, Shu Wu, and Liang Wang. 2022.
Evidence-Aware Fake News Detection with Graph Neural Networks.
In Proceedings of the ACM Web Conference 2022 (Virtual Event, Lyon,
France) (WWW ’22). Association for Computing Machinery, New
York, NY, USA, 2501–2510. https://doi.org/10.1145/3485447.3512122
[573] Yiheng Xu, Hongjin Su, Chen Xing, Boyu Mi, Qian Liu, Weijia Shi,
Binyuan Hui, Fan Zhou, Yitao Liu, Tianbao Xie, Zhoujun Cheng, Si-
heng Zhao, Lingpeng Kong, Bailin Wang, Caiming Xiong, and Tao Yu.
2023. Lemur: Harmonizing Natural Language and Code for Language
Agents. arXiv preprint arXiv: 2310.06830 (2023).
[574] Jun Yan, Vikas Yadav, Shiyang Li, Lichang Chen, Zheng Tang, Hai
Wang, Vijay Srinivasan, Xiang Ren, and Hongxia Jin. 2023. Vir-
tual Prompt Injection for Instruction-Tuned Large Language Models.
arXiv preprint arXiv: 2307.16888 (2023).
[575] Borui Yang, Wei Li, Liyao Xiang, and Bo Li. 2023. Towards Code
Watermarking with Dual-Channel Transformations. arXiv preprint
arXiv: 2309.00860 (2023).
[576] Fan Yang, Shiva K. Pentyala, Sina Mohseni, Mengnan Du, Hao Yuan,
Rhema Linder, Eric D. Ragan, Shuiwang Ji, and Xia (Ben) Hu. 2019.
XFake: Explainable Fake News Detector with Visualizations. In The
World Wide Web Conference, WWW 2019, San Francisco, CA, USA,
May 13-17, 2019, Ling Liu, Ryen W. White, Amin Mantrach, Fabrizio
Silvestri, Julian J. McAuley, Ricardo Baeza-Yates, and Leila Zia (Eds.).
ACM, 3600–3604. https://doi.org/10.1145/3308558.3314119
[577] Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang
Feng, Haoming Jiang, Bing Yin, and Xia Hu. 2023. Harnessing the
Power of LLMs in Practice: A Survey on ChatGPT and Beyond. arXiv
preprint arXiv: Arxiv-2304.13712 (2023).
[578] Kai-Cheng Yang and Filippo Menczer. 2023. Anatomy of an AI-
powered malicious social botnet. arXiv preprint arXiv: 2307.16336
(2023).
[579] Ruichao Yang, Jing Ma, Hongzhan Lin, and Wei Gao. 2022. A Weakly
Supervised Propagation Model for Rumor Verification and Stance
Detection with Multiple Instance Learning. In Proceedings of the 45th
International ACM SIGIR Conference on Research and Development
in Information Retrieval (Madrid, Spain) (SIGIR ’22). Association for
Computing Machinery, New York, NY, USA, 1761–1772. https://doi.
org/10.1145/3477495.3531930
[580] Ruichao Yang, Xiting Wang, Yiqiao Jin, Chaozhuo Li, Jianxun Lian,
and Xing Xie. 2022. Reinforcement Subgraph Reasoning for Fake
News Detection. In KDD.
[581] Shuo Yang, Kai Shu, Suhang Wang, Renjie Gu, Fan Wu, and Huan
Liu. 2019. Unsupervised fake news detection on social media: A gen-
erative approach. In Proceedings of 33rd AAAI Conference on Artificial
Intelligence.
[582] Sin-han Yang, Chung-chi Chen, Hen-Hsen Huang, and Hsin-Hsi
Chen. 2023. Entity-Aware Dual Co-Attention Network for Fake News
Detection. In Findings of the Association for Computational Linguistics:
EACL 2023. Association for Computational Linguistics, Dubrovnik,
Croatia, 106–113. https://aclanthology.org/2023.findings-eacl.7
[583] Xi Yang, Kejiang Chen, Weiming Zhang, Chang Liu, Yuang Qi, Jie
Zhang, Han Fang, and Nenghai Yu. 2023. Watermarking Text Gener-
ated by Black-Box Language Models. arXiv preprint arXiv: 2305.08883
(2023).
[584] Xianjun Yang, Wei Cheng, Linda Petzold, William Yang Wang, and
Haifeng Chen. 2023. DNA-GPT: Divergent N-Gram Analysis for
Training-Free Detection of GPT-Generated Text. arXiv preprint arXiv:
2305.17359 (2023).
[585] Xiaoyu Yang, Yuefei Lyu, Tian Tian, Yifei Liu, Yudong Liu, and Xi
Zhang. 2020. Rumor Detection on Social Media with Graph Structured
Adversarial Learning. In Proceedings of the Twenty-Ninth Interna-
tional Joint Conference on Artificial Intelligence, IJCAI 2020, Christian
Bessiere (Ed.). ijcai.org, 1417–1423.
https://doi.org/10.24963/ijcai.
2020/197
[586] Xianjun Yang, Liangming Pan, Xuandong Zhao, Haifeng Chen, Linda
Petzold, William Yang Wang, and Wei Cheng. 2023. A Survey on De-
tection of LLMs-Generated Content. arXiv preprint arXiv: 2310.15654
(2023).
[587] Xianjun Yang, Xiao Wang, Qi Zhang, Linda Petzold, William Yang
Wang, Xun Zhao, and Dahua Lin. 2023. Shadow Alignment: The Ease
of Subverting Safely-Aligned Language Models. arXiv preprint arXiv:
2310.02949 (2023).
[588] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching
Lin, Zicheng Liu, and Lijuan Wang. 2023. The Dawn of LMMs: Pre-
liminary Explorations with GPT-4V(ision).
arXiv preprint arXiv:
2309.17421 (2023).
[589] Zhiwei Yang, Jing Ma, Hechang Chen, Hongzhan Lin, Ziyang Luo,
and Yi Chang. 2022. A Coarse-to-fine Cascaded Evidence-Distillation
Neural Network for Explainable Fake News Detection. In Proceedings
of the 29th International Conference on Computational Linguistics.
International Committee on Computational Linguistics, Gyeongju,
Republic of Korea, 2608–2621. https://aclanthology.org/2022.coling-
1.230
[590] Dongyu Yao, Jianshu Zhang, Ian G. Harris, and Marcel Carlsson.
2023. FuzzLLM: A Novel and Universal Fuzzing Framework for Proac-
tively Discovering Jailbreak Vulnerabilities in Large Language Models.
arXiv preprint arXiv: 2309.05274 (2023).
[591] Jing Yao, Xiaoyuan Yi, Xiting Wang, Jindong Wang, and Xing Xie.
2023. From Instructions to Intrinsic Human Values - A Survey of
Alignment Goals for Big Models. arXiv preprint arXiv: 2308.12014
(2023).
[592] Jia-Yu Yao, Kun-Peng Ning, Zhen-Hui Liu, Mu-Nan Ning, and Li
Yuan. 2023. LLM Lies: Hallucinations are not Bugs, but Features as
Adversarial Examples. arXiv preprint arXiv: 2310.01469 (2023).
[593] Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li,
Shumin Deng, Huajun Chen, and Ningyu Zhang. 2023. Editing Large
Language Models: Problems, Methods, and Opportunities. arXiv
preprint arXiv: 2305.13172 (2023).
[594] Hongbin Ye, Tong Liu, Aijia Zhang, Wei Hua, and Weiqiang Jia. 2023.
Cognitive Mirage: A Review of Hallucinations in Large Language
Models. arXiv preprint arXiv: 2309.06794 (2023).
[595] Wentao Ye, Mingfeng Ou, Tianyi Li, Yipeng chen, Xuetao Ma, Yi-
fan Yanggong, Sai Wu, Jie Fu, Gang Chen, Haobo Wang, and Junbo
Zhao. 2023. Assessing Hidden Risks of LLMs: An Empirical Study
on Robustness, Consistency, and Credibility. arXiv preprint arXiv:
2305.10235 (2023).
[596] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu,
and Enhong Chen. 2023. A Survey on Multimodal Large Language
Models. arXiv preprint arXiv: 2306.13549 (2023).
[597] Zhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu, Xipeng Qiu, and
Xuanjing Huang. 2023. Do Large Language Models Know What They
Don’t Know?. In Findings of the Association for Computational Linguis-
tics: ACL 2023. Association for Computational Linguistics, Toronto,
Canada, 8653–8665. https://doi.org/10.18653/v1/2023.findings-acl.
551
[598] Zheng-Xin Yong, Cristina Menghini, and Stephen H. Bach. 2023. Low-
Resource Languages Jailbreak GPT-4. arXiv preprint arXiv: 2310.02446
(2023).
33
[599] Kiyoon Yoo, W. Ahn, Jiho Jang, and N. Kwak. 2023. Robust Multi-bit
Natural Language Watermarking through Invariant Features. Annual
Meeting of the Association for Computational Linguistics (2023). https:
//doi.org/10.18653/v1/2023.acl-long.117
[600] Shehel Yoosuf and Yin Yang. 2019. Fine-Grained Propaganda Detec-
tion with Fine-Tuned BERT. In Proceedings of the Second Workshop on
Natural Language Processing for Internet Freedom: Censorship, Disinfor-
mation, and Propaganda. Association for Computational Linguistics,
Hong Kong, China, 87–91. https://doi.org/10.18653/v1/D19-5011
[601] Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. 2023. Mak-
ing Retrieval-Augmented Language Models Robust to Irrelevant Con-
text. arXiv preprint arXiv: 2310.01558 (2023).
[602] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang,
Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang. 2023.
Ferret: Refer and Ground Anything Anywhere at Any Granularity.
arXiv preprint arXiv: 2310.07704 (2023).
[603] Hamza Yousuf, Sander van der Linden, Luke Bredius, GA Ted van
Essen, Govert Sweep, Zohar Preminger, Eric van Gorp, Erik Scherder,
Jagat Narula, and Leonard Hofstra. 2021. A media intervention ap-
plying debunking versus non-debunking content to combat vaccine
misinformation in elderly in the Netherlands: A digital randomised
trial. EClinicalMedicine 35 (2021), 100881.
[604] Jiahao Yu, Xingwei Lin, and Xinyu Xing. 2023. GPTFUZZER : Red
Teaming Large Language Models with Auto-Generated Jailbreak
Prompts. arXiv preprint arXiv: 2309.10253 (2023).
[605] Jifan Yu, Xiaozhi Wang, Shangqing Tu, Shulin Cao, Daniel Zhang-Li,
Xin Lv, Hao Peng, Zijun Yao, Xiaohan Zhang, Hanming Li, Chunyang
Li, Zheyuan Zhang, Yushi Bai, Yantao Liu, Amy Xin, Nianyi Lin,
Kaifeng Yun, Linlu Gong, Jianhui Chen, Zhili Wu, Yunjia Qi, Weikai
Li, Yong Guan, Kaisheng Zeng, Ji Qi, Hailong Jin, Jinxin Liu, Yu Gu,
Yuan Yao, Ning Ding, Lei Hou, Zhiyuan Liu, Bin Xu, Jie Tang, and
Juanzi Li. 2023. KoLA: Carefully Benchmarking World Knowledge of
Large Language Models. arXiv preprint arXiv: 2306.09296 (2023).
[606] Peipeng Yu, Jiahan Chen, Xuan Feng, and Zhihua Xia. 2023. CHEAT:
A Large-scale Dataset for Detecting ChatGPT-writtEn AbsTracts.
arXiv preprint arXiv: Arxiv-2304.12008 (2023).
[607] Wenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu, Qingyun Wang,
Heng Ji, and Meng Jiang. 2020. A Survey of Knowledge-Enhanced
Text Generation. arXiv preprint arXiv: 2010.04389 (2020).
[608] Zihan Yu, Liang He, Zhen Wu, Xinyu Dai, and Jiajun Chen. 2023.
Towards Better Chain-of-Thought Prompting Strategies: A Survey.
arXiv preprint arXiv: 2310.04959 (2023).
[609] Chunyuan Yuan, Qianwen Ma, Wei Zhou, Jizhong Han, and Songlin
Hu. 2019. Jointly Embedding the Local and Global Relations of Het-
erogeneous Graph for Rumor Detection. In 2019 IEEE International
Conference on Data Mining (ICDM). 796–805. https://doi.org/10.1109/
ICDM.2019.00090
[610] Chunyuan Yuan, Qianwen Ma, Wei Zhou, Jizhong Han, and Songlin
Hu. 2020. Early Detection of Fake News by Utilizing the Credibility
of News, Publishers, and Users based on Weakly Supervised Learning.
In Proceedings of the 28th International Conference on Computational
Linguistics. International Committee on Computational Linguistics,
Barcelona, Spain (Online), 5444–5454. https://doi.org/10.18653/v1/
2020.coling-main.475
[611] Lifan Yuan, Yangyi Chen, Xingyao Wang, Yi R. Fung, Hao Peng, and
Heng Ji. 2023. CRAFT: Customizing LLMs by Creating and Retrieving
from Specialized Toolsets. arXiv preprint arXiv: 2309.17428 (2023).
[612] Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen tse Huang, Pinjia
He, Shuming Shi, and Zhaopeng Tu. 2023. GPT-4 Is Too Smart To
Be Safe: Stealthy Chat with LLMs via Cipher. arXiv preprint arXiv:
2308.06463 (2023).
[613] Zhenrui Yue, Huimin Zeng, Ziyi Kou, Lanyu Shang, and Dong Wang.
2022. Contrastive Domain Adaptation for Early Misinformation
Detection: A Case Study on COVID-19. In Proc. of CIKM.
[614] Zhenrui Yue, Huimin Zeng, Yang Zhang, Lanyu Shang, and Dong
Wang. 2023. MetaAdapt: Domain Adaptive Few-Shot Misinformation
Detection via Meta Learning. In Proceedings of the 61st Annual Meet-
ing of the Association for Computational Linguistics (Volume 1: Long
Papers). Association for Computational Linguistics, Toronto, Canada,
5223–5239. https://doi.org/10.18653/v1/2023.acl-long.286
[615] Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali
Farhadi, Franziska Roesner, and Yejin Choi. 2019.
Defending
Against Neural Fake News. In Advances in Neural Information Pro-
cessing Systems 32: Annual Conference on Neural Information Pro-
cessing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver,
BC, Canada, Hanna M. Wallach, Hugo Larochelle, Alina Beygelz-
imer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett
(Eds.). 9051–9062. https://proceedings.neurips.cc/paper/2019/hash/
3e9f0fc9b2f89e043bc6233994dfcf76-Abstract.html
[616] Fengzhu Zeng and Wei Gao. 2022. Early Rumor Detection Using Neu-
ral Hawkes Process with a New Benchmark Dataset. In Proceedings of
the 2022 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies. Associ-
ation for Computational Linguistics, Seattle, United States, 4105–4117.
https://doi.org/10.18653/v1/2022.naacl-main.302
[617] Fengzhu Zeng and Wei Gao. 2023. Prompt to be Consistent is Better
than Self-Consistent? Few-Shot and Zero-Shot Fact Verification with
Pre-trained Language Models. arXiv preprint arXiv: 2306.02569 (2023).
[618] Bohui Zhang, Ioannis Reklos, Nitisha Jain, Albert Meroño Peñuela,
and Elena Simperl. 2023. Using Large Language Models for Knowl-
edge Engineering (LLMKE): A Case Study on Wikidata. arXiv preprint
arXiv: 2309.08491 (2023).
[619] Chaoning Zhang, Chenshuang Zhang, Chenghao Li, Yu Qiao, Sheng
Zheng, Sumit Kumar Dam, Mengchun Zhang, Jung Uk Kim, Seong Tae
Kim, Jinwoo Choi, Gyeong-Moon Park, Sung-Ho Bae, Lik-Hang Lee,
Pan Hui, In So Kweon, and Choong Seon Hong. 2023. One Small Step
for Generative AI, One Giant Leap for AGI: A Complete Survey on
ChatGPT in AIGC Era. arXiv preprint arXiv: 2304.06488 (2023).
[620] Huaiwen Zhang, Quan Fang, Shengsheng Qian, and Changsheng Xu.
2019. Multi-modal Knowledge-aware Event Memory Network for
Social Media Rumor Detection. In Proceedings of the 27th ACM Inter-
national Conference on Multimedia, MM 2019, Nice, France, October
21-25, 2019. 1942–1951. https://doi.org/10.1145/3343031.3350850
[621] Hangfan Zhang, Zhimeng Guo, Huaisheng Zhu, Bochuan Cao, Lu
Lin, Jinyuan Jia, Jinghui Chen, and Dinghao Wu. 2023. On the Safety
of Open-Sourced Large Language Models: Does Alignment Really
Prevent Them From Being Misused? arXiv preprint arXiv: 2310.01581
(2023).
[622] Haopeng Zhang, Xiao Liu, and Jiawei Zhang. 2023. Extractive Sum-
marization via ChatGPT for Faithful Summary Generation. arXiv
preprint arXiv: 2304.04193 (2023).
[623] Muru Zhang, Ofir Press, William Merrill, Alisa Liu, and Noah A.
Smith. 2023. How Language Model Hallucinations Can Snowball.
arXiv preprint arXiv: 2305.13534 (2023).
[624] Pan Zhang, Xiaoyi Dong Bin Wang, Yuhang Cao, Chao Xu, Linke
Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang Zhang, Haodong
Duan, Hang Yan, et al. 2023.
InternLM-XComposer: A Vision-
Language Large Model for Advanced Text-image Comprehension
and Composition.
ArXiv preprint abs/2309.15112 (2023).
https:
//arxiv.org/abs/2309.15112
[625] Peitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou, and Jian-Yun
Nie. 2023. Retrieve Anything To Augment Large Language Models.
arXiv preprint arXiv: 2310.07554 (2023).
[626] Shuo Zhang, Liangming Pan, Junzhou Zhao, and William Yang Wang.
2023. Mitigating Language Model Hallucination with Interactive
Question-Knowledge Alignment. arXiv preprint arXiv: 2305.13669
(2023).
34
[627] Tianhua Zhang, Hongyin Luo, Yung-Sung Chuang, Wei Fang, Luc
Gaitskell, Thomas Hartvigsen, Xixin Wu, Danny Fox, Helen Meng,
and James Glass. 2023. Interpretable Unified Language Checking.
arXiv preprint arXiv: Arxiv-2304.03728 (2023).
[628] Wenxuan Zhang, Sharifah Mahani Aljunied, Chang Gao, Yew Ken
Chia, and Lidong Bing. 2023. M3Exam: A Multilingual, Multimodal,
Multilevel Benchmark for Examining Large Language Models. arXiv
preprint arXiv: 2306.05179 (2023).
[629] Xueyao Zhang, Juan Cao, Xirong Li, Qiang Sheng, Lei Zhong, and
Kai Shu. 2021. Mining Dual Emotion for Fake News Detection. In
Proc. of WWW.
[630] Xuan Zhang and Wei Gao. 2023. Towards LLM-based Fact Verification
on News Claims with a Hierarchical Step-by-Step Prompting Method.
arXiv preprint arXiv: 2310.00305 (2023).
[631] Xichen Zhang and Ali A Ghorbani. 2020. An overview of online
fake news: Characterization, detection, and discussion. Information
Processing & Management 57, 2 (2020), 102025.
[632] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu,
Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang,
Anh Tuan Luu, Wei Bi, Freda Shi, and Shuming Shi. 2023. Siren’s
Song in the AI Ocean: A Survey on Hallucination in Large Language
Models. arXiv preprint arXiv: 2309.01219 (2023).
[633] Yinuo Zhang, Zhulin Tao, Xi Wang, and Tongyue Wang. 2023. INO at
Factify 2: Structure Coherence based Multi-Modal Fact Verification.
arXiv preprint arXiv: 2303.01510 (2023).
[634] Yizhou Zhang, Loc Trinh, Defu Cao, Zijun Cui, and Yan Liu.
2023. Detecting Out-of-Context Multimodal Misinformation with
interpretable neural-symbolic model. arXiv preprint arXiv: Arxiv-
2304.07633 (2023).
[635] Zihan Zhang, Meng Fang, Ling Chen, Mohammad-Reza Namazi-Rad,
and Jun Wang. 2023. How Do Large Language Models Capture the
Ever-changing World Knowledge? A Review of Recent Advances.
arXiv preprint arXiv: 2310.07343 (2023).
[636] Zhexin Zhang, Leqi Lei, Lindong Wu, Rui Sun, Yongkang Huang,
Chong Long, Xiao Liu, Xuanyu Lei, Jie Tang, and Minlie Huang. 2023.
SafetyBench: Evaluating the Safety of Large Language Models with
Multiple Choice Questions. arXiv preprint arXiv: 2309.07045 (2023).
[637] Ruochen Zhao, Hailin Chen, Weishi Wang, Fangkai Jiao, Xuan Long
Do, Chengwei Qin, Bosheng Ding, Xiaobao Guo, Minzhi Li, Xingxuan
Li, and Shafiq Joty. 2023. Retrieving Multimodal Information for
Augmented Generation: A Survey. arXiv preprint arXiv: 2303.10868
(2023).
[638] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang,
Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican
Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao
Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu,
Jian-Yun Nie, and Ji-Rong Wen. 2023. A Survey of Large Language
Models. arXiv preprint arXiv: Arxiv-2303.18223 (2023).
[639] Xuandong Zhao, Prabhanjan Ananth, Lei Li, and Yu-Xiang Wang.
2023. Provable Robust Watermarking for AI-Generated Text. arXiv
preprint arXiv: 2306.17439 (2023).
[640] Jiaqi Zheng, Xi Zhang, Sanchuan Guo, Quan Wang, Wenyu Zang,
and Yongdong Zhang. 2022. MFAN: Multi-modal Feature-enhanced
Attention Networks for Rumor Detection. IJCAI.
[641] Kaizhi Zheng, Xuehai He, and Xin Eric Wang. 2023. MiniGPT-5:
Interleaved Vision-and-Language Generation via Generative Vokens.
arXiv preprint arXiv: 2310.02239 (2023).
[642] Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei Shen, Binghai
Wang, Yan Liu, Senjie Jin, Qin Liu, Yuhao Zhou, Limao Xiong, Lu
Chen, Zhiheng Xi, Nuo Xu, Wenbin Lai, Minghao Zhu, Cheng Chang,
Zhangyue Yin, Rongxiang Weng, Wensen Cheng, Haoran Huang,
Tianxiang Sun, Hang Yan, Tao Gui, Qi Zhang, Xipeng Qiu, and Xuan-
jing Huang. 2023. Secrets of RLHF in Large Language Models Part I:
PPO. arXiv preprint arXiv: 2307.04964 (2023).
[643] Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning
Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi
Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. 2023. LIMA:
Less Is More for Alignment. arXiv preprint arXiv: 2305.11206 (2023).
[644] Jiawei Zhou, Yixuan Zhang, Qianni Luo, Andrea G Parker, and
Munmun De Choudhury. 2023. Synthetic Lies: Understanding AI-
Generated Misinformation and Evaluating Algorithmic and Human
Solutions. In Proceedings of the 2023 CHI Conference on Human Factors
in Computing Systems. 1–20.
[645] Wangchunshu Zhou, Yuchen Eleanor Jiang, Long Li, Jialong Wu,
Tiannan Wang, Shi Qiu, Jintian Zhang, Jing Chen, Ruipu Wu, Shuai
Wang, Shiding Zhu, Jiyu Chen, Wentao Zhang, Ningyu Zhang, Hua-
jun Chen, Peng Cui, and Mrinmaya Sachan. 2023. Agents: An Open-
source Framework for Autonomous Language Agents. arXiv preprint
arXiv: 2309.07870 (2023).
[646] Xinyi Zhou and Reza Zafarani. 2020. A survey of fake news: Funda-
mental theories, detection methods, and opportunities. ACM Com-
puting Surveys (CSUR) 53, 5 (2020), 1–40.
[647] Chenguang Zhu, Yichong Xu, Xiang Ren, Bill Yuchen Lin, Meng Jiang,
and Wenhao Yu. 2022. Knowledge-Augmented Methods for Natu-
ral Language Processing. In Proceedings of the 60th Annual Meeting
of the Association for Computational Linguistics: Tutorial Abstracts.
Association for Computational Linguistics, Dublin, Ireland, 12–20.
https://doi.org/10.18653/v1/2022.acl-tutorials.3
[648] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elho-
seiny. 2023. MiniGPT-4: Enhancing Vision-Language Understanding
with Advanced Large Language Models. arXiv preprint arXiv: Arxiv-
2304.10592 (2023).
[649] Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yi-
dong Wang, Linyi Yang, Wei Ye, Neil Zhenqiang Gong, Yue Zhang, and
Xing Xie. 2023. PromptBench: Towards Evaluating the Robustness
of Large Language Models on Adversarial Prompts. arXiv preprint
arXiv: 2306.04528 (2023).
[650] Yongchun Zhu, Qiang Sheng, Juan Cao, Shuokai Li, Danding Wang,
and Fuzhen Zhuang. 2022. Generalizing to the Future: Mitigating
Entity Bias in Fake News Detection. In Proc. of SIGIR.
[651] Yongchun Zhu, Qiang Sheng, Juan Cao, Qiong Nan, Kai Shu, Ming
hui Wu, Jindong Wang, and Fuzhen Zhuang. 2022. Memory-Guided
Multi-View Multi-Domain Fake News Detection. IEEE Transactions
on Knowledge and Data Engineering (2022). https://doi.org/10.1109/
TKDE.2022.3185151
[652] Terry Yue Zhuo, Yujin Huang, Chunyang Chen, and Zhenchang Xing.
2023. Exploring AI Ethics of ChatGPT: A Diagnostic Analysis. arXiv
preprint arXiv: Arxiv-2301.12867 (2023).
[653] Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo,
Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-
Kathrin Dombrowski, Shashwat Goel, Nathaniel Li, Michael J. Byun,
Zifan Wang, Alex Mallen, Steven Basart, Sanmi Koyejo, Dawn Song,
Matt Fredrikson, J. Zico Kolter, and Dan Hendrycks. 2023. Repre-
sentation Engineering: A Top-Down Approach to AI Transparency.
arXiv preprint arXiv: 2310.01405 (2023).
[654] Anni Zou, Zhuosheng Zhang, and Hai Zhao. 2023. Decker: Dou-
ble Check with Heterogeneous Knowledge for Commonsense Fact
Verification. Annual Meeting of the Association for Computational
Linguistics (2023). https://doi.org/10.48550/arXiv.2305.05921
[655] Arkaitz Zubiaga, Ahmet Aker, Kalina Bontcheva, Maria Liakata, and
Rob Procter. 2018. Detection and Resolution of Rumours in Social
Media: A Survey. ACM Comput. Surv. 51, 2 (2018), 32:1–32:36. https:
//doi.org/10.1145/3161603
[656] Yuhui Zuo, Wei Zhu, and Guoyong GUET Cai. 2022. Continually De-
tection, Rapidly React: Unseen Rumors Detection Based on Continual
Prompt-Tuning. In Proceedings of the 29th International Conference
on Computational Linguistics. International Committee on Computa-
tional Linguistics, Gyeongju, Republic of Korea, 3029–3041.
35
"
9,10,Towards Vision Enhancing LLMs: Empowering Multimodal Knowledge Storage and Sharing in LLMs,"Yunxin Li, Baotian Hu, Wei Wang, Xiaochun Cao, Min Zhang","Recent advancements in multimodal large language models (MLLMs) have achieved
significant multimodal generation capabilities, akin to GPT-4. These models
predominantly map visual information into language representation space,
leveraging the vast knowledge and powerful text generation abilities of LLMs to
produce multimodal instruction-following responses. We could term this method
as LLMs for Vision because of its employing LLMs for visual-language
understanding, yet observe that these MLLMs neglect the potential of harnessing
visual knowledge to enhance overall capabilities of LLMs, which could be
regraded as Vision Enhancing LLMs. In this paper, we propose an approach called
MKS2, aimed at enhancing LLMs through empowering Multimodal Knowledge Storage
and Sharing in LLMs. Specifically, we introduce the Modular Visual Memory, a
component integrated into the internal blocks of LLMs, designed to store
open-world visual information efficiently. Additionally, we present a soft
Mixtures-of-Multimodal Experts architecture in LLMs to invoke multimodal
knowledge collaboration during generation. Our comprehensive experiments
demonstrate that MKS2 substantially augments the reasoning capabilities of LLMs
in contexts necessitating physical or commonsense knowledge. It also delivers
competitive results on multimodal benchmarks.","Towards Vision Enhancing LLMs:
Empowering Multimodal Knowledge Storage and Sharing in LLMs
Yunxin Li 1 Baotian Hu 1 Wei Wang 2 Xiaochun Cao 2 Min Zhang 1
Abstract
Recent advancements in multimodal large lan-
guage models (MLLMs) have achieved signifi-
cant multimodal generation capabilities, akin to
GPT-4. These models predominantly map visual
information into language representation space,
leveraging the vast knowledge and powerful text
generation abilities of LLMs to produce multi-
modal instruction-following responses. We could
term this method as LLMs for Vision because of its
employing LLMs for visual-language understand-
ing, yet observe that these MLLMs neglect the
potential of harnessing visual knowledge to en-
hance overall capabilities of LLMs, which could
be regraded as Vision Enhancing LLMs. In this
paper, we propose an approach called MKS2,
aimed at enhancing LLMs through empowering
Multimodal Knowledge Storage and Sharing in
LLMs. Specifically, we introduce the Modular
Visual Memory, a component integrated into the
internal blocks of LLMs, designed to store open-
world visual information efficiently. Addition-
ally, we present a soft Mixtures-of-Multimodal
Experts architecture in LLMs to invoke multi-
modal knowledge collaboration during generation.
Our comprehensive experiments demonstrate that
MKS2 substantially augments the reasoning capa-
bilities of LLMs in contexts necessitating physical
or commonsense knowledge. It also delivers com-
petitive results on multimodal benchmarks.
1. Introduction
Recent advances (Yin et al., 2023; Driess et al., 2023; Li
et al., 2023c; Ye et al., 2023) on Multimodal Large Lan-
guage Models (MLLMs) have opened the eyes of text-only
large language models (LLM, “blind” to visual informa-
1School of Computer Science and Technology, Harbin Institute
of Technology, Shenzhen 2School of Cyber Science and Technol-
ogy, Shenzhen Campus of Sun Yat-sen University. Correspondence
to: Baotian Hu <hubaotian@hit.edu.cn>.
Preliminary work. Working in Progress.
Figure 1. Comparisons between the proposed MKS2 and previous
supervised fine-tuned (SFT) and multimodal LLMs. MKS2 fo-
cuses on improving LLMs with visual knowledge. VMN refers
to the visual mapping network, transferring image encoding to
the language space. MVM and MoMEs represent the proposed
modular visual memory and soft mixtures-of-multimodal experts
architecture in LLMs, respectively.
tion), allowing them to understand and process multimodal
information, thereby promoting the further development
of LLMs-centered Artificial General Intelligence (AGI). In
this line of works such as MiniGPT-4 (Zhu et al., 2023),
LLaVA (Liu et al., 2023a), and BLIP-2 (Li et al., 2023a),
information outside language modality is usually aligned
into language space, and then the rich knowledge stored
in LLM and its powerful text generation capability are
used to understand various multimodal information and
generate the response corresponding to human instructions.
They took an significant step towards constructing a multi-
modal large visual-language model similar to GPT-4 (Ope-
nAI, 2023), contributing a lot of multimodal instruction-
following data (Zhang et al., 2023; Liu et al., 2023a;b) and
efficient multimodal fine-tuning technical (Ye et al., 2023;
Zhu et al., 2023). These approaches concentrating on mul-
timodal information understanding could be regarded as
“LLMs for vision” because of mainly utilizing LLMs for
1
arXiv:2311.15759v1  [cs.CL]  27 Nov 2023
Multimodal Knowledge Storage and Sharing within LLM
processing visual-language problems.
However, current MLLMs, pretrained and supervised fine-
tuned (SFT) LLMs both overlook enhancing the ability of
LLMs to tap into visual knowledge. Ideally, just as the hu-
man brain retains and utilizes visual information, MLLMs
or LLMs should be equipped to store external visual infor-
mation. In situations that require visual common sense, even
in the absence of direct visual input, LLMs should be able to
access this stored visual-language knowledge for combined
reasoning. This goes beyond merely processing multimodal
input, as “LLMs for Vision” depicted in Figure 1. Hence,
we present a term “Vision Enhancing LLMs” to describe
the desired capability for LLMs. Through this enhancement,
large models would store and effectively draw upon multi-
modal knowledge and their knowledge base and reasoning
capabilities would be enhanced.
To this end, We present MKS2, an innovative approach de-
signed for empowering Multimodal Knowledge Storage and
Sharing within LLM, consisting of two core stages: Visual
Information Storage and Multimodal Knowledge Collab-
oration. In the first stage, we introduce Modular Visual
Memory (MVM) in internal transformer blocks of LLMs to
store visual information. Specifically, inspired by previous
works (Kazemnejad et al., 2023; Wang et al., 2022b) fo-
cused on measuring parametric knowledge of pretrained lan-
guage models and observing the knowledge storage role of
feed-forward neural networks (FNN), we incorporate a two
layers of FNN into each LLM block to build a lightweight
visual memory. Subsequently, we employ a collection of
image-text pairs to exclusively train and update MVM us-
ing two learning approaches: image-to-text generation and
text-to-image retrieval. In both ways, soft image and to-
ken embeddings pass through the visual memory following
attention calculations. These strategies empower LLMs
to comprehend, translate, and store visual information in
LLMs via a linguistic framework.
For multimodal knowledge collaboration, we introduce a
soft Mixtures-of-Multimodal Experts (MoMEs) architecture.
This framework leverages specialized experts, including the
Modular Visual Memory (Visual Expert) and the original
MLPs (Textual Expert) in LLMs during the generation pro-
cess. To efficiently achieve this, we freeze all parameters
of LLMs, apply Low-Rank Adaption (LoRA (Hu et al.,
2021)) to each expert module and facilitate information
integration across LLM blocks through a token-level soft
mixing approach. By doing so, the overall model becomes
adept at accommodating both multimodal and text-modality
information, enabling seamless collaboration across vari-
ous input forms. During training, we collect a diverse set
of instruction data, containing text-only instructions and
image-text multimodal instruction-following data, to ensure
the effectiveness of MoMEs in handling multimodal as well
Figure 2. MKS2-Llama-2-13b achieves SOTA zero-shot perfor-
mance on seven natural language reasoning tasks. It indicates
achieving multimodal knowledge storage and share is effective for
improving the overall capability of LLMs.
as text-only tasks.
To validate the effectiveness of our approach, we evaluate
MKS2 on seven natural language processing (NLP) bench-
marks and six image-text understanding datasets. Extensive
experiment results indicate that MKS2 achieves superior
performances on NLP tasks requiring physical or visual
world knowledge, e.g, MKS2-Llama-2 significantly exceeds
Llama-2-chat as shown in Figure 2. It also achieves compet-
itive performances on image-text understanding scenarios
compared to previous MLLMs.
Our main contributions can be summarized as follows:
• We introduce MKS2, a vision-enhanced learning frame-
work for LLMs, designed for effective storage and
sharing of multimodal knowledge. This framework ef-
ficiently handles both multimodal and text-only inputs.
• MKS2 demonstrates superior outcomes in knowledge-
intensive tasks over traditional SFT LLMs and LLMs
employing Reinforcement Learning from Human Feed-
back (RLHF).
• Ablation studies validate the efficacy of mixtures-of-
multimodal-experts that incorporates a visual knowl-
edge expert. This architecture distinctly improves the
performance of LLMs beyond the capacities of conven-
tional supervised fine-tuned LLMs.
• Our experiments indicate that multimodal instruction-
following data further enhances LLMs’ performance in
natural language reasoning tasks that require extensive
commonsense.
2
Multimodal Knowledge Storage and Sharing within LLM
Figure 3. The overall work flow of MKS2. It realizes visual information storage and multimodal knowledge collaboration in LLMs.
In the first stage, we introduce the modular visual memory (MVM, presented in blue blocks) and train it through language-centered
learning strategies. We also present a soft mixtures-of-multimodal experts (MoMEs) architecture to accomplish multimodal knowledge
collaboration during generation.
2. Preliminaries
We first review the supervised fine-tuning approach and
recently proposed multimodal instruction-following tuning
method for LLMs.
2.1. Supervised Fine-tuning
A pure pretrained LLM is fine-tuned on high-quality la-
beled datasets using token-level supervision to produce a
Supervised Fine-Tuned model, dubbed as SFT-LLM. Com-
mon methods are using GPT-4 automatically constructed
instruction data (Wang et al., 2022c) and manually anno-
tated high-quality data from downstream tasks (Chung et al.,
2022) to fine-tune pure LLMs. To reduce training costs,
recent works present some efficient instruction-tuning ap-
proaches, e.g., LoRA (Hu et al., 2021), QLoRA (Dettmers
et al., 2023), etc. These SFT-LLMs are capable of generat-
ing human-like responses for various text-only instructions,
having a profound impact on all walks of life.
2.2. Multimodal Instruction-Following Tuning
Compared to traditional visual-language models such as
Oscar (Li et al., 2020), Flamingo (Alayrac et al., 2022),
OFA (Wang et al., 2022a), etc, the multimodal instruction-
following tuning approach explored extending the text-
only instruction tuning in LLMs to multi-modality. These
MLLMs applying LLMs as the multimodal information
processor achieve impressive zero-shot performances on un-
seen tasks. Generally, as the traditional approach depicted
in Figure 1, a frozen visual encoder (e.g., visual encoder
of CLIP) is used to obtain the sequence representation of
an image and a visual mapping network (VMN, a linear
projection layer or Q-former from BLIP-2) projects the im-
age encoding into soft image embeddings into the language
space of LLMs. Then, we can utilize an efficient fine-tuning
technical to allow LLMs to process multimodal information,
thereby turning LLMs into MLLMs.
Formally, a multimodal image-text instruction sample could
be expressed in the following triplet form, i.e., (I, T, R),
where I, T, R represent the input image, text description
(about human demands or image-related premises), and
ground-truth response, respectively. During training, the
constructed MLLMs is forced to predict the next token of
response via the autoregressive objective, which could be
presented as:
L(θ) = −
N
X
i=1
log P (Ri | I, T, R<i; θ) ,
(1)
where N is the length of response and θ refers to the training
parameters in the whole framework.
In conclusion, we find that these two approaches ignore
introducing visual knowledge to improve overall capabilities
of LLMs for processing text-only tasks.
3
Multimodal Knowledge Storage and Sharing within LLM
3. Methodology
In the following subsections, we will present the two stages
of MKS2 in detail: Visual Information Storage and Multi-
modal Knowledge Collaboration.
3.1. Visual Information Storage
To realize visual information storage in LLMs, we propose
injecting Modular Visual Memory (MVM) into internal
blocks of LLMs and forcing MVM to memorize open-world
visual information via language-centered learning strategies.
Modular Visual Memory (MVM). This module is two
layers of feed-forward neural networks (FFN) and injected
into each transformer block of LLMs. As the top part shown
in Figure 3, the input image I is first projected into soft
image embedding hI via the pretrained visual encoder, Q-
former from BLIP-2, and a learnable linear layer. Take the
first block as an example; the calculation process can be
presented as follows:
hT
s = Self-Attention (hI) ,
hT
F = hT
s + MVM
 layernorm(hT
s )

,
(2)
where Self-Attention is the original attention calculation in
LLMs. We just inserted MVM inside the original LLMs and
did not change other structures. All hidden states pass the
MVM after gaining the output hT
s of Self-Attention, and we
also set the overall size of visual memory by controlling the
hidden state dimensions of FFN.
Language-Centered Learning Strategies. As we consider
LLMs as analogs to the human brain, we have embarked
on a groundbreaking endeavor to create the visual storage
memory in LLMs. Our ultimate goal is to empower LLMs
with the capability to comprehend a given image and con-
jure related visual scenarios based on textual input, akin to
human cognition. To this end, we adopt two learning objects
to train MVM with a large mount of image-text pairs. As
shown in Figure 3, we allow LLMs to generate the language
description of an image, which resembles understanding
and translating an image like brain. Additionally, given a
sentence with some visual objects, LLM should attach to the
sentence-related image, which resembles imagination. Sup-
pose that the short description (caption) of an input image I
is D, the description generation loss is
Lc = −1
N
N
X
i=1
lc (IMGi, Di) ,
(3)
where N is the number of image-text pairs in a batch and lc
refers to the cross-entropy loss.
While retrieving the related image, we use the output hidden
state he of the end token ⟨/s⟩of input caption to match the
image embedding. Concretely, we employ a learnable linear
layer to project it into the same dimension with image global
encoding obtained by the visual encoder. Then we calcu-
late the cosine similarity between them and minimize the
InfoNCE loss for text-to-image (t2i) retrieval over a batch
of N samples. The negatives are other irrelevant images in
a batch. Hence, the total language-centered learning loss is
LStage1 = Lc + Lt2i,
Lt2i = −1
N
N
X
i=1
 
log
exp (sim (Di, IMGi) /τ)
PN
j=1 exp (sim (Di, IMGj) /τ)
!
,
(4)
where τ is a learnable temperature parameter. During train-
ing, we freeze all pretrained parameters of LLMs and only
update MVM. In addition to the way of retrieving images
to achieve visual information association, using image gen-
eration technology for joint training is also an alternative
approach.
3.2. Multimodal Knowledge Collaboration
After gaining visual information storage inside LLMs, we
need consider how to realize multimodal knowledge collab-
oration during generation. Regarding pretrained MVM and
MLP in LLMs as visual and textual experts respectively, we
propose a soft mixtures-of-multimodal experts (MoMEs)
approach to achieve multimodal knowledge utilization at
the token level.
Mixtures-of-Multimodal Experts (MoMEs). To speed up
training process, as the bottom part shown in Figure 3, we
freeze MVM and other parameters of LLMs, applying Low-
Rank Adaption (Hu et al., 2021) (LoRA) to tow-modality
experts: MVM and MLP. We denote the inputs tokens for
one sequence inputted to MoMEs by X ∈Rm×d, where
m is the number of tokens and d is their dimension. The
computed process for visual and language knowledge expert
could be given in
hV E = LoRA-MVM(X),
hT E = LoRA-MLP(X),
LoRA(W0) := W0X + ∆WX = W0x + BAX,
(5)
where B, A are learnable parameters added for each pre-
trained weight of visual and textual experts. LoRA-MVM
and LoRA-MLP(X) represent original knowledge experts
equipped with additional LoRA calculation. By doing so,
the training process is efficient because of doing not update
the overall parameters of experts.
Each MoE layer uses expert functions (shown in E.q. 5)
applied on individual tokens, namely

fi : Rd →Rd	
1:2.
Each expert will process p slots, and each slot has a cor-
responding d-dimensional vector of parameters. As S→M
shown in Figure 3, the token-level combination for expert
4
Multimodal Knowledge Storage and Sharing within LLM
outputs can be presented as
S = Softmax(wsX + bs),
hM = S1hV E + S2hT E,
ho = hT
s + hM,
(6)
where S ∈RX×2 and the final dimension is normalized
with Softmax calculation. The output of each block in LLMs
is denoted to ho.
3.3. Training
In the first stage, the size of used image-text pairs are about
2.3M from CC3M (Changpinyo et al., 2021), COCO Cap-
tioning (Chen et al., 2015), and Flick-30k (Plummer et al.,
2015). To achieve multimodal knowledge collaboration,
as shown in Figure 1, we use text-only and image-text
instruction-following data to train the overall architecture.
The added modular visual memory and LLMs are frozen dur-
ing training. We use widely-used instruction data including:
high-quality natural language processing tasks from Flan-
T5 (Chung et al., 2022), complex instruction-finetuning data
from WizardLLMs (Xu et al., 2023), and multimodal in-
struction data LLaVAR (Zhang et al., 2023), which totally
consists of 1.5M text-only and 166k image-text instruction
tuning data.
4. Experiments
4.1. Datasets
Natural Language Processing Benchmarks. We use seven
text-only downstream datasets to comprehensively evalu-
ate MKS2, which consists of physical world knowledge-
relevant datasets and basic ability assessment benchmark
MMLU (Hendrycks et al., 2021). We use multiple choice
question answering tasks that can benefit from visual knowl-
edge: PIQA (Bisk et al., 2020) that requires physical com-
monsense reasoning, Commonsense QA (CSQA) (Talmor
et al., 2019) for evaluating the commonsense reasoning capa-
bility of models, OpenBook QA (OBQA) (Mihaylov et al.,
2018) that requires multi-step reasoning, use of additional
common and commonsense knowledge, and rich text com-
prehension, RiddleSense (RS) (Lin et al., 2021) for complex
understanding of figurative language and counterfactual rea-
soning skills, Social IQA (Sap et al., 2019) which focuses
on physical or taxonomic knowledge for testing social com-
monsense intelligence, StrategyQA (Geva et al., 2021) that
needs the reasoning steps should be inferred using a strategy.
Image-Text Understanding Benchmarks.
To evalu-
ate the multimodal capability of our proposed model,
we introduce six classical Visual Question Answering
(VQA) datasets:
VQAv2 (Antol et al., 2015), OK-
VQA (Marino et al., 2019), ST-VQA (Biten et al., 2019),
OCR-VQA (Mishra et al., 2019), TextVQA (Singh et al.,
2019), and DocVQA (Mathew et al., 2021). VQAv2 is a
classic open-world VQA dataset, containing more than 1
million samples. Scene Text Visual Question Answering
(STVQA) consists of 31,000+ questions across 23,000+ im-
ages collected from various public datasets. The OCRVQA
dataset includes more than 1 million question-answer pairs
that cover over 207,000 book cover images. The TextVQA
dataset consists of over 45,000 questions related to text on
more than 28,000 images selected from specific categories
of the OpenImages dataset. DocVQA is a comprehensive
dataset comprising 12,767 document images with diverse
types and content, accompanied by over 50,000 questions
and answers. For datasets containing far more than 5000
image-question pairs, we selected the first 5000 pairs for
test, similar to Liu et al. (2023c).
4.2. Comparing Models
The comparing models mainly comprise three types of
open-source large language models Llama-2: SFT Llama-
2, RLHF-tuned Llama-2, and recently proposed MLLMs.
To verify the new vision enhancing supervised fine-tuning
method MKS2, we present text-only instruction tuned vari-
ant Llama-2-7b-INST-LoRA and Vicuna-Llama-2 (Penedo
et al., 2023), where we adopt the text instruction data iden-
tical to our approach and set r = 16 for LoRA to train
Llama2-7b-INST. Hence, the training parameters of Llama-
2-7b-INST-LoRA is similar to the proposed MKS2-Llama-
2-7B, about 14M. RLHF-tuned models are are language
models that have been trained using a combination of human
feedback and reinforcement learning techniques, achieving
better performance to understand human instruction and gen-
erate high-quality responses. We mainly comparing with
Llama-2-7b-chat and Llama-2-13b-chat variants released
by Meta. Additionally, to evaluate the multimodal infor-
mation processing capability of MKS2, we also introduce
recently proposed MLLMs as baselines. Flamingo (Alayrac
et al., 2022) and OFA (Wang et al., 2022a) are traditionally
pretrained visual and language models, which have seen
an amount of image-text pairs. BLIP-2 (Li et al., 2023a)
is a widely-used visual and language models, achieving
remarkable zero-shot performance on downstream image-
text understanding tasks. MiniGPT-4 (Zhu et al., 2023),
FROMAGe (Koh et al., 2023), mPLUG-Owl (Ye et al.,
2023), LLaVR (Liu et al., 2023a) and InstructBLIP (Li et al.,
2023a) are multimodal instruction tuned MLLMs, trained
with enormous image-text instruction-following data.
4.3. Implementation Details
We take the pretrained Llama-2 version (Touvron et al.,
2023) as the backbone of MKS2 and run all models with
Adam Optimizer (Kingma & Ba, 2014) on 4 A100-80G
GPUs with python environment. All models are trained and
tested with Bfloat16 floating-point format. The dimension
5
Multimodal Knowledge Storage and Sharing within LLM
Table 1. Zero-shot model performances on natural language processing benchmarks. Models with † indicate that their SFT or RLHF
tuned data are unknown or unused in our work. “INST-LoRA” refers to applying the widely-used LoRA technical to fine-tune LLMs with
same text-only instruction data. “Multimodal-SFT” represents the multimodal instruction-following data. “Avg.Score” shows the average
evaluation score on the total tasks. Bold and underlined numbers refer to the best and second-best performance for comparative model
variants of Llama-2-7b/13b, respectively.
Models↓Types →
COQA StrategyQA Social IQA OBQA PIQA
RS
MMLU Avg.Score
Llama-2-13b-chat† (Touvron et al., 2023)
37.02
37.80
49.46
42.89
67.29 27.45
47.69
44.23
Vicuna-Llama-2-13b† (Chiang et al., 2023)
58.21
38.82
55.85
55.46
67.01 37.02
50.96
51.90
Llama-2-13b-INST-LoRAr=16
57.68
63.73
63.80
58.6
71.98 38.51
46.70
57.28
MKS2-Llama-2-13b
62.10
74.68
65.71
67.6
76.11 41.03
48.83
62.30
w/o Multimodal-SFT
58.77
74.73
64.56
60.6
75.03 38.74
48.44
60.12
w/o Multimodal-SFT & MoMEs
54.81
68.21
62.25
54.0
67.95 35.08
46.50
55.54
Llama-2-7b-chat† (Touvron et al., 2023)
31.62
36.83
42.37
35.3
64.90 23.53
37.05
38.82
Vicuna-Llama-2-7b† (Chiang et al., 2023)
42.58
67.58
39.71
38.2
55.62 29.32
38.94
44.56
Llama-2-7b-INST-LoRAr=16
41.93
74.10
54.65
39.4
53.42 27.01
38.68
47.02
MKS2-Llama-2-7b
49.38
76.15
58.51
54.0
68.19 33.20
39.27
54.10
w/o Multimodal-SFT
44.06
76.46
57.72
50.5
67.10 28.99
37.45
51.84
w/o Multimodal-SFT & MoMEs
42.84
70.46
55.42
37.0
60.71 25.27
37.71
47.06
of the middle layer of the inserted visual memory module
is 1/4 of the hidden state size of LLMs. For Llama-2-7b,
the total parameters of MVM is about 410 million. Dur-
ing visual information storage, we take the frozen visual
encoder and Q-former from BLIP-2-FlanT5-xxl to obtain
the image encoding, thus the length of soft image embed-
ding is 32. Additionally, we set the initial learning rate to
1e-4 and train the model for about 2 epochs with warm up
steps equaling 5000. The batch size is set to 32 with four-
step gradient accumulation for single GPU device. While
performing instruction-following learning, we set the batch
size, r in LoRA to 3 and 8, respectively, and the max length
of input is set to 1024. To tag the position of image embed-
ding, we introduce two learnable tokens ⟨img-start⟩and
⟨img-end⟩. Similar to Llama-2-chat, we add [INST] and
[/INST] at the starting and ending of inputting text instruc-
tion, as like “[INST] Please write a short story about cat
and dog [/INST]”. During generation, we set beam sizes to
1 and 4 for text-only and VQA tasks respectively.
4.4. Overall Performances
Performance of vision enhancing LLMs. We present zero-
shot model performances on Table 1, aiming to evaluate the
instruction-understanding and open-world problem solving
abilities of LLMs. We observe that the proposed method
MKS2-Llama-2-7B/13B achieves best performances on al-
most all evaluation datasets, especially on substantially
suppressing Llama-2-7b/13b-chat. Compared to powerful
Llama-2-7b-INST-LoRA of the same magnitude, MKS2-
Llama-2-7b could gains by about 8% on CommensenseQA,
14.5% on OpenBookQA, 16% on PIQA, and 8.6% on RS,
respectively. Hence, MKS2 is capable of markedly improv-
ing the overall performance on text-only tasks requiring
physical world knowledge. Compared to Vicuna-Llama-2
models with all parameters of Llama-2 updating, our ap-
proach stands out by requiring being fine-tuned on only a
small fraction of parameters (< 0.2% of LLM parameters)
while still achieving superior performance on several tasks.
Competitive performances on multimodal benchmarks.
We also present the model’s zero-shot performance on the
VQA dataset in Table 2. To gain suitable and robust image
embeddings, we further fine-tuned the visual mapping net-
work for one epoch and freeze all other parameters, which
does not affect any text-only performance of LLMs. We can
see that MKS2-Llama-2-7b could achieve comparative per-
formances on open-world and scene text VQA datasets. It’s
noteworthy that there was no discernible performance degra-
dation when comparing MKS2 performance on multimodal
tasks to that of the original large language model without
visual enhancement. This implies that the addition of visual
enhancement in LLMs did not lead to a loss in text-related
knowledge, while performing LLMs for vision. Moreover,
the incorporation of text-only data proves to be a valuable
strategy for enhancing the model proficiency in answering
open visual questions. While mixed instruction data leads
to improvements in open-world question answering, it ap-
pears to have a detrimental effect on scene text recognition,
possibly due to shifts in training data distribution. Further
investigation into the fine-tuning process and data distribu-
tions can help optimize MKS2 performance across a wider
range of tasks involving both text and images.
6
Multimodal Knowledge Storage and Sharing within LLM
Table 2. Zero-shot performances on some multimodal datasets.“NumImg” represents the total number of images contained in the
pretraining stage. The size of input image is always 2242 for the following models. “‡” indicates that the corresponding model employs
the training samples of following evaluation benchmarks such as VQAv2, OK-VQA, and OCR-VQA, leading to unfair comparison.
Models↓Types →
NumImg VQAv2 OK-VQA STVQA OCR-VQA TextVQA DocVQA
Flamingo (Alayrac et al., 2022)
>1B
49.2
41.2
19.3
27.8
29.0
5.0
MiniGPT-4 (Vicuna-7b) (Zhu et al., 2023)
5M
44.3
32.1
14.0
11.5
18.7
3.0
LLaVA (Vicuna-7b) (Liu et al., 2023a)
0.6M
53.5
47.4
22.1
11.4
28.9
4.5
LLaVAR (Vicuna-13b) (Zhang et al., 2023)
1M
-
-
30.2
23.4
39.5
6.2
OFA-Large (Wang et al., 2022a)
20M
40.2
19.3
-
-
-
-
FROMAGe (OPT-6.7b) (Koh et al., 2023)
3.3M
44.1
20.1
-
-
-
-
mPLUG-Owl‡ (Ye et al., 2023)
11B
-
-
29.3
28.6
40.3
6.9
InstructBLIP‡ (FlanT5XL) (Liu et al., 2023a)
129M
62.6
50.1
23.9
39.7
33.1
3.8
BLIP-2 (OPT-6.7b) (Li et al., 2023a)
129M
50.1
36.4
13.4
10.6
21.2
0.8
BLIP-2 (FlanT5XL) (Li et al., 2023a)
129M
42.8
25.6
15.8
26.6
25.2
2.9
BLIP-2 (FlanT5XXL-11B) (Li et al., 2023a)
129M
45.4
27.8
21.7
30.7
32.2
4.9
MKS2-Llama-2-13b
2.3M
54.4
45.1
23.4
35.7
34.2
6.7
MKS2-Llama-2-7b
2.3M
53.3
42.1
22.3
25.2
33.1
6.7
w/o Text-SFT
2.3M
50.2
40.8
21.5
36.5
34.2
7.4
w/o Text-SFT & MoMEs
2.3M
50.1
41.2
21.4
35.3
34.3
7.3
4.5. Ablation Study and Analysis
Effects of MKS2. Comparing the experimental results of
MKS2 w/o Multimodal-SFT, MKS2 w/o Multimodal-SFT
& MoMEs, and Llama-2-7b-INST-LoRA in Table 1, we
observed that the incorporation of visual information into
the MVM positively impacted the model’s performance on
common sense reasoning tasks. This demonstrates that Mo-
MEs can leverage stored visual information to improve its
understanding and reasoning abilities in various contexts.
Additionally, the performances of MKS2 variants w/o Text-
SFT and w/o Text-SFT + MoMEs on multimodal tasks
further emphasizes its ability to access textual knowledge
without significant compromise. The integration of visual in-
formation alongside textual data did not hinder the model’s
capacity to extract and utilize textual knowledge effectively.
This suggests that the core text-related capabilities of LLMs
remain robust when dealing with multimodal inputs.
Impact of multimodal instruction data for MKS2 per-
formance. Our experimental results in Table 1 highlights
the impact of multimodal instruction data on MKS2’s per-
formance. While it effectively enhances the accuracy in
addressing questions related to physical world knowledge,
it does not provide a substantial boost in solving intricate
and complex problems that demand advanced reasoning and
strategic thinking. These findings emphasize the importance
of tailoring data and approaches to specific task require-
ments when leveraging multimodal data in large language
models for optimal performance. Further research may
uncover strategies to bridge this gap for complex problem-
solving tasks.
Table 3. Ablation Experiments on five datasets requiring common
sense. We explore various data and visual memory sizes to check
model performances. All models are built upon Llama-2-7b, where
MKS2♣signifies MKS2 w/o Multimodal-SFT. ”-AM-BM” indi-
cates that it includes a ”A” size of visual memory and was trained
using ”B” number of image-text pairs.
Models↓Types →
COQA OBQA PIQA
RS
Social IQA
Llama-2-7b-chat†
31.62
35.3
64.90 23.53
42.37
Vicuna-Llama-2-7b†
42.58
38.2
53.42 29.32
39.71
MKS2♣-410M-2.3M
44.06
50.5
67.10 28.99
57.72
MKS2♣-410M-12.3M 46.12
50.8
64.68↓30.06
55.31↓
MKS2♣-810M-2.3M
43.78
48.6
66.97 28.89
57.21
MKS2♣-810M-12.3M 46.56
49.8
68.10 30.65
57.84
Could text-only instruction data be effective for enhanc-
ing multimodal performance while building MLLMs?
In the ablation experiments discussed in Table 2, text-only
instruction data was found to enhance the performance of
pre-trained LLMs on various open multimodal problems,
particularly open-ended questions that demand a fusion of
textual and visual information. However, the introduction of
text-only instruction data may lead to a trade-off, as it could
potentially diminish the LLMs to answer visual questions
involving scene text recognition. These findings underscore
the importance of striking a careful balance when incorpo-
rating additional data modalities into LLMs, with a keen
consideration of task requirements and data characteristics.
Such careful integration ensures that overall model perfor-
mance is optimized without unintended consequences on
7
Multimodal Knowledge Storage and Sharing within LLM
Figure 4. An illustration of cases generated by comparative models and MKS2. Green words refer to the correct answer in the natural
question answering tasks. Yellow parts represent the interesting and correct content in the response.
its ability to handle diverse multimodal challenges. Conse-
quently, optimizing LLMs for multimodal tasks requires a
nuanced approach tailored to the specific problem domain.
Impact of visual memory size and data scale. We intro-
duce more image-text pairs from LAION-400M (Schuh-
mann et al., 2021) to analyse the impacts of vision memory
and data sizes, and the experimental results are shown in Ta-
ble 3. Our experimental results reveal that enlarging the size
of the pre-trained image-text data leads to improvements in
the MKS2 model’s performance across various downstream
tasks. Additionally, we observe that expanding the visual
storage module of the MKS2 model not only enhances per-
formance but also contributes to greater stability in this
enhancement, as the size of the image-text data increases.
To summarize, these findings suggest a twofold strategy for
optimizing the MKS2-based LLMs during the supervised
fine-tuning phase: increasing the size of the image-text data
and selecting a proportionately larger visual storage size.
This approach is crucial for achieving more consistent and
stable performance improvements.
4.6. Case Study
We present some cases in Figure 4 to further show the overall
capability of MKS2-Llama-2. We can see that the proposed
model achieve better performance while answering com-
monsense Q&A with physical knowledge. In addition, we
also observe that the multimodal understanding capability
of MKS2-Llama-2 is powerful, such as it could recognize
the funny of the cat with a lion’s head. In addition, it can em-
ploy the relevant knowledge to enrich the response based on
visual clues, e.g., the short story around the content shown
in the second image.
5. Related Works
Visual knowledge enhanced methods. There is a long line
of work on utilizing explicit visual information to improve
the imaginative representation of language, thus promot-
ing diverse generation capability of LLMs. Particularly,
Jin et al. (2022) leverage visual knowledge in NLP tasks,
developing multiple cross-model enhanced methods to im-
prove the representation capability of pretrained language
models. Some works (Shi et al., 2019; Lu et al., 2022; Li
et al., 2023b) proposed to retrieve images corresponding to
texts from the image corpus and use visual knowledge to
improve the performance on the downstream tasks such as
text completion (Zellers et al., 2019), story generation (Fan
et al., 2018), and concept-to-text (Barzilay & Lapata, 2005).
Recently, some researchers (Long et al., 2021; Yang et al.,
2021; Zhu et al., 2022) proposed to utilize the powerful
text-to-image technical to obtain the imagination represen-
8
Multimodal Knowledge Storage and Sharing within LLM
tation of language and infuse them into the language model
via the prefix-tuning way. In this paper, we present visual
information storage in LLMs and achieve visual knowledge
enhancing LLMs without explicitly inputting images to lan-
guage models.
LLMs for vision. Recent works (Zhang et al., 2023; Zhu
et al., 2023; Li et al., 2023a) towards multimodal LLMs fo-
cus on utilizing the extensive knowledge and language gen-
eration capabilities of LLMs to solve multimodal tasks (Li
et al., 2023d), especially for visual understanding and rea-
soning. Firstly, these works usually map the visual informa-
tion obtained by pretrained visual encoder into the represen-
tation space of LLMs, through a learnable linear projection
layer (Merullo et al., 2023), MLP, or Q-Former (Li et al.,
2023a). This stage is usually called feature alignment and
only a few hundred thousand data may be needed to do a
good job (Liu et al., 2023a). Afterwards, the initial MLLMs
will be tuned via multimodal instruction-following data (Ye
et al., 2023; Li et al., 2023c; Bai et al., 2023). At this stage,
LLMs and projection layer are often tuned together only
with multimodal instruction data. The commonly used large
language model is the SFT LLM and the tuning approach
adopts widely used lightweight LoRA (Hu et al., 2021).
These works, however, rarely consider using visual knowl-
edge to enhance the pure text processing capabilities of
LLMs, thereby building a more robust LLM or MLLM.
6. Conclusion
In this paper, we present a new approach MKS2 that al-
lows LLMs to memorize and employ visual information,
achieving multimodal knowledge storage and collaboration
in LLMs. MKS2 consists of modular visual memory and
soft mixtures-of-multimodal experts, which are used to store
visual information and realize multimodal knowledge col-
laboration, respectively. We conduct extensive experiments
on many NLP and VQA tasks and the experimental results
show that MKS2 is capable of enhancing the reasoning
capability of LLMs and being used to solve multimodal
problems.
References
Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I.,
Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds,
M., et al. Flamingo: a visual language model for few-shot
learning. Advances in Neural Information Processing
Systems, 35:23716–23736, 2022.
Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D.,
Zitnick, C. L., and Parikh, D. VQA: Visual Question
Answering. In International Conference on Computer
Vision (ICCV), 2015.
Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J.,
Zhou, C., and Zhou, J. Qwen-vl: A frontier large vision-
language model with versatile abilities. arXiv preprint
arXiv:2308.12966, 2023.
Barzilay, R. and Lapata, M. Collective content selection
for concept-to-text generation. In Proceedings of the con-
ference on Human Language Technology and Empirical
Methods in Natural Language Processing, pp. 331–338,
2005.
Bisk, Y., Zellers, R., Bras, R. L., Gao, J., and Choi, Y.
Piqa: Reasoning about physical commonsense in natural
language. In Thirty-Fourth AAAI Conference on Artificial
Intelligence, 2020.
Biten, A. F., Tito, R., Mafla, A., Gomez, L., Rusinol, M.,
Valveny, E., Jawahar, C., and Karatzas, D. Scene text vi-
sual question answering. In Proceedings of the IEEE/CVF
international conference on computer vision, pp. 4291–
4301, 2019.
Changpinyo, S., Sharma, P., Ding, N., and Soricut, R. Con-
ceptual 12m: Pushing web-scale image-text pre-training
to recognize long-tail visual concepts. In Proceedings
of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 3558–3568, 2021.
Chen, X., Fang, H., Lin, T.-Y., Vedantam, R., Gupta, S.,
Doll´
ar, P., and Zitnick, C. L. Microsoft coco captions:
Data collection and evaluation server. arXiv preprint
arXiv:1504.00325, 2015.
Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang,
H., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E.,
Stoica, I., and Xing, E. P.
Vicuna: An open-source
chatbot impressing gpt-4 with 90%* chatgpt quality,
March 2023.
URL https://lmsys.org/blog/
2023-03-30-vicuna/.
Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y.,
Fedus, W., Li, E., Wang, X., Dehghani, M., Brahma,
S., et al. Scaling instruction-finetuned language models.
arXiv preprint arXiv:2210.11416, 2022.
Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer,
L. Qlora: Efficient finetuning of quantized llms. arXiv
preprint arXiv:2305.14314, 2023.
Driess, D., Xia, F., Sajjadi, M. S., Lynch, C., Chowdhery,
A., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T.,
et al. Palm-e: An embodied multimodal language model.
arXiv preprint arXiv:2303.03378, 2023.
Fan, A., Lewis, M., and Dauphin, Y. Hierarchical neural
story generation. In Proceedings of the 56th Annual Meet-
ing of the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pp. 889–898, Melbourne, Australia,
9
Multimodal Knowledge Storage and Sharing within LLM
July 2018. Association for Computational Linguistics.
URL https://aclanthology.org/P18-1082.
Geva, M., Khashabi, D., Segal, E., Khot, T., Roth, D., and
Berant, J. Did Aristotle Use a Laptop? A Question An-
swering Benchmark with Implicit Reasoning Strategies.
Transactions of the Association for Computational Lin-
guistics (TACL), 2021.
Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M.,
Song, D., and Steinhardt, J. Measuring massive multitask
language understanding. Proceedings of the International
Conference on Learning Representations (ICLR), 2021.
Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang,
S., Wang, L., and Chen, W. Lora: Low-rank adaptation of
large language models. arXiv preprint arXiv:2106.09685,
2021.
Jin, W., Lee, D.-H., Zhu, C., Pujara, J., and Ren, X. Lever-
aging visual knowledge in language tasks: An empirical
study on intermediate pre-training for cross-modal knowl-
edge transfer. In Proceedings of the 60th Annual Meeting
of the Association for Computational Linguistics (Volume
1: Long Papers), pp. 2750–2762, Dublin, Ireland, May
2022. Association for Computational Linguistics.
Kazemnejad, A., Rezagholizadeh, M., Parthasarathi, P.,
and Chandar, S. Measuring the knowledge acquisition-
utilization gap in pretrained language models.
arXiv
preprint arXiv:2305.14775, 2023.
Kingma, D. P. and Ba, J. Adam: A method for stochastic
optimization. arXiv preprint arXiv:1412.6980, 2014.
Koh, J. Y., Salakhutdinov, R., and Fried, D. Grounding
language models to images for multimodal generation.
arXiv preprint arXiv:2301.13823, 2023.
Langley, P. Crafting papers on machine learning. In Langley,
P. (ed.), Proceedings of the 17th International Conference
on Machine Learning (ICML 2000), pp. 1207–1216, Stan-
ford, CA, 2000. Morgan Kaufmann.
Li, J., Li, D., Savarese, S., and Hoi, S. Blip-2: Bootstrapping
language-image pre-training with frozen image encoders
and large language models. ICML, 2023a.
Li, X., Yin, X., Li, C., Zhang, P., Hu, X., Zhang, L., Wang,
L., Hu, H., Dong, L., Wei, F., et al.
Oscar: Object-
semantics aligned pre-training for vision-language tasks.
In Computer Vision–ECCV 2020: 16th European Con-
ference, Glasgow, UK, August 23–28, 2020, Proceedings,
Part XXX 16, pp. 121–137. Springer, 2020.
Li, Y., Hu, B., Chen, X., Ding, Y., Ma, L., and Zhang, M. A
multi-modal context reasoning approach for conditional
inference on joint textual and visual clues. ACL, 2023b.
Li, Y., Hu, B., Chen, X., Ma, L., and Zhang, M. Lmeye: An
interactive perception network for large language models.
arXiv preprint arXiv:2305.03701, 2023c.
Li, Y., Wang, L., Hu, B., Chen, X., Zhong, W., Lyu, C.,
and Zhang, M. A comprehensive evaluation of gpt-4v on
knowledge-intensive visual question answering. arXiv
preprint arXiv:2311.07536, 2023d.
Lin, B. Y., Wu, Z., Yang, Y., Lee, D.-H., and Ren, X. Rid-
dlesense: Reasoning about riddle questions featuring lin-
guistic creativity and commonsense knowledge. arXiv
preprint arXiv:2101.00376, 2021.
Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction
tuning. arXiv preprint arXiv:2304.08485, 2023a.
Liu, Y., Duan, H., Zhang, Y., Li, B., Zhang, S., Zhao, W.,
Yuan, Y., Wang, J., He, C., Liu, Z., et al. Mmbench: Is
your multi-modal model an all-around player?
arXiv
preprint arXiv:2307.06281, 2023b.
Liu, Y., Li, Z., Li, H., Yu, W., Huang, M., Peng, D., Liu,
M., Chen, M., Li, C., Jin, L., et al. On the hidden mys-
tery of ocr in large multimodal models. arXiv preprint
arXiv:2305.07895, 2023c.
Long, Q., Wang, M., and Li, L. Generative imagination
elevates machine translation. In Proceedings of the 2021
Conference of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Language
Technologies, pp. 5738–5748, Online, June 2021. Asso-
ciation for Computational Linguistics. URL https://
aclanthology.org/2021.naacl-main.457.
Lu, Y., Zhu, W., Wang, X. E., Eckstein, M., and Wang, W. Y.
Imagination-augmented natural language understanding.
NACCL, 2022.
Marino, K., Rastegari, M., Farhadi, A., and Mottaghi, R.
Ok-vqa: A visual question answering benchmark requir-
ing external knowledge. In Proceedings of the IEEE/cvf
conference on computer vision and pattern recognition,
pp. 3195–3204, 2019.
Mathew, M., Karatzas, D., and Jawahar, C. Docvqa: A
dataset for vqa on document images. In Proceedings
of the IEEE/CVF winter conference on applications of
computer vision, pp. 2200–2209, 2021.
Merullo, J., Castricato, L., Eickhoff, C., and Pavlick, E.
Linearly mapping from image to text space. ICLR, 2023.
Mihaylov, T., Clark, P., Khot, T., and Sabharwal, A. Can a
suit of armor conduct electricity? a new dataset for open
book question answering. In EMNLP, 2018.
10
Multimodal Knowledge Storage and Sharing within LLM
Mishra, A., Shekhar, S., Singh, A. K., and Chakraborty, A.
Ocr-vqa: Visual question answering by reading text in
images. In ICDAR, 2019.
OpenAI.
Gpt-4
technical
report.
https://arxiv.org/abs/2303.08774, 2023.
Penedo, G., Malartic, Q., Hesslow, D., Cojocaru, R., Cap-
pelli, A., Alobeidli, H., Pannier, B., Almazrouei, E., and
Launay, J. The RefinedWeb dataset for Falcon LLM:
outperforming curated corpora with web data, and web
data only. arXiv preprint arXiv:2306.01116, 2023. URL
https://arxiv.org/abs/2306.01116.
Plummer, B. A., Wang, L., Cervantes, C. M., Caicedo, J. C.,
Hockenmaier, J., and Lazebnik, S. Flickr30k entities:
Collecting region-to-phrase correspondences for richer
image-to-sentence models. In Proceedings of the IEEE
international conference on computer vision, pp. 2641–
2649, 2015.
Sap, M., Rashkin, H., Chen, D., LeBras, R., and Choi, Y.
Socialiqa: Commonsense reasoning about social interac-
tions. arXiv preprint arXiv:1904.09728, 2019.
Schuhmann, C., Vencu, R., Beaumont, R., Kaczmarczyk,
R., Mullis, C., Katta, A., Coombes, T., Jitsev, J., and
Komatsuzaki, A. Laion-400m: Open dataset of clip-
filtered 400 million image-text pairs.
arXiv preprint
arXiv:2111.02114, 2021.
Shi, H., Mao, J., Gimpel, K., and Livescu, K. Visually
grounded neural syntax acquisition. In Proceedings of
the 57th Annual Meeting of the Association for Compu-
tational Linguistics, pp. 1842–1861, Florence, Italy, July
2019. Association for Computational Linguistics. URL
https://aclanthology.org/P19-1180.
Singh, A., Natarjan, V., Shah, M., Jiang, Y., Chen, X., Batra,
D., Parikh, D., and Rohrbach, M. Towards vqa models
that can read. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pp. 8317–
8326, 2019.
Talmor, A., Herzig, J., Lourie, N., and Berant, J. Common-
senseQA: A question answering challenge targeting com-
monsense knowledge. In Proceedings of the 2019 Confer-
ence of the North American Chapter of the Association for
Computational Linguistics: Human Language Technolo-
gies, Volume 1 (Long and Short Papers), pp. 4149–4158,
Minneapolis, Minnesota, June 2019. Association for
Computational Linguistics. doi: 10.18653/v1/N19-1421.
URL https://aclanthology.org/N19-1421.
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,
Bhosale, S., et al. Llama 2: Open foundation and fine-
tuned chat models. arXiv preprint arXiv:2307.09288,
2023.
Wang, P., Yang, A., Men, R., Lin, J., Bai, S., Li, Z.,
Ma, J., Zhou, C., Zhou, J., and Yang, H.
Unifying
architectures, tasks, and modalities through a simple
sequence-to-sequence learning framework. arXiv preprint
arXiv:2202.03052, 2022a.
Wang, X., Wen, K., Zhang, Z., Hou, L., Liu, Z., and Li,
J. Finding skill neurons in pre-trained transformer-based
language models. In Proceedings of the 2022 Conference
on Empirical Methods in Natural Language Processing,
pp. 11132–11152, Abu Dhabi, United Arab Emirates, De-
cember 2022b. Association for Computational Linguis-
tics. URL https://aclanthology.org/2022.
emnlp-main.765.
Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A.,
Khashabi, D., and Hajishirzi, H. Self-instruct: Aligning
language model with self generated instructions, 2022c.
Xu, C., Sun, Q., Zheng, K., Geng, X., Zhao, P., Feng, J., Tao,
C., and Jiang, D. Wizardlm: Empowering large language
models to follow complex instructions. arXiv preprint
arXiv:2304.12244, 2023.
Yang, Z., Wu, W., Hu, H., Xu, C., Wang, W., and Li, Z.
Open domain dialogue generation with latent images. In
Proceedings of the AAAI Conference on Artificial Intelli-
gence, volume 35, pp. 14239–14247, 2021.
Ye, Q., Xu, H., Xu, G., Ye, J., Yan, M., Zhou, Y., Wang, J.,
Hu, A., Shi, P., Shi, Y., et al. mplug-owl: Modulariza-
tion empowers large language models with multimodality.
arXiv preprint arXiv:2304.14178, 2023.
Yin, S., Fu, C., Zhao, S., Li, K., Sun, X., Xu, T., and Chen,
E. A survey on multimodal large language models. arXiv
preprint arXiv:2306.13549, 2023.
Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y.
HellaSwag: Can a machine really finish your sentence?
In Proceedings of the 57th Annual Meeting of the Asso-
ciation for Computational Linguistics, pp. 4791–4800,
Florence, Italy, July 2019. Association for Computational
Linguistics. URL https://aclanthology.org/
P19-1472.
Zhang, Y., Zhang, R., Gu, J., Zhou, Y., Lipka, N., Yang,
D., and Sun, T. Llavar: Enhanced visual instruction
tuning for text-rich image understanding. arXiv preprint
arXiv:2306.17107, 2023.
Zhu, D., Chen, J., Shen, X., Li, X., and Elhoseiny, M.
Minigpt-4: Enhancing vision-language understanding
with advanced large language models. arXiv preprint
arXiv:2304.10592, 2023.
11
Multimodal Knowledge Storage and Sharing within LLM
Zhu, W., Yan, A., Lu, Y., Xu, W., Wang, X. E., Eck-
stein, M., and Wang, W. Y. Visualize before you write:
Imagination-guided open-ended text generation. arXiv
preprint arXiv:2210.03765, 2022.
12
"
